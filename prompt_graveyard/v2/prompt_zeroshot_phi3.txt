        <|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Per concept list each cited paper's authors and contributions they've made. The examples of how authors can be notated in the text are "Author et al. (2019)" and "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- *Author at al. (2020)*: contribution
To notate the authors of the paper the text was taken from, use the following structure:
- *Authors of the paper*: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

        Text: "To the best of our knowledge, HappyDB is the first crowdsourced corpus of happy moments which can be used for understanding the language people use to describe happy events.  There has been recent interest in creating datasets in the area of mental health.  Althoff et al. (Althoff et al., 2016) conducted a large scale analysis on counseling conversational logs collected from short message services (SMS) for mental illness study.  They studied how various linguistic aspects of conversations are correlated with conversation outcomes.  Mihalcea et al. (Mihalcea and Liu, 2006) performed text analysis on blog posts from LiveJournal (where posts can be assigned happy/sad tags by their authors).  Lin et al. (Lin et al., 2016) measure stress from short texts by identifying the stressors and the stress levels.  They classified tweets into 12 stress categories defined by the stress scale in (Holmes and Rahe, 1967).  Last year alone, there were multiple research efforts that obtained datasets via crowdsourcing and applied natural language techniques to understand different corpora.  For example, SQuAD (Rajpurkar et al., 2016) created a largescale dataset for question-answering.  The crowdsourced workers were asked to create questions based on a paragraph obtained from Wikipedia.  They employed MTurk workers with strong experience and a high approval rating to ensure the quality of the dataset.  We did not select the workers based on their qualification for HappyDB as our task is cognitively easier than SQuAD's and we want to avoid bias in our corpus.  HappyDB is similar to SQuAD in terms of the scale of the crowdsourced dataset.  However, unlike SQuAD, which was designed specifically for studying the question answering problem, the problems that HappyDB can be used for are more open-ended. "<|end|>
        <|assistant|>