{
 "oneshot": " 1. **Question Generation (QG)**:\n   - **Reiter and Dale (2000)**, **Saggion and Poibeau (2013)**, and **Balakrishnan et al. (2019)** discussed QG as a natural language generation task.\n   - Traditional methods for QG:\n     - **Heilman and Smith (2010)** and **Mazidi and Nielsen (2014)** used rule-based methods relying on manual rules or templates.\n   - Neural QG models:\n     - **Du et al. (2017)** proposed a feature-rich encoder for the Seq2Seq model.\n     - **Zhou et al. (2017)** introduced a maxout pointer and gated self-attention for processing paragraph-level inputs.\n   - Reinforcement learning models for QG:\n     - **Zhang and Bansal (2019)** and **Chen et al. (2020)** applied reinforcement learning models to address the exposure bias problem.\n\n2. **Deep Question Generation (DQG)**:\n   - Inspired by multi-hop question answering:\n     - **Song et al. (2018)**, **Chen and Durrett (2019)**, and **Tu et al. (2020)** focused on multi-hop question answering.\n   - Pan et al. (2020) proposed a framework for DQG incorporating semantic graphs to enhance document representations and jointly train content selection and question decoding.\n   - Our work improves upon Pan et al. (2020) by:\n     - Utilizing the target answer as guidance for question generation.\n     - Introducing an answer-aware initialization module.\n     - Implementing a semantic-rich fusion attention mechanism.\n     - Applying reinforcement learning with syntactic and semantic metrics as the reward.",
 "fewshot_rankings": " 1. **Question Generation (QG) Overview**\n   - Reiter and Dale (2000), Saggion and Poibeau (2013), Balakrishnan et al. (2019): QG is a typical NLG task involving question creation from diverse sources.\n   - Traditional methods (Heilman and Smith, 2010; Mazidi and Nielsen, 2014): Relied on rule-based systems, which are costly and lack diversity.\n   - Neural QG models (Du et al., 2017; Zhou et al., 2017; Sun et al., 2018; Pan et al., 2019; Wang et al., 2019): Typically use an encoder-decoder framework, with some addressing the exposure bias issue through reinforcement learning (Zhang and Bansal, 2019; Chen et al., 2020).\n\n2. **Challenges in QG**\n   - Zhou et al. (2017): Proposed a feature-rich encoder for Seq2Seq models.\n   - Zhao et al. (2018): Processed paragraphs with maxout pointer and gated self-attention.\n   - Previous QG methods: Generated questions with limited deep comprehension, focusing on single-sentence facts.\n\n3. **Deep Question Generation (DQG)**\n   - Song et al. (2018), Chen and Durrett (2019), Tu et al. (2020): Inspired DQG, aiming to require multi-hop reasoning over documents.\n   - Pan et al. (2020): Introduced a framework using semantic graphs but did not focus on answer information crucial for question generation.\n   - Authors of the paper: Proposed an improved framework that utilizes the target answer for better question generation, with an answer-aware initialization module and semantic-rich fusion attention mechanism.\n   - Reinforcement learning: Applied to enhance training by integrating syntactic and semantic metrics.",
 "fewshot_explanation": " 1. **Question Generation in NLP**:\n   - Reiter and Dale (2000), Saggion and Poibeau (2013), Balakrishnan et al. (2019): Identified question generation as a typical NLP task.\n   - Traditional methods (Heilman and Smith, 2010; Mazidi and Nielsen, 2014): Relied on rule-based systems, which are costly and lack diversity.\n   - Neural QG models (Du et al., 2017; Zhou et al., 2017; Sun et al., 2018; Pan et al., 2019; Wang et al., 2019): Utilized encoder-decoder frameworks, with Zhou et al. (2017) proposing a feature-rich encoder and Zhao et al. (2018) using maxout pointer and gated self-attention.\n   - Reinforcement learning models (Zhang and Bansal, 2019; Chen et al., 2020): Addressed the exposure bias problem in QG.\n   - Pan et al. (2020): Introduced a framework using semantic graphs but did not focus on answer information for question generation.\n   - Authors of the paper: Proposed a DQG framework that uses semantic-rich fusion attention and answer-aware initialization to facilitate question generation with deep comprehension and reasoning.\n\n2. **Deep Question Generation (DQG)**:\n   - Song et al. (2018), Chen and Durrett (2019), Tu et al. (2020): Inspired the DQG task, which requires multi-hop reasoning over document-level contexts.\n   - Authors of the paper: Developed a DQG framework that leverages semantic graphs, answer-aware initialization, and reinforcement learning to generate complex questions with deep comprehension."
}