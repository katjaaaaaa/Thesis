{
 "oneshot": " 1. **Question Generation**:\n   - **Reiter and Dale (2000)**, **Saggion and Poibeau (2013)**, and **Balakrishnan et al. (2019)**: Question Generation is a typical natural language generation task.\n   - **Our work**: Most related to previous work on generating questions from texts.\n   - **Traditional methods**: Mostly rule-based, relying on manual rules or templates, and ranking generated questions by human-designed features.\n   - **Heilman and Smith (2010)**, **Mazidi and Nielsen (2014)**: Traditional methods are costly and lack diversity.\n   - **Neural QG models**: Usually variants of the encoder-decoder framework.\n   - **Du et al. (2017)**, **Zhou et al. (2017)**, **Sun et al. (2018)**, **Pan et al. (2019)**, **Wang et al. (2019)**: Proposed various neural QG models.\n   - **Zhou et al. (2017)**: Proposed a feature-rich encoder for the Seq2Seq model.\n   - **Zhao et al. (2018)**: Processed paragraph level inputs with maxout pointer and gated self-attention.\n   - **Reinforcement learning models**: Applied to deal with the \"exposure bias\" problem.\n   - **Zhang and Bansal (2019)**, **Chen et al. (2020)**: Applied reinforcement learning models.\n   - **Our work**: Focuses on generating deep questions with multi-hop reasoning over document-level contexts.\n   - **Pan et al. (2020)**: First to study the task of Deep Question Generation (DQG).\n   - **Our work**: Make better use of the target answer as guidance to facilitate question generation with the help of the answer-aware initialization module and semantic-rich fusion attention mechanism.\n   - **Our work**: Reinforcement learning which integrates both syntactic and semantic metrics as the reward is also applied to enhance the training process.",
 "fewshot_rankings": " 1. **Question Generation Overview**\n   - Reiter and Dale (2000), Saggion and Poibeau (2013), Balakrishnan et al. (2019): Defined question generation as a typical NLG task.\n\n2. **Traditional Methods**\n   - Heilman and Smith (2010), Mazidi and Nielsen (2014): Described traditional rule-based methods for question generation.\n\n3. **Neural QG Models**\n   - Du et al. (2017), Zhou et al. (2017), Sun et al. (2018), Pan et al. (2019), Wang et al. (2019): Introduced neural QG models based on the encoder-decoder framework.\n\n4. **Deep Question Generation (DQG)**\n   - Song et al. (2018), Chen and Durrett (2019), Tu et al. (2020): Inspired DQG with multi-hop reasoning for question answering.\n   - Authors of the paper: Proposed a new framework for DQG using semantic graphs and joint training for content selection and question decoding.\n   - Zhang and Bansal (2019), Chen et al. (2020): Applied reinforcement learning to address exposure bias in QG.\n   - Authors of the paper: Developed an answer-aware initialization module and semantic-rich fusion attention mechanism to improve question generation.\n\n5. **Contributions**\n   - Du et al. (2017): Proposed a feature-rich encoder for the Seq2Seq model.\n   - Zhou et al. (2017): Processed paragraph-level inputs with maxout pointer and gated self-attention.\n   - Authors of the paper: Aimed to address the exposure bias problem in QG.\n   - Authors of the paper: Enhanced DQG with semantic-rich fusion attention mechanism and answer-aware initialization module.",
 "fewshot_explanation": " 1. **Introduction to Question Generation**\n   - Reiter and Dale (2000), Saggion and Poibeau (2013), Balakrishnan et al. (2019): Question generation as a typical NLG task.\n\n2. **Traditional Methods vs. Neural Models**\n   - Heilman and Smith (2010), Mazidi and Nielsen (2014): Traditional rule-based methods vs. neural QG models.\n   - Du et al. (2017), Zhou et al. (2017), Sun et al. (2018), Pan et al. (2019), Wang et al. (2019): Variants of the encoder-decoder framework for QG.\n\n3. **Challenges in Question Generation**\n   - Zhou et al. (2017): Feature-rich encoder for Seq2Seq model.\n   - Zhao et al. (2018): Processing paragraphs with maxout pointer and gated self-attention.\n   - Zhang and Bansal (2019), Chen et al. (2020): Reinforcement learning to address exposure bias.\n\n4. **Deep Question Generation (DQG)**\n   - Song et al. (2018), Chen and Durrett (2019), Tu et al. (2020): Multi-hop question answering inspiration.\n   - Pan et al. (2020): First study of DQG with semantic graphs.\n   - Authors of the paper: Propose a new framework for DQG, utilizing semantic graphs and answer-aware mechanisms for improved question generation.\n\n5. **Contributions of the Paper**\n   - Authors of the paper: Developed a DQG framework that uses semantic graphs and answer-aware initialization for better question generation, addressing the limitations of previous methods."
}