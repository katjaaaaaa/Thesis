{
 "oneshot": " **Introduction to HappyDB**\n   - **Authors of the paper**: Created the first crowdsourced corpus of happy moments, HappyDB, for understanding language used to describe happy events.\n\n**Interest in Mental Health Datasets**\n   - **Althoff et al. (2016)**: Conducted a large-scale analysis on counseling conversational logs from SMS for mental illness study, exploring correlations between linguistic aspects and conversation outcomes.\n   - **Mihalcea et al. (2006)**: Performed text analysis on blog posts from LiveJournal, where posts can be tagged as happy or sad by authors.\n   - **Lin et al. (2016)**: Measured stress from short texts by identifying stressors and stress levels, classifying tweets into 12 stress categories defined by the stress scale in (Holmes and Rahe, 1967).\n\n**Recent Crowdsourced Research Efforts**\n   - **Rajpurkar et al. (2016)**: Created a large-scale dataset for question-answering, SQuAD, by crowdsourcing workers to create questions based on Wikipedia paragraphs.\n   - **Authors of the paper**: Unlike SQuAD, which was designed for studying the question-answering problem, the problems that HappyDB can be used for are more open-ended.",
 "fewshot_rankings": " 1. **Introduction to HappyDB**\n   - Authors of the paper: Proposed the first crowdsourced corpus of happy moments, HappyDB, for understanding language used in happy events.\n\n2. **Background and Need for HappyDB**\n   - Althoff et al. (2016): Conducted a large-scale analysis on counseling conversations from SMS to study linguistic aspects correlated with outcomes.\n   - Mihalcea et al. (2006): Analyzed blog posts from LiveJournal with happy/sad tags.\n   - Lin et al. (2016): Measured stress from short texts, classifying tweets into categories based on a stress scale.\n\n3. **Comparison with Other Datasets**\n   - Rajpurkar et al. (2016): Created SQuAD, a large-scale dataset for question-answering, using crowdsourced workers with high approval ratings.\n   - Authors of the paper: Highlighted that HappyDB differs from SQuAD in purpose and design, focusing on open-ended problems.\n\n4. **HappyDB Methodology**\n   - Authors of the paper: Selected workers without specific qualifications for HappyDB to avoid bias, aiming for a cognitively easier task than SQuAD.\n\n5. **HappyDB vs. SQuAD**\n   - Authors of the paper: Noted the similarity in scale between HappyDB and SQuAD, but emphasized the different applications and design intentions.",
 "fewshot_explanation": " 1. **Introduction to HappyDB**\n   - Authors of the paper: Created the first crowdsourced corpus of happy moments for studying language used in happy events.\n\n2. **Background and Need**\n   - Althoff et al. (2016): Conducted a large-scale analysis of counseling conversations to study linguistic aspects correlated with outcomes.\n   - Mihalcea et al. (2006): Analyzed blog posts from LiveJournal with happy/sad tags.\n   - Lin et al. (2016): Measured stress from short texts, classifying tweets into categories based on a stress scale.\n\n3. **Comparison with Other Datasets**\n   - Rajpurkar et al. (2016): Created SQuAD for question-answering, employing MTurk workers for dataset quality.\n   - Authors of the paper: Differentiated HappyDB from SQuAD by the nature of tasks and dataset selection process.\n\n4. **HappyDB Characteristics**\n   - Authors of the paper: Designed a corpus for open-ended applications, unlike SQuAD's question-answering focus.\n\n5. **Dataset Creation and Worker Selection**\n   - Authors of the paper: Aimed for a cognitively easier task than SQuAD, avoiding bias by not selecting workers based on qualifications.\n\n6. **HappyDB vs. SQuAD**\n   - Authors of the paper: Highlighted the open-ended potential of HappyDB compared to SQuAD's specific question-answering design."
}