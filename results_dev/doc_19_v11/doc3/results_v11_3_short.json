{
 "oneshot": ". **Multimodal Sentiment Analysis**:\n   - **Zadeh et al. (2017)** proposed Tensor Fusion for capturing interactions between unimodal, bimodal, and trimodal features.\n   - To address the high time cost of Tensor Fusion, **Liu et al. (2018)** introduced the Efficient Low-rank Multimodal Fusion method.\n   - **Mai et al. (2020)** proposed a graph fusion network to model interactions successively.\n\n2. **Utterance-Level Multimodal Feature Fusion**:\n   - This method uses overall utterance features, obtained by averaging frame-level visual or acoustic features.\n   - **Zadeh et al. (2018a)** proposed the Memory Fusion Network (MFN) for capturing interactions across modalities and timesteps.\n\n3. **Word-Level Multimodal Feature Fusion**:\n   - The first step in this method is applying force alignment to obtain timestamps for each word.\n   - **Wang et al. (2019)** proposed the Recurrent Attended Variation Embedding Network (RAVEN), which uses non-verbal features to shift word embeddings.\n   - **Tsai et al. (2019)** introduced the multimodal transformer (Mult) to capture bimodal interactions.\n   - **Pham et al. (2019)** proposed the Multimodal Cyclic Translation Network model (MCTN) for learning joint multimodal representations.\n   - The current study distinguishes shared and private non-textual features using a crossmodal prediction task, differing from MCTN.\n\n4. **Authors of the Paper**:\n   - The authors of the paper are not explicitly mentioned in the provided text.",
 "fewshot_rankings": ". **Multimodal Sentiment Analysis**:\n   - Focus on multimodal feature fusion at both utterance and word levels.\n   - **Zadeh et al. (2017)**: Proposed Tensor Fusion for capturing interactions across modalities and timesteps.\n   - **Liu et al. (2018)**: Introduced Efficient Low-rank Multimodal Fusion to accelerate feature fusion.\n   - **Mai et al. (2020)**: Proposed a graph fusion network for modeling interactions across modalities.\n   - **Zadeh et al. (2018a)**: Presented Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n   - **Wang et al. (2019)**: Proposed RAVEN, using Attention Gating to dynamically integrate non-verbal features.\n   - **Tsai et al. (2019)**: Presented Multimodal Transformer (Mult) to capture bimodal interactions using cross-modal attention.\n   - **Pham et al. (2019)**: Proposed MCTN for learning joint representations through cross-modal translation.\n   - Authors of the paper: Proposed a model using cross-modal prediction to distinguish shared and private features, enhancing the extraction of useful information.\n\n2. **Word-Level Feature Fusion**:\n   - Emphasis on capturing local information through word-level features.\n   - **Authors of the paper**: Utilized force alignment to obtain timestamps for words, splitting utterances into video clips, and averaging frame-level features for word-level features.\n   - **Authors of the paper**: Adopted methods like MFN and RAVEN for capturing interactions across modalities and timesteps, and Mult for leveraging bimodal interactions.\n\n   - **Vaswani et al. (2017)**: Provided the foundation for the success of transformer models in NLP, which influenced the development of Mult.",
 "fewshot_explanation": ". **Multimodal Sentiment Analysis**:\n   - Focus on multimodal feature fusion at the utterance and word levels.\n   - **Zadeh et al. (2017)**: Proposed Tensor Fusion for capturing interactions across modalities and timesteps.\n   - **Liu et al. (2018)**: Presented Efficient Low-rank Multimodal Fusion to accelerate feature fusion.\n   - **Mai et al. (2020)**: Proposed a graph fusion network for modeling interactions across modalities.\n   - **Zadeh et al. (2018a)**: Introduced Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n   - **Wang et al. (2019)**: Proposed RAVEN, using Attention Gating to dynamically integrate non-verbal features.\n   - **Tsai et al. (2019)**: Presented Mult, utilizing cross-modal attention for bimodal interactions.\n   - **Pham et al. (2019)**: Proposed MCTN, leveraging translation for learning joint representations.\n   - Authors of the paper: Proposed a model using crossmodal prediction to distinguish shared and private features, enhancing the extraction of useful information.\n\n2. **Word-Level Feature Fusion**:\n   - Emphasis on capturing local information through word-level features.\n   - **Authors of the paper**: Utilized force alignment to obtain timestamps, split utterances into video clips, and averaged frame-level features for word-level feature extraction.\n   - **Authors of the paper**: Adopted crossmodal prediction as the core task to differentiate shared and private features, providing deeper insights into cross-modal representations."
}