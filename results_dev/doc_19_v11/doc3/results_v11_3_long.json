{
 "oneshot": "*Multimodal Sentiment Analysis: Feature Fusion**\n\n- **Zadeh et al. (2017)**: Proposed Tensor Fusion to capture unimodal, bimodal, and trimodal interactions. However, this method uses the three-fold Cartesian product to fuse multimodal features, leading to high time costs.\n- **Liu et al. (2018)**: Presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.\n- **Mai et al. (2020)**: Proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.\n\n**Word-level Multimodal Feature Fusion**\n\n- **Zadeh et al. (2018a)**: Proposed the Memory Fusion Network (MFN) to capture interactions across different modalities and timesteps.\n- **Wang et al. (2019)**: Proposed the Recurrent Attended Variation Embedding Network (RAVEN), which applies the Attention Gating module to fuse word-level features dynamically.\n- **Tsai et al. (2019)**: Presented multimodal transformer (Mult) using cross-modal attention to capture bimodal interactions.\n- **Pham et al. (2019)**: Proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations through translation from source to target modality.\n\n**Differences in Feature Fusion Approaches**\n\n- The authors of the current paper propose a different approach to feature fusion. They use the crossmodal prediction task to distinguish shared and private non-textual features, rather than training the model as an auxiliary task. This approach allows them to obtain more useful information by deeply probing the cross-modal prediction model.",
 "fewshot_rankings": ". **Multimodal Sentiment Analysis**:\n   - **Zadeh et al. (2017)**: Proposed Tensor Fusion for capturing interactions at multiple modalities and timesteps.\n   - **Liu et al. (2018)**: Introduced Efficient Low-rank Multimodal Fusion to accelerate feature fusion.\n   - **Mai et al. (2020)**: Proposed a graph fusion network for modeling interactions at different modalities.\n   - **Zadeh et al. (2018a)**: Presented Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n   - **Wang et al. (2019)**: Proposed Recurrent Attended Variation Embedding Network (RAVEN) to dynamically use non-verbal features.\n   - **Tsai et al. (2019)**: Presented Multimodal Transformer (Mult) to capture bimodal interactions using cross-modal attention.\n   - **Pham et al. (2019)**: Proposed Multimodal Cyclic Translation Network (MCTN) for learning joint representations through translation tasks.\n   - **Authors of the paper**: Our work focuses on word-level feature fusion and uses crossmodal prediction to distinguish shared and private non-textual features, differing from MCTN's approach.\n\n2. **Feature Extraction**:\n   - **Authors of the paper**: Extracted word-level features using force alignment and timestamps, then averaged frame-level features from video clips.",
 "fewshot_explanation": ". **Multimodal Sentiment Analysis**:\n   - **Zadeh et al., 2017**: Proposed Tensor Fusion for capturing interactions at multiple modalities and timesteps.\n   - **Liu et al., 2018**: Introduced Efficient Low-rank Multimodal Fusion to accelerate feature fusion.\n   - **Mai et al., 2020**: Proposed a graph fusion network for modeling interactions at different modalities.\n   - **Zadeh et al., 2018a**: Presented Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n   - **Wang et al., 2019**: Proposed Recurrent Attended Variation Embedding Network (RAVEN) to dynamically use non-verbal features.\n   - **Tsai et al., 2019**: Presented Multimodal Transformer (Mult) to capture bimodal interactions using cross-modal attention.\n   - **Pham et al., 2019**: Proposed the Multimodal Cyclic Translation Network (MCTN) for learning joint representations through translation tasks.\n   - **Authors of the paper**: Our work focuses on word-level feature fusion, using force alignment to obtain timestamps, splitting utterances into video clips, and averaging frame-level features for multimodal sentiment analysis.\n\n2. **Contributions of the Paper**:\n   - **Authors of the paper**: Utilized word-level features, force alignment, video clips, and averaging for multimodal sentiment analysis, distinguishing shared and private features through crossmodal prediction tasks."
}