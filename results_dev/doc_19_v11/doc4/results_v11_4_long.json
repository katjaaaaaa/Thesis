{
 "oneshot": " 1. **Attention-based Sequence-to-Sequence Model**:\n   - Authors of the paper: Bahdanau et al., 2015\n\n2. **Coverage Modeling**:\n   - Tu et al., 2016: Maintain a coverage vector to indicate translated and untranslated source words.\n   - Mi et al., 2016: Update coverage vectors by accumulating attention probabilities.\n   - Authors of the paper: We extend the concept by manipulating the attention vector to model translated and untranslated source contents directly.\n   - Authors of the paper: We explicitly model both translated (with PAST-RNN) and untranslated (with FUTURERNN) contents, instead of using a single coverage vector.\n   - Authors of the paper: We feed PAST and FUTURE contents to the attention mechanism and the decoder's states.\n\n3. **Future Modeling**:\n   - Li et al., 2017: Employ actor-critic algorithms to predict future properties.\n   - Bahdanau et al., 2017: Use an interpolation of the actor and critic for decision making.\n   - Weng et al., 2017: Guide the decoder's hidden states to predict untranslated target words.\n   - Authors of the paper: We introduce a FUTURE layer to maintain untranslated source contents, updated by subtracting the source content being translated.\n\n4. **Functionality Separation**:\n   - Reed and Freitas, 2015: Separate representations to alleviate model training difficulties.\n   - Ba et al., 2016;Miller et al., 2016;Gulcehre et al., 2016;Rocktaschel et al., 2017: Separate functionality of look-up keys and memory contents.\n   - Authors of the paper: We further separate PAST and FUTURE functionalities from the decoder's hidden representations.",
 "fewshot_rankings": " 1. **Attention-based Sequence-to-Sequence Model**\n   - Authors of the paper: Developed upon Bahdanau et al.'s model (2015), with additional features for coverage modeling, future modeling, and functionality separation.\n\n2. **Coverage Modeling**\n   - Tu et al. (2016), Mi et al. (2016): Maintained coverage vectors to track translated and untranslated source words.\n   - Authors of the paper: Enhanced the model by manipulating the attention vector to distinguish between translated and untranslated source words and by using separate layers for translated and untranslated contents.\n\n3. **Future Modeling**\n   - Li et al. (2017), Bahdanau et al. (2017): Utilized actor-critic algorithms to predict future properties of the target sentence.\n   - Weng et al. (2017): Guided the decoder to predict untranslated target words at each step.\n   - Authors of the paper: Introduced a FUTURE layer to maintain untranslated source contents and update it by subtracting translated source content at each decoding step.\n\n4. **Functionality Separation**\n   - Reed and Freitas (2015), Ba et al. (2016), Miller et al. (2016), Rocktaschel et al. (2017): Proposed methods to separate overloaded representations for easier model training.\n   - Authors of the paper: Further separated PAST and FUTURE functionalities from the decoder's hidden representations.",
 "fewshot_explanation": " 1. **Attention-based Sequence-to-Sequence Model**\n   - Authors of the paper: Developed upon Bahdanau et al.'s model (2015), with additional features for coverage modeling, future modeling, and functionality separation.\n\n2. **Coverage Modeling**\n   - Tu et al., 2016: Maintained a coverage vector to track translated and untranslated source words.\n   - Mi et al., 2016: Similar approach to track source words.\n   - Authors of the paper: Proposed a method to model translated and untranslated contents directly through attention vectors.\n   - Authors of the paper: Introduced separate layers for translated and untranslated contents, differing from Tu et al. (2016) by feeding these to the decoder's states.\n\n3. **Future Modeling**\n   - Li et al., 2017; Bahdanau et al., 2017: Used actor-critic algorithms to predict future properties of the target sentence.\n   - Weng et al., 2017: Guided the decoder to predict untranslated target words.\n   - Authors of the paper: Introduced a FUTURE layer to maintain untranslated source contents for future modeling.\n\n4. **Functionality Separation**\n   - Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rocktaschel et al., 2017: Advocated for separating overloaded representations.\n   - Authors of the paper: Further separated PAST and FUTURE functionalities from the decoder's hidden representations."
}