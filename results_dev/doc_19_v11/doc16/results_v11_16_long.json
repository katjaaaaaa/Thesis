{
 "oneshot": " 1. **Noisy Channel Models (NCMs)**:\n   - **Johnson and Charniak (2004)**: NCMs use complex tree adjoining grammar (TAG) based channel models to find \"rough copy\" dependencies between words.\n   - **Johnson et al. (2004)**: NCMs generate n-best disfluency analyses for each sentence at test time using probabilities from TAG channel model and a bigram language model (LM).\n   - **Johnson et al. (2004)**: NCMs are reranked using a language model sensitive to the global properties of the sentence, such as a syntactic parser based LM.\n   - **Johnson et al. (2004)**: Rescoring the n-best analyses with external n-gram and deep learning LMs improves the performance of the baseline NCM.\n\n2. **Parsing-based Approaches**:\n   - **Rasooli and Tetreault (2013)**: Augment a transition-based dependency parser with a new action to detect and remove disfluent parts of the sentence and their dependencies from the stack.\n   - **Honnibal and Johnson (2014)**: Detect disfluencies while simultaneously identifying the syntactic structure of the sentence.\n   - **Yoshikawa et al. (2016)**: Joint parsing and disfluency detection can compare favorably to pipelined approaches.\n\n3. **Sequence Tagging Approaches**:\n   - **Liu et al. (2006)**: Use conditional random fields (CRFs) for sequence tagging.\n   - **Ostendorf and Hahn (2013)**: Extend the baseline state space with new explicit repair states to consider the words at repair region.\n   - **Zayats et al. (2014, 2016)**: Use a model with begin-inside-outside (BIO) style states to label individual words as fluent or disfluent.\n   - **Georgila (2009)**: Apply integer linear programming (ILP) constraints to the output of classifier to avoid inconsistencies between neighboring labels.\n   - **Zayats et al. (2016)**: Investigate the performance of a bidirectional long-short term memory network (BLSTM) for disfluency detection.\n   - **Fergus0.000000, ando00000,0,00.00.00, and00.00,000000000000.0.000, and0.00,000000000000000.000.or [0000.0.0000,\no,\no.00000000000oidental0.00.00.0000.0.00.0.000.000.0000000oidentic0, or, or.0.\niscote\n\n000identic,0.0, and000,\n,00.000000.0.00.0.0,0000000000.0000000000.0.0enative,00.0000.0.0othal00000andothic0000.0.",
 "fewshot_rankings": " 1. **Noisy Channel Models (NCMs) for Disfluency Detection**:\n   - Johnson and Charniak (2004), Shieber and Schabes (1990): Utilized TAG-based channel models to identify dependencies between words.\n   - Johnson et al. (2004): Combined TAG channel model with a bigram LM for n-best disfluency analyses and reranking with a syntactic parser-based LM.\n   - Zwarts and Johnson (2011), Jamshid Lou and Johnson (2017): Enhanced NCMs with external n-gram LMs and deep learning LMs for improved performance.\n\n2. **Parsing-Based Approaches**:\n   - Rasooli and Tetreault (2013), Honnibal and Johnson (2014), Yoshikawa et al. (2016): Detected disfluencies and identified syntactic structures simultaneously, requiring large annotated treebanks.\n\n3. **Sequence Tagging Approaches**:\n   - Liu et al. (2006, 2010), Ostendorf and Hahn (2013), Zayats et al. (2014, 2016): Employed various models like CRFs, HMMs, and deep learning models for labeling words as fluent or disfluent.\n   - Georgila (2009), Georgila et al. (2010): Applied ILP constraints to resolve label inconsistencies.\n   - Authors of the paper: Proposed an ACNN for sequence tagging, labeling words directly as fluent or disfluent without post-processing.\n   - Zayats et al. (2016): Investigated BLSTM for disfluency detection, which underperformed compared to models with hand-crafted features.\n   - Authors of the paper: Aimed to overcome architectural limitations by capturing dependencies without manual features.\n\n4. **Prosodic Information**:\n   - Kahn et al. (2005), Ferguson et al. (2015), Tran et al. (2018): Incorporated prosodic information for disfluency detection.\n\n   - Authors of the paper: Focused on addressing limitations of deep neural networks in learning features from words alone.",
 "fewshot_explanation": " 1. **Noisy Channel Models for Disfluency Detection**\n   - Johnson and Charniak (2004), Shieber and Schabes (1990): Utilized TAG-based channel models to identify dependencies between words.\n   - Johnson et al. (2004): Developed n-best disfluency analyses using TAG channel models and LMs, later reranked by a syntactic parser-based LM.\n   - Zwarts and Johnson (2011), Jamshid Lou and Johnson (2017): Enhanced NCMs with external n-gram LMs and deep learning LMs, leading to complex runtime dependencies.\n\n2. **Parsing-Based Approaches**\n   - Rasooli and Tetreault (2013), Honnibal and Johnson (2014), Yoshikawa et al. (2016): Parsing-based approaches identified disfluencies and syntactic structures, requiring large annotated treebanks.\n\n3. **Sequence Tagging Approaches**\n   - Liu et al. (2006, 2010), Ostendorf and Hahn (2013), Zayats et al. (2014, 2016): Employed various models like CRFs, HMMs, and deep learning models for sequence tagging, often extending state space for pattern matching.\n   - Georgila et al. (2009, 2010), Zayats et al. (2016): Addressed labeling inconsistencies with ILP constraints.\n\n4. **Proposed Approach**\n   - Authors of the paper: Proposed an autocorrelational neural network (ACNN) for sequence tagging, labeling words directly as fluent or disfluent without post-processing or annotation modifications.\n   - Contrast with Zayats et al. (2016): Our model does not rely on hand-crafted features or representations from dependency parsers, unlike the BLSTM model which underperformed when augmented with additional features.\n\n5. **Incorporation of Prosodic Information**\n   - Kahn et al. (2005), Ferguson et al. (2015), Tran et al. (2018): Included prosodic information in disfluency detection, unlike the focus of the current work on architectural limitations."
}