{
 "oneshot": " 1. **Approaches to Disfluency Detection Task**:\n   - **Noisy Channel Models (NCMs)**:\n     - **Johnson and Charniak (2004)** and **Johnson et al. (2004)**: NCMs use complex tree adjoining grammar (TAG) based channel models to find \"rough copy\" dependencies between words.\n     - **Shieber and Schabes (1990)**: Provided the basis for TAG.\n     - **Zwarts and Johnson (2011)**: Showed that rescoring the n-best analyses with external n-gram LMs improves the performance of the baseline NCM.\n     - **Jamshid Lou and Johnson (2017)**: Demonstrated that deep learning LMs trained on large speech and non-speech corpora improve the performance of the baseline NCM.\n   - **Parsing-based Approaches**:\n     - **Rasooli and Tetreault (2013)**: Augmented a transition-based dependency parser to detect and remove disfluent parts of the sentence and their dependencies.\n     - **Honnibal and Johnson (2014)**: Augmented a transition-based dependency parser to detect and remove disfluent parts of the sentence and their dependencies.\n     - **Yoshikawa et al. (2016)**: Augmented a transition-based dependency parser to detect and remove disfluent parts of the sentence and their dependencies.\n   - **Sequence Tagging Approaches**:\n     - **Liu et al. (2006)**, **Ostendorf and Hahn (2013)**, **Zayats et al. (2014, 2016)**, **Ferguson et al. (2015)**, **Hough and Schlangen (2015)**, **Georgila (2009)**, **Georgila et al. (2010)**, **Zayats et al. (2016)**: These approaches use classification techniques such as conditional random fields, hidden Markov models, and deep learning based models to label individual words as fluent or disfluent.\n     - **Ostendorf and Hahn (2013)**, **Zayats et al. (2014, 2016)**: Extended the baseline state space with new explicit repair states to consider the words at repair region.\n     - **Georgila et al. (2010)**, **Zayats et al. (2016)**: Applied integer linear programming (ILP) constraints to the output of classifier to avoid inconsistencies between neighboring labels.\n   - **Proposed Approach**:\n     - **Zayats et al. (2016)**: Investigated the performance of a bidirectional long-short term memory network (BLSTM) for disfluency detection.\n     - **Our Proposed Approach**: Based on an autocorrelational neural network (ACNN), it belongs to the class of sequence tagging approaches and directly labels words as being fluent or disfluent without requiring any post-processing or annotation modifications.",
 "fewshot_rankings": " 1. **Noisy Channel Models (NCMs) for Disfluency Detection**:\n   - **Johnson and Charniak (2004)**: Utilized TAG-based channel models for disfluency detection.\n   - **Johnson et al. (2004)**: Combined TAG channel model with a bigram LM for n-best disfluency analyses.\n   - **Zwarts and Johnson (2011)**: Rescored n-best analyses with external n-gram LMs to improve NCM performance.\n   - **Jamshid Lou and Johnson (2017)**: Employed deep learning LMs to enhance NCMs.\n\n2. **Parsing-Based Approaches**:\n   - Detect disfluencies and identify syntactic structure simultaneously.\n   - **Rasooli and Tetreault (2013)**: Augmented a transition-based parser to remove disfluent parts.\n   - **Honnibal and Johnson (2014)**: Similar approach to parsing and disfluency detection.\n   - **Yoshikawa et al. (2016)**: Parsing-based approach for disfluency detection.\n\n3. **Sequence Tagging Approaches**:\n   - **Liu et al. (2006)**, **Ostendorf and Hahn (2013)**, **Zayats et al. (2014)**, **Ferguson et al. (2015)**: Used various models like CRFs, HMMs, and deep learning models for sequence tagging.\n   - **Georgila (2009)**, **Georgila et al. (2010)**: Applied ILP constraints to resolve label inconsistencies.\n   - **Zayats et al. (2016)**: Investigated BLSTM for disfluency detection, found it underperformed with hand-crafted features.\n\n4. **Proposed Approach**:\n   - **Authors of the paper**: Proposed an ACNN for sequence tagging, labeling words directly as fluent or disfluent without post-processing.\n   - **Contrast with Zayats et al. (2016)**: Differentiated from BLSTM by avoiding manual features and aiming to capture dependencies automatically.",
 "fewshot_explanation": " 1. **Noisy Channel Models (NCMs) for Disfluency Detection**:\n   - **Johnson and Charniak (2004)**: Introduced NCMs using TAG-based channel models.\n   - **Johnson et al. (2004)**: Described the process of generating n-best disfluency analyses and reranking them.\n   - **Zwarts and Johnson (2011)**: Showed improvement in NCM performance using external n-gram LMs.\n   - **Jamshid Lou and Johnson (2017)**: Demonstrated further enhancement with deep learning LMs.\n\n2. **Parsing-Based Approaches**:\n   - **Rasooli and Tetreault (2013)**: Detected disfluencies while identifying syntactic structures.\n   - **Honnibal and Johnson (2014)**: Augmented parsers to remove disfluent parts and their dependencies.\n   - **Yoshikawa et al. (2016)**: Achieved comparable performance to pipelined approaches through joint parsing and disfluency detection.\n\n3. **Sequence Tagging Approaches**:\n   - **Liu et al. (2006)**, **Ostendorf and Hahn (2013)**, **Zayats et al. (2014)**, **Ferguson et al. (2015)**: Utilized various models like CRFs, HMMs, and deep learning models for sequence tagging.\n   - **Georgila (2009)**, **Georgila et al. (2010)**, **Zayats et al. (2016)**: Addressed labeling scheme issues with ILP constraints.\n\n4. **Proposed Approach**:\n   - The paper introduces an autocorrelational neural network (ACNN) for disfluency detection.\n   - It contrasts with previous work by not requiring manual features or augmentation with pattern match features.\n   - The approach directly labels words as fluent or disfluent.\n   - Reference to similar work: **Zayats et al. (2016)** for BLSTM performance on disfluency detection.\n\n5. **Incorporation of Prosodic Information**:\n   - Some previous works used prosodic information (Kahn et al., 2005;Ferguson et al., 2015;Tran et al., 2018).\n   - The paper's approach aims to overcome architectural limitations of deep neural networks."
}