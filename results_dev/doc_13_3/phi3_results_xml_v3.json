{
 "zeroshot": "\n<summary>\n- Focus on utterance-level multimodal feature fusion: \n    - Authors of the paper: focus on utterance-level multimodal feature fusion\n- Focus on word-level multimodal feature fusion: \n    - Authors of the paper: focus on word-level multimodal feature fusion\n- Zadeh et al. (2017): proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions.\n- Liu et al. (2018): presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.\n- Mai et al. (2020): proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.\n- Zadeh et al. (2018a): proposed the Memory Fusion Network(MFN) to capture the interactions across both different modalities and timesteps.\n- Wang et al. (2019): proposed the Recurrent Attended Variation Embedding Network (RAVEN).\n- Tsai et al. (2019): presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions.\n- Pham et al. (2019): proposed that translation from a source to a target modality provides away to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN).\n- Authors of the paper: proposed a model using crossmodal prediction task to distinguish the shared and private non-textual features and obtain more useful information by deeply probing the cross-modal prediction model.\n</summary>\n<|solution|>\n- Focus on utterance-level multimodal feature fusion: \n    - Authors of the paper: focus on utterance-level multimodal feature fusion\n- Focus on word-level multimodal feature fusion: \n    - Authors of the paper: focus on word-level multimodal feature fusion\n- Zadeh et al. (2017): proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions.\n- Liu et al. (2018): presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.\n- Mai et al. (2020): proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.\n- Zadeh et al. (2018a): proposed the Memory Fusion Network(MFN) to capture the interactions across both different modalities and timesteps.\n- Wang et al. (2019): proposed the Recurrent Attended Variation Embedding Network (RAVEN).\n- Tsai et al. (2019): presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions.\n- Pham et al. (2019): proposed that translation from a source to a target modality provides away to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN).\n- Authors of the paper: proposed a model using crossmodal prediction task to distinguish the shared and private non-textual features and obtain more useful information by deeply probing the cross-modal prediction model.\n</solution>\n<|user\n- assistant: The summary accurately captures the key concepts and contributions from the text, including the focus on both utterance-level and word-level multimodal feature fusion, as well as the contributions from various authors mentioned in the text. It also correctly identifies the authors of the paper in question and their specific contribution to the field of multimodal sentiment analysis. The summary is structured to reflect the hierarchy and relationships between the different works and the paper being summarized.",
 "oneshot": "\n\n<summary of the text>\n\n1. **Works on Multimodal Sentiment Analysis**\n\t- **Authors of the paper**: The authors of the paper the text was taken from.\n\t- **Contribution**: Discussing two lines of works on multimodal sentiment analysis, focusing on utterance-level and word-level multimodal feature fusion.\n\n2. **Utterance-Level Multimodal Feature Fusion**\n\t- **Authors of the paper**: The authors of the paper the text was taken from.\n\t- **Contribution**: Describing methods that use overall utterance features, including the extraction of frame-level visual or acoustic features and the use of RNNs for textual features.\n\n3. **Zadeh et al. (2017)**\n\t- **Authors**: Zadeh et al.\n\t- **Contribution**: Proposing Tensor Fusion to capture unimodal, bimodal, and trimodal interactions.\n\n4. **Liu et al. (2018)**\n\t- **Authors**: Liu et al.\n\t- **Contribution**: Presenting the Efficient Low-rank Multimodal Fusion to accelerate the fusion process using low-rank tensors.\n\n5. **Mai et al. (2020)**\n\t- **Authors**: Mai et al.\n\t- **Contribution**: Proposing a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.\n\n6. **Word-Level Multimodal Feature Fusion**\n\t- **Authors of the paper**: The authors of the paper the text was taken from.\n\t- **Contribution**: Discussing methods based on word-level features, including force alignment, splitting utterances into video clips, and averaging frame-level features.\n\n7. **Zadeh et al. (2018a)**\n\t- **Authors**: Zadeh et al.\n\t- **Contribution**: Proposing the Memory Fusion Network to capture interactions across modalities and timesteps.\n\n8. **Wang et al. (2019)**\n\t- **Authors**: Wang et al.\n\t- **Contribution**: Proposing the Recurrent Attended Variation Embedding Network to dynamically use non-verbal features to shift word embeddings.\n\n9. **Tsai et al. (2019)**\n\t- **Authors**: Tsai et al.\n\t- **Contribution**: Presenting multimodal transformer to capture bimodal interactions using cross-modal attention.\n\n10. **Pham et al. (2019)**\n\t- **Authors**: Pham et al.\n\t- **Contribution**: Proposing the Multimodal Cyclic Translation Network model to learn joint multimodal representations through translation from source to target modality.\n\n11. **Differences in the Paper's Model**\n\t- **Authors of the paper**: The authors of the paper the text was taken from.\n\t- **Contribution**: Highlighting the significant difference in the proposed model, which uses crossmodal prediction to distinguish shared and private non-textual features, providing more useful information.\n</summary of the text>\n\n<summary of the example article>\n\n1. **HappyDB: Crowdsourced Corpus of Happy Moments**\n\t- **Authors of the paper**: The authors of the paper the text was taken from.\n\t- **Contribution**: Creating the first crowdsourced corpus of happy moments, which can be used to understand the language people use to describe happy events.\n\n2. **Interest in Mental Health Datasets**\n\t- **Authors of the paper**: Not explicitly mentioned.\n\t- **Contribution**: Highlighting recent interest in creating datasets related to mental health.\n\n3. **Althoff et al. (2016)**\n\t- **Authors**: Althoff et al.\n\t- **Contribution**: Conducting a large-scale analysis on counseling conversational logs collected from SMS for mental illness study.\n\n4. **Mi",
 "fewshot_rankings": "\n\n<example article summary>\n**Key Concepts:**\n\n1. Multimodal Sentiment Analysis: Two lines of works, focusing on utterance-level and word-level multimodal feature fusion.\n2. Utterance-level Feature Fusion: Methods use features of the overall utterance, averaging frame-level visual or acoustic features.\n3. Word-level Feature Fusion: Recent works focus on capturing local information by extracting word-level features and applying various fusion methods.\n4. Zadeh et al. (2017): Proposed Tensor Fusion to capture interactions at different levels.\n5. Liu et al. (2018): Presented Efficient Low-rank Multimodal Fusion to accelerate the fusion process.\n6. Mai et al. (2020): Proposed a graph fusion network to model interactions successively.\n7. Liu et al. (2018): Proposed the Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n8. Wang et al. (2019): Proposed the Recurrent Attended Variation Embedding Network (RAVEN) to dynamically use non-verbal features.\n9. Tsai et al. (2019): Presented multimodal transformer (Mult) to capture bimodal interactions using cross-modal attention.\n10. Pham et al. (2019): Proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations through translation tasks.\n11. Authors of the paper: The paper's authors propose a model using crossmodal prediction task to distinguish shared and private non-textual features.\n\n**Contributions:**\n\n1. Zadeh et al. (2017): Proposed Tensor Fusion for capturing interactions at different levels.\n2. Liu et al. (2018): Presented Efficient Low-rank Multimodal Fusion to accelerate the fusion process.\n3. Mai et al. (2020): Proposed a graph fusion network to model interactions successively.\n4. Liu et al. (2018): Proposed the Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n5. Wang et al. (2019): Proposed the Recurrent Attended Variation Embedding Network (RAVEN) to dynamically use non-verbal features.\n6. Tsai et al. (2019): Presented multimodal transformer (Mult) to capture bimodal interactions using cross-modal attention.\n7. Authors of the paper: Proposed a model using crossmodal prediction task to distinguish shared and private non-textual features.\n</example article summary>\n\n<rankings example article summary>\nQuality structure: 1/5\nQuality summarization: 5/5\nQuality sources citation: 4/5\n</rankings example article\n\n\n\n\n\n\n\n\n0000\n\n\n00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n>0s (\n>\n\n>\n0)\n0\n\n\n\n\n\npartic0on0s\n\ns (\n\n\n00s\n\n\n\n\n\n\n\n\n\n\n\n0-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n>\n>\n\n>00s\n\n\n>\n>\n\non000\n\n\n\n0s0\n\n000s (\n\n\n\n>\n\n\n\n00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00s\n\n\n\n\n\n\n\n>\n\n\n\n\n\n\n\n>\n\n\n\n\n> (\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n>0it0on0>\n\n\n\n\n\n>\n\n\n\n\n\n\n\n\n\n\n>\n>\n>\n\n>\n>\n\n\n\n>\n\n\n\n\n\n\n\n\n\n\n\n>\n\n>\n\n>\n>\n>0>>>>",
 "fewshot_explanation": "\n\n<example article summary 4>\n**Key Concepts:**\n\n1. Multimodal Sentiment Analysis\n   - **Authors**: Not mentioned in the excerpt\n   - **Contribution**: The paper discusses two lines of works in multimodal sentiment analysis, focusing on utterance-level and word-level feature fusion.\n\n2. Utterance-level Feature Fusion\n   - **Authors**: Althoff et al. (2016), Liu et al. (2018), Mai et al. (2020), Zadeh et al. (2017), Zadeh et al. (2018a)\n   - **Contribution**: Methods using features of the overall utterance, including fusion models like Tensor Fusion, Efficient Low-rank Multimodal Fusion, and Memory Fusion Network.\n\n3. Word-level Feature Fusion\n   - **Authors**: Liu et al. (2018), Wang et al. (2019), Tsai et al. (2019), Pham et al. (2019)\n   - **Contribution**: Methods focusing on word-level features, including force alignment, timestamps, video clips, and fusion models like Recurrent Attended Variation Embedding Network, multimodal transformer, and Multimodal Cyclic Translation Network.\n\n4. Crossmodal Prediction Task\n   - **Authors**: Not mentioned in the excerpt\n   - **Contribution**: The paper proposes using the crossmodal prediction task to distinguish shared and private non-textual features, obtaining more useful information by deeply probing the cross-modal prediction model.\n\n5. Related Work: SQuAD Dataset\n   - **Authors**: Rajpurkar et al. (2016)\n   - **Contribution**: Created a large-scale crowdsourced dataset (SQuAD) for question answering, which is used as a comparison to HappyDB in the paper.\n\n6. HappyDB vs. SQuAD\n   - **Authors**: The authors of the paper the text was taken from\n   - **Contribution**: HappyDB, similar in scale to SQuAD, focuses on open-ended explative,\native, ,,\n\n\n\n,\n0,\n\n\n\n\n\n\nur,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2,0, and\n\n20ur\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n, **\n\n\n\n\n, (\n0 **, topic **\n\n\n\n\n\n\n\n\n,\n\n\n\n\n\n\n\n\n\n\n0,0ur,\n\n\n\n\n\n\n0, **0 **\n\n\n\n\n\n\new0000\n\n\n000 **000\n\n\n\n\n\n\n\n\n\n00000,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0,00 0000 00,0,0 sub000000000000 0urative,\n\n, topic\n0,00 \n\n0 0  0 00 \n\n\nw\n0 \n\n\n\n  00 00 \n  000 in\n0  0000 \n\n\n\n \n \n\n\n0 0 \n\n\n\n 0000 (\n  000 (\n  0 (  \n  \n  \n  \n   ( \n000000000\n\n0,0000 \n000 ativing overv0,0,00, over0 0,\n  \n   , ( ( ( (\n\n\n\n\n\n0 ( (  (\n\n\n\n  \n\n,0 \n\n\n\n\n\n\n\n\n\n\n,0 \n\n\n\n\n\n\n,\n\n\n\n\n0,\n\n\n\n\n\n\nited,\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
}