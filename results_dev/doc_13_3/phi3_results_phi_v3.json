{
 "zeroshot": "ey Concepts:\n\n1. Multimodal Sentiment Analysis\n   - Authors of the paper: Focuses on two lines of works conducted on multimodal sentiment analysis, one focusing on utterance-level multimodal feature fusion and the other on word-level multimodal feature fusion.\n\n2. Utterance-level Multimodal Feature Fusion\n   - Authors of the paper: Uses features of the overall utterance, extracts frame-level visual or acoustic features, and averages them to obtain the final features.\n   - Zadeh et al. (2017): Proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions.\n   - Liu et al. (2018): Presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.\n   - Mai et al. (2020): Proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.\n\n3. Word-level Multimodal Feature Fusion\n   - Authors of the paper: Based on word-level features, applies force alignment to obtain timestamps of each word, splits the utterance into video clips, and obtains word-level visual or acoustic features by averaging the frame-level features of the video clips.\n   - Zadeh et al. (2018a): Proposed the Memory Fusion Network (MFN) to capture interactions across different modalities and timesteps.\n   - Wang et al. (2019): Proposed the Recurrent Attended Variation Embedding Network (RAVEN), which applies the Attention Gating module to fuse the word-level features.\n   - Tsai et al. (2019): Presented multimodal transformer (Mult), which uses cross-modal attention to capture bimodal interactions.\n   - Pham et al. (2019): Proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations.\n   - Authors of the paper: Proposed a model that uses crossmodal prediction task to distinguish shared and private non-textual features, obtaining more useful information by deeply probing the cross-modal prediction model.",
 "oneshot": ". **Multimodal Sentiment Analysis: Utterance-Level Feature Fusion**\n\t- **Authors**: The authors of the paper the text was taken from.\n\t- **Contribution**: Focus on methods using utterance-level multimodal feature fusion, including feature extraction, fusion model application, and the use of RNNs for textual features.\n\n2. **Zadeh et al. (2017)**\n\t- **Authors**: Zadeh et al.\n\t- **Contribution**: Proposed Tensor Fusion for capturing interactions across modalities, though noted the high time cost due to the Cartesian product method.\n\n3. **Liu et al. (2018)**\n\t- **Authors**: Liu et al.\n\t- **Contribution**: Presented the Efficient Low-rank Multimodal Fusion method, which uses low-rank tensors to accelerate the fusion process.\n\n4. **Mai et al. (2020)**\n\t- **Authors**: Mai et al.\n\t- **Contribution**: Proposed a graph fusion network to model interactions across modalities successively.\n\n5. **Word-Level Multimodal Feature Fusion**\n\t- **Authors**: The authors of the paper the text was taken from.\n\t- **Contribution**: Shifted focus to word-level multimodal feature fusion, including the extraction of word-level features, splitting utterances into video clips, and obtaining word-level features by averaging frame-level features.\n\n6. **Zadeh et al. (2018a)**\n\t- **Authors**: Zadeh et al.\n\t- **Contribution**: Proposed the Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n\n7. **Wang et al. (2019)**\n\t- **Authors**: Wang et al.\n\t- **Contribution**: Proposed the Recurrent Attended Variation Embedding Network (RAVEN), which uses the Attention Gating module to dynamically fuse word-level features.\n\n8. **Tsai et al. (2019)**\n\t- **Authors**: Tsai et al.\n\t- **Contribution**: Presented the multimodal transformer (Mult) model, which uses cross-modal attention to capture bimodal interactions.\n\n9. **Pham et al. (2019)**\n\t- **Authors**: Pham et al.\n\t- **Contribution**: Proposed the Multimodal Cyclic Translation Network (MCTN) model for learning joint multimodal representations through translation tasks.\n\n10. **Comparative Difference in Model Approach**\n\t- **Authors**: The authors of the paper the text was taken from.\n\t- **Contribution**: Highlighted the difference in approach compared to Pham et al. (2019), using crossmodal prediction tasks to distinguish shared and private non-textual features for obtaining more useful information.",
 "fewshot_rankings": "# Key Concepts in Multimodal Sentiment Analysis\n\n**1. Multimodal Sentiment Analysis Approaches**\n\n* Authors: Not specified in the excerpt\n* Contribution: Discusses two main approaches in multimodal sentiment analysis: utterance-level and word-level feature fusion.\n\n**2. Utterance-Level Multimodal Feature Fusion**\n\n* Authors: Zadeh et al. (2017)\n* Contribution: Proposed Tensor Fusion for capturing interactions across modalities, though noted for high time costs due to Cartesian product usage.\n\n* Authors: Liu et al. (2018)\n* Contribution: Introduced Efficient Low-rank Multimodal Fusion to accelerate feature fusion processes.\n\n* Authors: Mai et al. (2020)\n* Contribution: Proposed a graph fusion network for modeling interactions across modalities.\n\n**3. Word-Level Multimodal Feature Fusion**\n\n* Authors: Zadeh et al. (2018a)\n* Contribution: Proposed Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n\n* Authors: Wang et al. (2019)\n* Contribution: Proposed Recurrent Attended Variation Embedding Network (RAVEN) to dynamically use non-verbal features for shifting word embeddings.\n\n* Authors: Vaswani et al. (2017)\n* Contribution: Introduced transformers in NLP, inspiring subsequent multimodal transformer models.\n\n* Authors: Pham et al. (2019)\n* Contribution: Proposed Multimodal Cyclic Translation Network (MCTN) for learning joint multimodal representations through translation tasks.\n\n* Authors of the paper: The paper's authors\n* Contribution: The authors propose a model that uses crossmodal prediction to distinguish shared and private non-textual features, aiming to deeply probe the cross-modal prediction model for more useful information.\n\n**4. Methodology for Feature Extraction**\n\n* Authors of the paper: The authors\n* Contribution: Outlines a method for extracting word-level features by applying force alignment to obtain timestamps and splitting utterances into video clips for feature extraction.\n\n* Authors: Not specified in the excerpt\n* Contribution: The paper details a process for obtaining word-level features from video clips, which is a focus for recent sentiment analysis methods.",
 "fewshot_explanation": "# Key Concepts in Multimodal Sentiment Analysis\n\n**1. Multimodal Feature Fusion Methods**\n\n* Authors: Zadeh et al. (2017)\n* Contribution: Proposed Tensor Fusion to capture interactions across modalities, though noted for high time cost due to Cartesian product usage.\n\n* Authors: Liu et al. (2018)\n* Contribution: Introduced Efficient Low-rank Multimodal Fusion to accelerate the fusion process using low-rank tensors.\n\n* Authors: Mai et al. (2020)\n* Contribution: Proposed a graph fusion network for modeling interactions across modalities.\n\n**2. Word-Level Feature Fusion**\n\n* Authors: Zadeh et al. (2018a)\n* Contribution: Proposed Memory Fusion Network (MFN) to capture interactions across modalities and timesteps.\n\n* Authors: Wang et al. (2019)\n* Contribution: Proposed Recurrent Attended Variation Embedding Network (RAVEN) to dynamically use non-verbal features for shifting word embeddings.\n\n* Authors: Vaswani et al. (2017)\n* Contribution: Introduced transformers in NLP, which inspired subsequent multimodal fusion methods.\n\n* Authors: Pham et al. (2019)\n* Contribution: Proposed the Multimodal Cyclic Translation Network (MCTN) for learning joint representations through translation tasks.\n\n* Authors of the paper: The authors of the paper\n* Contribution: The paper presents a model that uses crossmodal prediction to distinguish shared and private non-textual features, obtaining more useful information by deeply probing the cross-modal prediction model.\n\n**3. Limitations of Global Utterance-Level Features**\n\n* Authors of the paper\n* Contribution: The paper notes that utterance-level features may fail to capture local information, leading to a focus on word-level feature fusion.\n\n**4. Methodology for Word-Level Feature Extraction**\n\n* Authors of the paper\n* Contribution: The paper details a method involving force alignment to obtain timestamps for words, splitting utterances into video clips, and averaging frame-level features to extract word-level features.\n\n**5. Comparison with Related Work**\n\n* Authors of the paper\n* Contribution: The paper highlights a significant difference in their approach compared to Pham et al. (2019), using crossmodal prediction as a task to distinguish features rather than training an auxiliary model."
}