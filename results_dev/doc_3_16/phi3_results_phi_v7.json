{
 "zeroshot": " I. Introduction\n- Approaches to disfluency detection task fall into three main categories: noisy channel models, parsing-based approaches, and sequence tagging approaches.\n\nII. Noisy Channel Models (NCMs)\n- Noisy channel models (Johnson and Charniak, 2004;Johnson et al., 2004) use complex tree adjoining grammar (TAG) based channel models to find the \"rough copy\" dependencies between words.\n- The channel model uses the similarity between the reparandum and the repair to allocate higher probabilities to exact copy reparandum words.\n- Using the probabilities of TAG channel model and a bigram language model (LM) derived from training data, the NCM generates n-best disfluency analyses for each sentence at test time.\n- The analyses are then reranked using a language model which is sensitive to the global properties of the sentence, such as a syntactic parser based LM (Johnson and Charniak, 2004;Johnson et al., 2004).\n- Some works have shown that rescoring the n-best analyses with external n-gram (Zwarts and Johnson, 2011) and deep learning LMs (Jamshid Lou and Johnson, 2017) trained on large speech and non-speech corpora, and using the LM scores along with other features (i.e. pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies.\n\nIII. Parsing-based Approaches\n- Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence.\n- This is achieved by augmenting a transition-based dependency parser with a new action to detect and remove the disfluent parts of the sentence and their dependencies from the stack (Rasooli and Tetreault, 2013;Honnibal and Johnson, 2014;Yoshikawa et al., 2016).\n- Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training.\n\nIV. Sequence Tagging Approaches\n- Sequence tagging approaches use classification techniques such as conditional random fields (Liu et al., 2006;Ostendorf and Hahn, 2013;Zayats et al., 2014;Ferguson et al., 2015), hidden Markov models (Liu et al., 2006;Schuler et al., 2010) and deep learning based models (Hough and Schlangen, 2015;Zayats et al., 2016) to label individual words as fluent or disfluent.\n- In much of the previous work on sequence tagging approaches, improved performance has been gained by proposing increasingly complicated labeling schemes.\n- A model with begin-inside-outside (BIO) style states which labels words as being inside or outside of edit region is usually used as the baseline sequence tagging model.\n- Then in order to come up with different pattern matching lexical cues for repetition and correction disfluencies, they extend the baseline state space with new explicit repair states to consider the words at repair region, in addition to edit region (Ostendorf and Hahn, 2013;Zayats et al., 2014, 2016).\n- A model which uses such labeling scheme may generate illegal label sequences at test time.\n- As a solution, integer linear programming (ILP) constraints are applied to the output of classifier to avoid inconsistencies between neighboring labels (Georgila, 2009;Georgila et al., 2010;Zayats et al., 2016).\n- This contrasts with our more straightforward approach, which directly labels words as being fluent or disfluent, and does not require any post-processing or annotation modifications.\n\nV. Comparison with Similar Work\n- The most similar work to ours is recent work by Zayats et al. (2016) that investigated the performance of a bidirectional long-short term memory network (BLSTM) for dis",
 "oneshot": " **Approaches to Disfluency Detection Task**\n\n- **Noisy Channel Models (NCMs)**:\n  - **Johnson and Charniak (2004)**: Proposed a baseline NCM using a bigram language model (LM) and a transition-based dependency parser.\n  - **Johnson et al. (2004)**: Introduced a language model sensitive to global properties of the sentence for reranking n-best disfluency analyses.\n  - **Zwarts and Johnson (2011)**: Showed that rescoring n-best analyses with external n-gram LMs improves the performance of baseline NCMs.\n  - **Jamshid Lou and Johnson (2017)**: Demonstrated that deep learning LMs trained on large speech and non-speech corpora can enhance the performance of baseline NCMs.\n  - **Johnson et al. (2004)**: Implemented a MaxEnt reranker using LM scores and other features to improve the performance of baseline NCMs.\n\n- **Parsing-based Approaches**:\n  - **Rasooli and Tetreault (2013)**: Augmented a transition-based dependency parser with an action to detect and remove disfluent parts of the sentence and their dependencies.\n  - **Honnibal and Johnson (2014)**: Developed a parsing-based approach for disfluency detection.\n  - **Yoshikawa et al. (2016)**: Presented a parsing-based approach for disfluency detection.\n\n- **Sequence Tagging Approaches**:\n  - **Liu et al. (2006)**: Proposed a sequence tagging approach using hidden Markov models (HMMs) for disfluency detection.\n  - **Ostendorf and Hahn (2013)**: Extended the baseline state space with new explicit repair states for pattern matching lexical cues.\n  - **Zayats et al. (2014, 2016)**: Investigated the performance of a bidirectional long-short term memory network (BLSTM) for disfluency detection and reported the use of hand-crafted pattern match features and POS tags.\n  - **Ferguson et al. (2015)**: Incorporated prosodic information extracted from speech into disfluency detection.\n  - **Tran et al. (2018)**: Investigated the performance of a bidirectional long-short term memory network (BLSTM) for disfluency detection.\n\n- **Proposed Approach**:\n  - **Authors of the Paper**: Their proposed approach belongs to the class of sequence tagging approaches. It uses an autocorrelational neural network (ACNN) to label individual words as fluent or disfluent directly, without requiring any post-processing or annotation modifications.",
 "fewshot_rankings": " **Key Concepts and Contributions:**\n\n1. **Noisy Channel Models (NCMs):**\n   - Use tree adjoining grammar (TAG) based channel models (Shieber and Schabes, 1990) to find \"rough copy\" dependencies (Johnson and Charniak, 2004;Johnson et al., 2004).\n   - Generate n-best disfluency analyses and rerank them using a language model sensitive to sentence properties (Johnson and Charniak, 2004;Johnson et al., 2004).\n   - Performance improved with rescoring using external n-gram models and deep learning LMs, but with complex runtime dependencies (Zwarts and Johnson, 2011;Jamshid Lou and Johnson, 2017).\n\n2. **Parsing-Based Approaches:**\n   - Detect disfluencies and identify syntactic structure simultaneously (Rasooli and Tetreault, 2013;Honnibal and Johnson, 2014;Yoshikawa et al., 2016).\n   - Require large annotated treebanks for training (Rasooli and Tetreault, 2013;Honnibal and Johnson, 2014;Yoshikawa et al., 2016).\n\n3. **Sequence Tagging Approaches:**\n   - Utilize classification techniques like conditional random fields, hidden Markov models, and deep learning models (Liu et al., 2006;Ostendorf and Hahn, 2013;Ferguson et al., 2015).\n   - Extend baseline sequence tagging models with new states for better pattern matching (Ostendorf and Hahn, 2013;Zayats et al., 2014, 2016).\n   - Constraints are applied to avoid inconsistencies in labeling (Georgila, 2009;Georgila et al., 2010;Zayats et al., 2016).\n\n**Authors of the Paper**\n\n- **Authors of the Paper**: Proposed an autocorrelational neural network (ACNN) for sequence tagging approaches, aiming to capture dependencies without relying on manually engineered features.\n\n**Comparison with Related Work:**\n\n- **Zayats et al. (2016)**: Investigated BLSTM for disfluency detection, which underperformed when augmented with hand-crafted features.\n- **Authors of the Paper**: Their approach differs by directly labeling words as fluent or disfluent without post-processing or annotation modifications.\n\n**Prosody Information:**\n\n- Some works use prosodic information from speech for disfluency detection (Kahn et al., 2005;Ferguson et al., 2015;Tran et al., 2018).\n\n- The paper's primary motivation is to overcome architectural limitations of deep neural networks, avoiding the need for hand-crafted features or representations from other models.",
 "fewshot_explanation": " **Key Concepts and Contributions:**\n\n1. **Noisy Channel Models (NCMs):**\n   - Use complex tree adjoining grammar (TAG) based channel models (Shieber and Schabes, 1990).\n   - Johnson and Charniak (2004); Johnson et al. (2004): Developed methods to generate n-best disfluency analyses and rerank them using a language model sensitive to sentence structure.\n   - Zwarts and Johnson (2011); Jamshid Lou and Johnson (2017): Improved NCM performance with external n-gram LMs and deep learning LMs.\n\n2. **Parsing-Based Approaches:**\n   - Detect disfluencies and identify syntactic structure simultaneously (Rasooli and Tetreault, 2013;Honnibal and Johnson, 2014;Yoshikawa et al., 2016).\n   - Require large annotated treebanks for training.\n\n3. **Sequence Tagging Approaches:**\n   - Utilize classification techniques like conditional random fields, hidden Markov models, and deep learning models (Liu et al., 2006;Ostendorf and Hahn, 2013;Zayats et al., 2014;Ferguson et al., 2015).\n   - Employ BIO style states for labeling and apply ILP constraints to avoid inconsistencies (Georgila, 2009;Georgila et al., 2010;Zayats et al., 2016).\n   - Our approach, using an autocorrelational neural network (ACNN), labels words directly as fluent or disfluent without post-processing.\n\n**Authors of the Paper:**\n\n- **Authors of the Paper**: Developed an autocorrelational neural network (ACNN) for sequence tagging, aiming to capture dependencies without relying on manually engineered features.\n\n**Comparison with Related Work:**\n\n- Zayats et al. (2016): Investigated BLSTM for disfluency detection, but their model required hand-crafted features and POS tags.\n- Our model differs by not using such features and directly labeling words as fluent or disfluent."
}