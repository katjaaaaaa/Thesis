{
 "oneshot": " **Approaches to Disfluency Detection Task**:\n   - **Noisy Channel Models (NCMs)**:\n      - **Johnson and Charniak (2004)** and **Johnson et al. (2004)**: NCMs use complex tree adjoining grammar (TAG) based channel models to find \"rough copy\" dependencies between words.\n      - **Shieber and Schabes (1990)**: Provided the basis for TAG.\n      - **Johnson et al. (2004)**: NCM generates n-best disfluency analyses for each sentence at test time.\n      - **Johnson and Charniak (2004)**: NCM analyses are reranked using a language model sensitive to global properties of the sentence.\n      - **Zwarts and Johnson (2011)** and **Jamshid Lou and Johnson (2017)**: Improved performance of baseline NCM by rescoring n-best analyses with external n-gram and deep learning LMs.\n   - **Parsing-based Approaches**:\n      - **Rasooli and Tetreault (2013)**: Augmented a transition-based dependency parser to detect and remove disfluent parts of the sentence.\n      - **Honnibal and Johnson (2014)**: Similar approach to augmenting a dependency parser.\n      - **Yoshikawa et al. (2016)**: Similar approach to augmenting a dependency parser.\n   - **Sequence Tagging Approaches**:\n      - **Liu et al. (2006)**, and0ovemem,\nurovesurovememememememally,\nhe,\no,\n\n\n\n\n\n\n\n00000,\n00ets,\n0,\n\n\n0emem,\n\nemoves,\nurim,\n\n\n\n\n\n0000emata0000000000\n\n\nem0emata, and000,\n\n and and,\n\n\n\n,\n000em,\n\n,\nur0)\n)\n,\nal000000,000,00000000,\n00000,\n\n, ofo00,\non0000000,\n\n\n,000000000000,\n000000,\n\n0,\n0000.\n\nates,\nurouses, with,\nes.\n\nal.00o\no.\n00000000000es, on,0o,\no)0)0) ofem, or000000,\no000000)\nouses00urousesur\n\n\nes, and0000\n0.\n\n0.\nurated.\nurative,00urated, onidesur,0,0urated.0.\n.s\n00.\no,  0000\n  \n\nurue,0o",
 "fewshot_rankings": " 1. **Noisy Channel Models for Disfluency Detection**:\n   - **Johnson and Charniak (2004)**: Developed NCMs using TAG for disfluency detection.\n   - **Johnson et al. (2004)**: Utilized NCMs with a bigram LM for generating n-best analyses, which are reranked for performance.\n   - **Zwarts and Johnson (2011)**: Proposed rescoring n-best analyses with external n-gram LMs to improve NCM performance.\n   - **Jamshid Lou and Johnson (2017)**: Employed deep learning LMs to enhance NCMs.\n\n2. **Parsing-Based Approaches**:\n   - Detect disfluencies and identify syntactic structures simultaneously.\n   - **Rasooli and Tetreault (2013)**: Augmented parsers to remove disfluent parts and their dependencies.\n   - **Honnibal and Johnson (2014)**: Parsed and detected disfluencies concurrently.\n   - **Yoshikawa et al. (2016)**: Parsed sentences for disfluency detection and syntactic structure identification.\n\n3. **Sequence Tagging Approaches**:\n   - **Liu et al. (2006)**, **Ostendorf and Hahn (2013)**, **Zayats et al. (2014)**, **Ferguson et al. (2015)**: Used various models like CRFs, HMMs, and deep learning for tagging words as fluent or disfluent.\n   - **Georgila (2009)**, **Georgila et al. (2010)**: Applied ILP constraints to resolve label inconsistencies in sequence tagging models.\n   - **Zayats et al. (2016)**: Investigated BLSTM for disfluency detection, which underperformed compared to models with additional features.\n   - **Kahn et al. (2005)**, **Ferguson et al. (2015)**, **Tran et al. (2018)**: Incorporated prosodic information for disfluency detection.\n   - **Authors of the paper**: Proposed an ACNN for sequence tagging, aiming to automatically learn features without manual engineering or dependency parser outputs.",
 "fewshot_explanation": " 1. **Noisy Channel Models for Disfluency Detection**:\n   - **Johnson and Charniak (2004)**: Utilized TAG-based channel models for disfluency detection.\n   - **Johnson et al. (2004)**: Combined TAG channel model with a bigram LM for n-best disfluency analyses.\n   - **Zwarts and Johnson (2011)**: Rescored n-best analyses with external n-gram LMs.\n   - **Jamshid Lou and Johnson (2017)**: Enhanced NCMs with deep learning LMs.\n\n2. **Parsing-Based Approaches**:\n   - Detect disfluencies and identify syntactic structure simultaneously.\n   - **Rasooli and Tetreault (2013)**: Augmented parsers to remove disfluent parts.\n   - **Honnibal and Johnson (2014)**: Parsed and detected disfluencies concurrently.\n   - **Yoshikawa et al. (2016)**: Parsed sentences for disfluency detection.\n\n3. **Sequence Tagging Approaches**:\n   - Used classification techniques like CRFs, HMMs, and deep learning models for labeling words as fluent or disfluent.\n   - **Liu et al. (2006; Schuler et al., 2010)**: Proposed various sequence tagging models.\n   - **Zayats et al. (2014, 2016)**: Extended models with BIO style states and repair states.\n   - **Georgila et al. (2009; Georgila, 2010)**: Applied ILP constraints to avoid label inconsistencies.\n   - **Our Paper**: Proposed an ACNN for direct labeling of fluent/disfluent without post-processing or dependency parser reliance.\n   - **Zayats et al. (2016)**: Investigated BLSTM for disfluency detection, which our approach contrasts with.\n\n4. **Incorporation of Prosodic Information**:\n   - Some studies used prosodic features for disfluency detection (Kahn et al., 2005; Ferguson et al., 2015; Tran et al., 2018).\n   - Our work aims to overcome architectural limitations by capturing dependencies without manual features."
}