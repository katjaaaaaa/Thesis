{
 "zeroshot": " Subsection 2.1: Style Transfer\n- Xu et al. (2012): Used phrase-based MT method on Shakespeare M2A corpus for style transfer.\n- Jhamtani et al. (2017): Improved results with DNN seq2seq approach using a copy mechanism for style transfer.\n- Rao and Tetreault (2018): Adapted phrased-based and neural MT models for formality transfer using GYAFC corpus.\n- Niu et al. (2018): Improved results on GYAFC by creating a multi-task system for both formality transfer and English-French MT.\n- Roy and Grangier (2019): Proposed a monolingual system for paraphrasing and compared it to unsupervised and translation methods for style transfer.\n\nSubsection 2.2: Style Transfer Methods\n- Xu et al. (2012): Used phrase-based MT method for style transfer.\n- Jhamtani et al. (2017): Improved results with DNN seq2seq approach for style transfer.\n- Rao and Tetreault (2018): Adapted phrased-based and neural MT models for formality transfer for style transfer.\n- Niu et al. (2018): Improved results on GYAFC with a multi-task system for formality transfer and English-French MT for style transfer.\n- Roy and Grangier (2019): Proposed a monolingual system for paraphrasing and compared it to unsupervised and translation methods for style transfer.\n\nContributions:\n- Authors of the paper: Claim that the Style Transfer task consists mostly in good paraphrasing and much less in adding the target style. Focus on background methods for the first task paraphrasing, and then for style transfer.\n- Prakash et al. (2016): Presumably, the first paper presenting a deep learning model for the paraphrase task.\n- Li et al. (2018b): Implemented paraphrase generation with deep reinforcement learning that proved better performance than the previous seq2seq results.\n- Artetxe et al. (2018): Research on variational autoencoders (VAEs) for paraphrasing.\n- Conneau and Lample (2019): Approaches to paraphrasing using variational autoencoders (VAEs).\n- Wan et al. (2005): Statistical paraphrase generation using probabilistic methods.\n- Zhao et al. (2009): Statistical paraphrase generation using probabilistic methods.\n- McKeown (1983): Rule and dictionary-based approaches for paraphrasing.\n- Bolshakov and Gelbukh (2004): Rule and dictionary-based approaches for paraphrasing.\n- Kauchak and Barzilay (2006): Rule and dictionary-based approaches for paraphrasing.\n- Sutskever et al. (2014): DNNs used for automatic paraphrasing with seq2seq models.\n- Hochreiter and Schmidhuber (1997): LSTM-derived seq2seq models for paraphrasing.\n- Bahdanau et al. (2014): LSTM-derived seq2seq models for paraphrasing.\n- Quirk et al. (2004): Statistical paraphrase generation using probabilistic methods.\n- Quora (2017): Dataset used for paraphrasing performance comparison.\n- Lan et al. (2017): Dataset used for paraphrasing performance comparison.\n- Wubben et al. (2012): Matched WikipediaSimple Wikipedia parallel data used for text simplification.\n- Wang et al. (2016): Matched WikipediaSimple Wikipedia parallel data used for text simplification.\n- Xu et al. (2016): Automatic measures for text simplification.\n- Alva-Manchego et al. (2019): Automatic measures for text simplification.\n- Johnson et al. (2017): Training multi-task model on a single language for paraphrasing.\n- Wieting and Gimp",
 "oneshot": " **Style Transfer: Not Too Far From Paraphrasing**\n\n- **Xu et al. (2012)**: Used a phrase-based MT method on Shakespeare M2A corpus for style transfer.\n- **Jhamtani et al. (2017)**: Improved results with the DNN seq2seq approach using a copy mechanism for style transfer.\n- **Rao and Tetreault (2018)**: Adapted phrased-based and neural MT models for formality transfer using GYAFC corpus.\n- **Niu et al. (2018)**: Improved results on GYAFC by creating a multi-task system for both formality transfer and English-French MT.\n- **Dryjanski et al. (2018)**: Used DNNs for both generation of style transfer phrases and their positions related to the input sentence.\n- **Wieting and Gimpel (2018)**: Demonstrated the use of pretrained embeddings from a large parallel paraphrase corpus for state-of-the-art results on several SemEval semantic textual similarity competitions.\n- **Authors of the Paper**: Their work is inspired by the authors, along with the results of Xu et al. (2012), in their measure propositions. They trained the multi-task model on a single language, but with paraphrases and a small neutral-to-style dataset.\n\n**Paraphrasing**\n\n- **McKeown (1983)**: Rule and dictionary-based approaches for paraphrasing.\n- **Bolshakov and Gelbukh (2004)**: Rule and dictionary-based approaches for paraphrasing.\n- **Kauchak and Barzilay (2006)**: Rule and dictionary-based approaches for paraphrasing.\n- **Quirk et al. (2004)**: Statistical paraphrase generation for paraphrasing.\n- **Wan et al. (2005)**: Statistical paraphrase generation for paraphrasing.\n- **Zhao et al. (2009)**: Statistical paraphrase generation for paraphrasing.\n- **Prakash et al. (2016)**: First paper presenting a deep learning model for the paraphrase task.\n- **Sutskever et al. (2014)**: Used DNNs for automatic paraphrasing with seq2seq models.\n- **Prakash et al. (2016)**: Compared their residual LSTM to previous LSTM-derived seq2seq models.\n- **Chung et al. (2015)**: Research on variational autoencoders (VAEs) for paraphrasing.\n- **Bowman et al. (2016)**: Other approaches to paraphrasing emerged from research on VAEs.\n- **Gupta et al. (2018)**: Other approaches to paraphrasing emerged from research on VAEs.\n- **Li et al. (2018b)**: Implemented paraphrase generation with deep reinforcement learning.\n- **Artetxe et al. (2018)**: New practical studies on unsupervised approach for paraphrasing.\n- **Conneau and Lample (2019)**: New practical studies on unsupervised approach for paraphrasing.\n- **Roy and Grangier (2019)**: Proposed a monolingual system for paraphrasing.\n- **Authors of the Paper**: Their work focuses on background methods for paraphrasing and then for style transfer. They trained the multi-task model on a single language, but with paraphrases and a small neutral-to-style dataset.",
 "fewshot_rankings": " **Key Concepts and Contributions:**\n\n1. **Style Transfer:**\n   - Defined as a task composed of paraphrasing and style adjustment.\n   - Emphasizes good paraphrasing over adding target style.\n   - Focuses on paraphrasing methods and then style transfer.\n   - Cited works:\n     - **McKeown (1983), Bolshakov and Gelbukh (2004), Kauchak and Barzilay (2006)**: Traditional rule and dictionary-based paraphrasing approaches.\n     - **Quirk et al. (2004), Wan et al. (2005), Zhao et al. (2009)**: Statistical paraphrase generation methods.\n     - **Prakash et al. (2016)**: Deep learning model for paraphrasing, specifically residual LSTM.\n     - **Li et al. (2018b)**: Paraphrase generation with deep reinforcement learning.\n     - **Rao and Tetreault (2018)**: Adapted phrase-based and neural MT models for formality transfer.\n     - **Niu et al. (2018)**: Multi-task system for formality transfer and MT.\n     - **Dryjanski et al. (2018)**: DNNs for generating ST phrases and their positions.\n\n2. **Style Transfer Methods:**\n   - Xu et al. (2012) pioneered phrase-based MT for style transfer.\n   - Jhamtani et al. (2017) improved with DNN seq2seq and a copy mechanism.\n   - Rao and Tetreault (2018) adapted models for formality transfer.\n   - Niu et al. (2018) created a system for formality transfer and MT.\n   - The authors of the paper draw inspiration from these works for their own style transfer solution.\n\n3. **Paraphrasing and Style Transfer Measurements:**\n   - Xu et al. (2012), Alva-Manchego et al. (2019): Proposed measures for style transfer.\n   - Johnson et al. (2017): Trained multi-task models on paraphrases and a small dataset.\n   - The authors of the paper aim for a universal method for text style transfer, following the inspiration from previous studies.\n\n**Authors of the Paper**\n\n- **Authors of the Paper**: Their work builds upon previous paraphrasing and style transfer methods, focusing on a universal approach for style transfer tasks.",
 "fewshot_explanation": " **Style Transfer in Sentiment Analysis:**\n\n- **Xu et al. (2012)**: Utilized a phrase-based MT method on Shakespeare M2A corpus for style transfer.\n- **Jhamtani et al. (2017)**: Improved style transfer with DNN seq2seq approach and a copy mechanism.\n- **Rao and Tetreault (2018)**: Adapted phrase-based and neural MT models for formality transfer.\n- **Niu et al. (2018)**: Created a multi-task system for formality transfer and English-French MT.\n- **Dryjanski et al. (2018)**: Used DNNs to generate style phrases and their positions in relation to the input sentence.\n- **Authors of the Paper**: Proposed a universal method for text style transfer, drawing inspiration from previous studies and aiming to tackle various tasks such as formality transfer and simplification.\n\n**Paraphrasing in Sentiment Analysis:**\n\n- **McKeown (1983);Bolshakov and Gelbukh (2004);Kauchak and Barzilay (2006);Quirk et al. (2004);Wan et al. (2005);Zhao et al. (2009)**: Traditional rule and dictionary-based approaches to paraphrasing.\n- **Sutskever et al. (2014)**: Used DNNs, specifically seq2seq models, for automatic paraphrasing.\n- **Prakash et al. (2016)**: Presented a deep learning model for paraphrasing, comparing residual LSTM to previous LSTM-derived seq2seq models.\n- **Li et al. (2018b)**: Implemented paraphrase generation with deep reinforcement learning, outperforming seq2seq models on specific datasets.\n- **Artetxe et al. (2018);Conneau and Lample (2019)**: Explored unsupervised approaches to paraphrasing due to insufficient corpora.\n- **Roy and Grangier (2019)**: Proposed a monolingual paraphrasing system and compared it to unsupervised and translation methods.\n\n**Measure Propositions for Style Transfer:**\n\n- **Authors of the Paper**: Based on previous studies and results, proposed measure propositions for style transfer, including paraphrasing and style adjustment.\n- **Johnson et al. (2017)**: Trained a multi-task model on a single language using paraphrases and a small neutral-to-style dataset, differing from the approach of using pretrained embeddings.\n- **Wieting and Gimpel (2018)**: Demonstrated state-of-the-art results using pretrained embeddings and out-of-the-box models for semantic textual similarity.\n- **Authors of the Paper**: Aimed to develop a universal method for text style transfer, inspired by the work of others and their results."
}