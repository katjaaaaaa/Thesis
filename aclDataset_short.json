[
  [
    "Modeling Past and Future for Neural Machine Translation",
    {
      "Sentences": "Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation.  We discuss these topics in the following.   Coverage Modeling.  Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not.  These vectors are updated by accumulating attention probabilities at each decoding step, which provides an opportunity for the attention model to distinguish translated source words from untranslated ones.  Viewing coverage vectors as a (soft) indicator of translated source contents, following this idea, we take one step further.  We model translated and untranslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) instead of attention probability (i.e., the probability of a source word being translated).   In addition, we explicitly model both translated (with PAST-RNN) and untranslated (with FUTURERNN) instead of using a single coverage vector to indicate translated source words.  The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder's states.   In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder and Meng et al. (2016) propose a memory-enhanced attention model.  Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents.  However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients for the success in this work.  In addition, we use two separate layers to explicitly model translated and untranslated contents, which is another distinguishing feature of the proposed approach.   Future Modeling.  Standard neural sequence decoders generate target sentences from left to right, thus failing to estimate some desired properties in the future (e.g., the length of target sentence).  To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017;Bahdanau et al., 2017), in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making.  Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder's hidden states to not only generate the current target word, but also predict the target words that remain untranslated.  Along the direction of future modeling, we introduce a FUTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far).  Functionality Separation.  Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be alleviated by explicitly separating these functions (Reed and Freitas, 2015;Ba et al., 2016;Miller et al., 2016;Gulcehre et al., 2016;Rocktaschel et al., 2017).  For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015).  Rocktaschel et al. (2017) propose a keyvalue-predict attention model, which outputs three vectors at each step: the first is used to predict the next-word distribution; the second serves as the key for decoding; and the third is used for the attention mechanism.  In this work, we further separate PAST and FUTURE functionalities from the decoder's hidden representations. "
    }
  ],
  [
    "Grounded Graph Decoding improves Compositional Generalization in Question Answering",
    {
      "Sentences": "Various approaches to the compositional generalization challenge posted by the CFQ dataset has been explored in prior or contemporary works, including Guo et al. (2020), Das et al. (2021), and Herzig et al. (2021).  These approaches are discussed in more detail in Section 4.   Another promising approach that has received relatively less attention so far is grammar induction, which can potentially derive grammar rules directly from question-query pairs.  Grammar induction methods, such as Zettlemoyer and Collins (2005), typically assumes a limited set of grammar rules to bootstrap the model, and then search some grammar spaces to find grammars that can lead to successful parsing of observed questions.   The idea of grammar induction has inspired various work that to different extent solved the SCAN dataset, such as Nye et al. (2020) or Chen et al. (2020).  The advantage of grammar induction methods is that they can potentially identify the complete set of transformation rules and thus attain perfect compositional generalization.  However, grammar induction methods are generally search-based, which limits their scalability to long sentences due to the size of search spaces.   Additionally, there has been considerable research in the semantic parsing literature to design neural network architectures that incorporate different query structures, including tree (Dong and Lapata, 2016), graph (Buys and Blunsom, 2017;Damonte et al., 2017;Lyu and Titov, 2018;Fancellu et al., 2019).  However, these architectures only incorporating query structures without incorporating syntactic structures in questions.  Our ablation study (Table 3) indicates that only incorporating query structure is insufficient for compositional generalization.  Our graph decoder alone only attains performance on bar with T5 models.   Similar to our work, Russin et al. (2019) also proposes to improve the compositional generalization of seq2seq models using attention.  However, their work only studies token-level attention without consideration of syntactic or semantic structures.  Both Russin et al. (2019) and Gordon et al. (2020) use part-of-speech (PoS) tags to attain some level of invariance among words that share the same PoS.   Finally, in the domain of semantic parsing, prior to Keysers et al. (2020), Finegan-Dollak et al. (2018) proposed to split datasets such that training and test sets contain no common SQL patterns.  Although this approach increases task difficulty, different SQL query patterns may still share similar substructures, which enables neural networks to solve the tasks relatively easily using the \"mix-andmatch\" strategy (Lake and Baroni, 2018). "
    }
  ],
  [
    "An Empirical Study on Multi-Task Learning for Text Style Transfer and Paraphrase Generation",
    {
      "Sentences": "subsection(2.1 Style transfer: not too far from paraphrasing)  Similarly to Xu et al. (2012) we see language ST as a task composed of two linked subtasks: paraphrasing and style adjustment.  In our paper we claim that the ST task consists mostly in good paraphrasing and much less in adding the target style.  Following this idea, we focus on background methods for the first task paraphrasing, and then for style transfer.   Traditionally, the paraphrasing task involved rule and dictionary-based approaches (McKeown, 1983;Bolshakov and Gelbukh, 2004;Kauchak and Barzilay, 2006).  Another popular method was the statistical paraphrase generation that recombined words probabilistically in order to create new sentences (Quirk et al., 2004;Wan et al., 2005;Zhao et al., 2009).  Currently, DNNs are used for automatic paraphrasing, e.g. by sequence-to-sequence (seq2seq) models (Sutskever et al., 2014).   Presumably, the first paper presenting a deep learning model for the paraphrase task is the one by Prakash et al. (2016).  The authors successfully compared their residual LSTM to previous LSTM-derived seq2seq models (Hochreiter and Schmidhuber, 1997;Schuster and Paliwal, 1997;Bahdanau et al., 2014;Vaswani et al., 2017).  In parallel, upon research on variational autoencoders (VAEs), e.g. Chung et al. (2015), other approaches to paraphrasing emerged (Bowman et al., 2016;Gupta et al., 2018).   More recently Li et al. (2018b) implemented paraphrase generation with deep reinforcement learning that proved better performance than the previous seq2seq results on Twitter (Lan et al., 2017) and Quora (Quora, 2017) datasets.  Insufficient corpora for paraphrase generation gave motivation to new practical studies on unsupervised approach (Artetxe et al., 2018;Conneau and Lample, 2019).  Recently, Roy and Grangier (2019) proposed a monolingual system for paraphrasing (without translation) and compared it to the unsupervised and translation methods, presenting various linguistic characteristics for each of them. subsection(2.2 Style transfer methods) We name here only some existing solutions.  Xu et al. (2012) used a phrase-based MT method on Shakespeare M2A corpus.  This research was followed by Jhamtani et al. (2017), who improved the results with the DNN seq2seq approach using a copy mechanism.   Rao and Tetreault (2018) adapted phrased-based and neural MT models for formality transfer using GYAFC corpus.  Later Niu et al. (2018) improved results on GYAFC by creating a multi-task system for both formality transfer and English-French MT.   It is worth mentioning Dryjanski et al. (2018) who used DNNs both elements: generation of ST phrases and their positions related to the input sentence.   Another distinctive study was conducted for text simplification (TS) task with the matched WikipediaSimple Wikipedia parallel data, e.g. Wubben et al. (2012;Wang et al. (2016).  What is significant for TS are the results for automatic measures in Xu et al. (2016) followed by Alva-Manchego et al. (2019).  We draw our inspiration from the authors, along with the results of Xu et al. (2012) in our measure propositions.   Our solution has common features with Wieting and Gimpel (2018).  The authors demonstrated that using pretrained embeddings from a large parallel paraphrase corpus (âˆ¼50 millions) and out-of-the-box models, it was possible to reach state-of-the-art results on several SemEval semantic textual similarity competitions.  In our work, instead of using pretrained embeddings we follow Johnson et al. (2017) and train the multi-task model on a single language, but with paraphrases and a small neutral-to-style dataset.  Compared to the previous solutions, our approach can be seen as a universal method to tackle text ST (e.g.  formality transfer, simplification and more)."
    }
  ],
  [
    "Adaptive Knowledge-Enhanced Bayesian Meta-Learning for Few-shot Event Detection",
    {
      "Sentences": "Event Detection.  Recent event detection methods based on neural networks have achieved good performance (Chen et al., 2015;Sha et al., 2016;Nguyen et al., 2016;Lou et al., 2021).  These methods use neural networks to construct the context features of candidate trigger words to classify events.  Pre-trained language models such as BERT (Devlin et al., 2019) have also become an indispensable component of event detection models (Yang et al., 2019;Wadden et al., 2019;Shen et al., 2020).  However, neural models rely on largescale labeled event datasets and fail to predict the labels of new event types.  A recent study utilized the basic metric-based few-shot learning method for event detection (Lai et al., 2020b).  Deng et al. (2020) tackles few-shot learning for event classification with a dynamic memory network.  To enhance background knowledge, ontology embedding is used in ED (Deng et al., 2021).  These methods have achieved encouraging results in the few-shot learning setting.  However, they do not address the problem of insufficient sample diversity in the support set.  Our method leverages the knowledge in FrameNet to augment the support set for event detection.   Few-shot Learning and Meta-learning.  Fewshot learning trains a model with only a few labeled samples in a support set and predicts the labels of unlabeled samples in the query set.  Various approaches have been proposed to solve the fewshot learning problem, which mainly fall into three categories: (1) metric-based methods (Vinyals et al., 2016;Snell et al., 2017;Garcia and Bruna, 2012;Sung et al., 2018), (2) optimization-based methods (Finn et al., 2017;Nichol et al., 2018;Ravi and Larochelle, 2016), and (3) model-based methods (Yan et al., 2015;Zhang et al., 2018b; Sukhbaatar et al., 2015;Zhang et al., 2018a).  However, these methods rely heavily on the support set and suffer from poor robustness caused by insufficient sample diversity of the support set.   Bayesian meta-learning (Ravi and Larochelle, 2016;Yoon et al., 2018) can construct the posterior distribution of the prototype vector through external information outside the support set.  The effectiveness of this method has been shown in the few-shot relation extraction task (Qu et al., 2020).  It inspires us to solve the problem of insufficient sample diversity in the task of few-shot event detection by introducing external knowledge.  However, this method ignores the semantic deviation between knowledge and target types.  Specifically, a knowledge base may provide incomplete coverage of target types in a given support set, which leads to inaccurate matching between a target type and knowledge. "
    }
  ],
  [
    "Summarizing Relationships for Interactive Concept Map Browsers",
    {
      "Sentences": "This study builds on prior efforts from Handler and O'Connor (2018), who propose extractively summarizing relationships via a two-stage process that first (1) identifies wellformed spans from a corpus that start with (t) and end with (t) and then (2) chooses the best summary statement from among these wellformed candidates.  Handler and O'Connor (2018) show that extracting wellformed spans can find many more readable candidates than traditional relation extraction techniques.  But they do not offer a method for the second step of picking a summary statement, which is the focus of this study.   We approach this new task of choosing the best summary statement from available candidates by collecting new supervision, tailored to the particular problem of summarizing relationships on concept maps.  This form of supervision has a different focus from the existing Falke and Gurevych (2017) concept map dataset.  Where Falke and Gurevych (2017) seek to create the best overall concept map for a given topic, this work seeks to find the best summary relationship for a given relationship.  Therefore, unlike Falke and Gurevych (2017), our dataset includes labels for the most readable and informative statement describing the relationship between a (t) \u2212 (t) query pair. "
    }
  ],
  [
    "Team Phoenix at WASSA 2021: Emotion Analysis on News Stories with Pre-Trained Language Models",
    {
      "Sentences": "Pre-trained language models have proved to be a breakthrough in analyzing a person's emotional state.  We now describe briefly some of these highly influential works.  subsection(2.1 Pre-trained Language Models)  Over the past few years, pre-trained language models have progressed greatly in learning contextualized representations.  Transformer (Vaswani et al., 2017), first proposed for machine translation, has enabled faster learning of complex representations of text.  GPT (Radford et al., 2018), BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) all leverage transformer architecture along with statistical tokenizers.  ELECTRA (Clark et al., 2020) a recent generator-discriminatorbased pre-training approach offers a competitive performance despite requiring lesser compute.  Domain specific language models also leads to a significant performance gain (Vaidhya and Kaushal, 2020;Lee et al., 2019;Beltagy et al., 2019).   subsection(2.2 Emotion Recognition)  Emotion recognition through facial expressions and speech data has been the subject of extensive study in the past.  (Tarnowski et al., 2017) presents an approach for recognition of seven emotional states based on facial expressions.  (Yoon et al., 2018) utilizes a novel deep dual recurrent encoder model to obtain a better understanding of speech data using text data and audio signals simultaneously.   For text, various approaches have been proposed for emotion recognition.  Deshmukh and Kirange (2012) proposed an SVM-based approach for predicting opinions on news headlines.  Acheampong et al. (2020) paper analyses the efficacy of utilizing transformer encoders for detecting emotions.  Kant et al. (2018) demonstrates the practical efficiency of large pre-trained language models for Multi-Emotion sentiment classification.   subsection(2.3 Computation of Empathy)  Empathy and distress are core components of a person's emotional state, and there has been a growing interest in computational approaches to model them.  Considering language variations across different regions, empathy and distress can also vary with demographics (Lin et al., 2018;Loveys et al., 2018), and recently (Guda et al., 2021) proposed a demographic-aware empathy modelling framework using BERT and demographics features.   Understanding empathy and distress are crucial for analyzing mental health and providing aid.  Recently (Sharma et al., 2020) explored language models for identifying empathetic conversations in the mental health support system. "
    }
  ],
  [
    "A Comparison of Sentence-Weighting Techniques for NMT",
    {
      "Sentences": "Domain adaptation strategies can be separated into four categories: data selection, data generation, instance weighting and model interpolation Chu and Wang (2018).  We focus our discussion on data selection and instance weighting, as these are closely related to our approach.   Data-centric methods.  Models are trained using in-domain and out-of-domain data to evaluate out-of-domain data and compute a similarity score.  Using a cut-off threshold on these scores the training data can be selected.  Language Models Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) or joint models Cuong and Sima'an (2014); Durrani et al. (2015) can traditionally be applied to score corpora.  Recently convolutional neural networks (CNN) Chen et al. (2016) were used.  Our work has similarities to this work but uses instance weighting rather than data selection.   In settings where the amount of parallel training corpora is not sufficient, generating pseudo-parallel sentences by information retrieval Utiyama and Isahara (2003), self-enhancing Lambert et al. (2011) or parallel word embeddings Marie and Fujita (2017).  Aside from generating sentences, other approaches generate monolingual n-grams Wang et al. (2014) or parallel phrase pairs Chu (2015).   In general, data-centric methods (data selection and data generation) are not SMT specific and can be directly applied to NMT.  However, because these methods are not directly related to NMT's training criterion, they only lead to minor improvements Wang et al. (2017a).   Model-centric methods.  Instance Weighting is a technique from SMT and was introduced to NMT as well Wang et al. (2017b).  An in-domain language model was trained to measure the similarity between sentences and the in-domain data via cross-entropy.  The weights are then integrated into the training objective.  We improve on their work by using state-of-the-art neural classifiers and showing that they are more effective than cross-entropy.   Two works that are closer to our work are Wang et al. (2018) and Chen et al. (2017).  In Wang et al. (2018) they generate sentence embeddings for all in-domain sentences and then measure the distance between every sentence and the in-domain core.  The underlying assumption is that the core of all in-domain sentence embeddings is atypical representative and proximity in their sentence embeddings indicates being part of the same domain.  This approach is appropriate when we have in-domain parallel text, but we study a different scenario, with no access to in-domain parallel text, which means the encoder has no access to in-domain training examples.  In Chen et al. (2017) a domain classifier is incorporated into the NMT system, using features from the encoder to distinguish between in-domain and out-of-domain data.  The classifier probabilities are used to weight sentences with regard to their similarity to in-domain data, when training the neural network.  Scaling the loss function is similar to multiplying the learning rate with the instance weight.  The classifier and NMT are trained at the same time, whereas we chose an approach with pretrained neural classifiers which are trained on a small amount of monolingual data (the scenario we study) with no access to parallel in-domain data.   Finally, while some previous work we have mentioned did look at various ways to use domain classification, such previous work has not focused on how to weight the classifier probabilities for effective use in NMT, which we showed is important for obtaining translation quality improvements, particularly when using neural classifiers which can be overconfident. "
    }
  ],
  [
    "Multimodal Topic Labelling",
    {
      "Sentences": "Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g.  text or images) for a given topic; and (2) the ranking of candidate labels by relevance to the topic.  Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar ticle titles (Lau et al., 2011;Aletras and Stevenson, 2014;Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014;Wan and Wang, 2016).  Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013;Aletras and Mittal, 2017).  Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016;Aletras and Mittal, 2017).   Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space.  The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine similarity between the topic and article title embeddings.  Finally, top labels are re-ranked in a supervised fashion using various features such as the PageRank score of the article in Wikipedia (Brin and Page, 1998), trigram letter ranking (Kou et al., 2015), topic word overlap, and word length of the label.   Aletras and Mittal (2017) use pre-computed dependency-based word embeddings (Levy and Goldberg, 2014) to represent the topics and the caption of the images, as well as image embeddings using the output layer of VGG-net (Simonyan and Zisserman, 2014) pretrained on ImageNet (Deng et al., 2009).  A concatenation of these three vectors is the input to a simple deep neural network with four hidden layers and a sigmoid output layer to predict the relevance score.   Textual or visual modalities for labelling topics have been studied extensively, although independently from one another.  Our work differs from the single-modality methods described above in that it uses a joint model to predict the continuousvalued rating for both textual and image labels.  This is, to the best of our knowledge, the first attempt at joint multimodal topic labelling. "
    }
  ],
  [
    "Answer-driven Deep Question Generation based on Reinforcement Learning",
    {
      "Sentences": "Question Generation Question Generation is one of the typical natural language generation tasks (Reiter and Dale, 2000;Saggion and Poibeau, 2013;Balakrishnan et al., 2019).  Generating questions from various kinds of sources, such as texts, search queries, knowledge bases and images, has attracted much attention recently.  Our work is most related to previous work on generating questions from texts.  Traditional methods are mostly rule-based, which rely on manual rules or templates and rank the generated questions by human-designed features (Heilman and Smith, 2010;Mazidi and Nielsen, 2014), which are costly and lack diversity.  Neural QG models are usually variants of the encoder-decoder framework (Du et al., 2017;Zhou et al., 2017;Sun et al., 2018;Pan et al., 2019;Wang et al., 2019).  Zhou et al. (2017) propose a feature-rich encoder for the Seq2Seq (Sutskever et al., 2014) model, and Zhao et al. (2018) process paragraph level inputs with maxout pointer and gated self-attention.  To deal with the \"exposure      bias\" problem, reinforcement learning models are applied (Zhang and Bansal, 2019;Chen et al., 2020).  However, despite considering longer contexts, the above QG methods generate questions related to only one fact obtained from a single sentence or article without deep comprehension and reasoning.  This work focus on generating deep questions with multi-hop reasoning over document-level contexts.  Deep Question Generation Deep Question Generation (DQG) aims to generate complex questions that require reasoning over multiple pieces of information.  This task is inspired by multi-hop question answering (Song et al., 2018;Chen and Durrett, 2019;Tu et al., 2020), which aggregate the scattered evidence fragments in multiple documents to predict the correct answer.   Pan et al. (2020) is the first to study the task of DQG.  They propose anew framework which incorporates semantic graphs to enhance the document representations and jointly train the tasks of content selection and question decoding.  However, they do not pay much attention to the answer information that is a key to question generation and simply introduce the answer to the decoder based on the encoding of word embedding.  Our work make better use of the target answer as a guidance to facilitate question generation with the help of the answer-aware initialization module and semantic-rich fusion attention mechanism.  Reinforcement learning which integrates both syntactic and semantic metrics as the reward is also applied to enhance the training process. "
    }
  ],
  [
    "Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation",
    {
      "Sentences": "There exist several toolkits that focus on one or a few specific tasks.  For neural machine translation and alike, there are Tensor2Tensor (Vaswani et al., 2018) on TensorFlow, OpenNMT (Klein et al., 2017) on PyTorch, Nematus (Sennrich et al., 2017) on Theano, MarianNMT (JunczysDowmunt et al., 2018) on C++, etc.  ParlAI (Miller et al., 2017) is a specialized platform for dialogue.  Differing from the task-focusing tools, Texar aims to cover as many text generation tasks as possible.  The goal of versatility poses unique design challenges.   On the other end of the spectrum, there are libraries for more general NLP or ML applications: AllenNLP (allennlp.org), GluonNLP (gluon-nlp.mxnet.io) and others are designed for the broad NLP tasks in general, while Keras (keras.io) is for high conceptual-level programming without specific task focuses.  In comparison, Texar has a proper focus on the text generation sub-area, and provide a comprehensive set of modules and functionalities that are welltailored and readily-usable for relevant tasks.  For example, Texar provides rich text docoder with optimized interfaces to support over ten decoding methods (see section 3.3 for an example). "
    }
  ],
  [
    "A Boundary-aware Neural Model for Nested Named Entity Recognition",
    {
      "Sentences": "NER has drawn the attention of NLP researchers because several downstream tasks such as entity linking (Gupta et al., 2017), relation extraction (Mintz et al., 2009;Liu et al., 2017), co-reference resolution (Chang et al., 2013) and conversation system (Ren et al., 2019) rely on it.  Several methods have been proposed on flat named entity recognition (Lample et al., 2016;Ma and Hovy, 2016;Strubell et al., 2017) while few of them address nested entities.  Early work on nested entities rely on hand-craft features or rule-based postprocessing (Zhang et al., 2004;Zhou et al., 2004;Zhou, 2006).  They detect the innermost flat entities with a Hidden Markov Model and then use rule-based post-processing to extract the outer entities.   While most work concerns about named entities, Lu and Roth (2015) present a novel hypergraph-based method to tackle the problem of entity mention detection.  One issue of their method is the spurious structure of hyper-graphs.  Muis and Lu (2017) improve the method of Lu and Roth (2015) by incorporating mention separators along with features.   Recent studies reveal that stacking sequence model like conditional random filed(CRF) layer can extract entities from inner to outer.  Alex et al. (2007) propose several CRF-based methods for the GENIA dataset.  However, their approach can not recognize nested entities of the same type.  Finkel and Manning (2009) present a chart-based parsing method where each named entity is a constituent in the parsing tree.  However, their method is not scalable to larger corpus with a cubic time complexity.  Ju et al. (2018) dynamically stack flat NER layers to extract nested entities, each flat layer is based on a Bi-LSTM layer and then a cascaded CRF layer.  Their model suffers error propagation from layer to layer, an inner entity can not be detected when a outer entity is identified first.   It is difficult for sequence model, like CRF, to extract nested entities where a token can be included in several entities.  Wang et al. (2018) present a transition-based model for nested mention detection using a forest representation.  One drawback of their model is the greedy training and decoding.  Sohrab and Miwa (2018) consider all possible regions in a sentence and classify them into their entity type or non-entity.  However, their exhaustive method considers too many irrelevant regions(non-entity regions) into detecting entity types and the regions are classified individually, without considering the contextual information.  Our model focuses on the boundary-relevant regions which is much fewer and the explicit leveraging of boundary information helps to locate entities more precisely. "
    }
  ],
  [
    "A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis",
    {
      "Sentences": "There are two lines of works conducted on multimodal sentiment analysis.   One is focusing on utterance-level multimodal feature fusion.  These methods use the features of the overall utterance.  For example, they first extract the frame-level visual or acoustic features and then average them to obtain the final features, which are called utterance-level features.  The utterance-level textual features can be obtained by applying RNNs for words.  The obtained utterance-level features are fed into the fusion model to get the multimodal representation.  Some models have been proposed for effective multimodal feature fusion.  Zadeh et al. (2017) proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions.  But this method uses the three-fold Cartesian product to fuse the multimodal features, which makes the time cost very high.  To address it, Liu et al. (2018) presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.  Mai et al. (2020) proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.   The utterance-level features mainly contain global information, which may fail to capture local information.  Therefore, recent works are mostly focusing on word-level multimodal feature fusion.  And our work in this paper is also based on wordlevel features.  To extract word-level features, the first step is applying force alignment to obtain the timestamps of each word including the start time and end time.  And then following the timestamps, the utterance is split into some video clips.  Finally, word-level visual or acoustic features are obtained by averaging the frame-level features of the video clips.  Based on word-level features, lots of methods are proposed for performing word-level multimodal feature fusion.  Zadeh et al. (2018a) proposed the Memory Fusion Network(MFN) to capture the interactions across both different modalities and timesteps.  Inspired by the observation that the meaning of words often varies dynamically in different non-verbal contexts, Wang et al. (2019) proposed the Recurrent Attended Variation Embedding Network (RAVEN).  This model applies the Attention Gating module to fuse the word-level features, which can dynamically use the non-verbal features to shift the word embeddings.  Tsai et al. (2019) presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions, motivated by the great success of transformer in NLP(Vaswani et al., 2017).   Besides, there is a related work (Pham et al., 2019) need to be noticed, which proposed that translation from a source to a target modality provides away to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations.  Comparing to this work, our model has a significant difference.  That is we use the crossmodal prediction task to distinguish the shared and private non-textual features instead of training the model as an auxiliary task.  In this way, we can obtain more useful information by deeply probing the cross-modal prediction model. "
    }
  ],
  [
    "HappyDB: A Corpus of 100,000 Crowdsourced Happy Moments",
    {
      "Sentences": "To the best of our knowledge, HappyDB is the first crowdsourced corpus of happy moments which can be used for understanding the language people use to describe happy events.  There has been recent interest in creating datasets in the area of mental health.  Althoff et al. (Althoff et al., 2016) conducted a large scale analysis on counseling conversational logs collected from short message services (SMS) for mental illness study.  They studied how various linguistic aspects of conversations are correlated with conversation outcomes.  Mihalcea et al. (Mihalcea and Liu, 2006) performed text analysis on blog posts from LiveJournal (where posts can be assigned happy/sad tags by their authors).  Lin et al. (Lin et al., 2016) measure stress from short texts by identifying the stressors and the stress levels.  They classified tweets into 12 stress categories defined by the stress scale in (Holmes and Rahe, 1967).  Last year alone, there were multiple research efforts that obtained datasets via crowdsourcing and applied natural language techniques to understand different corpora.  For example, SQuAD (Rajpurkar et al., 2016) created a largescale dataset for question-answering.  The crowdsourced workers were asked to create questions based on a paragraph obtained from Wikipedia.  They employed MTurk workers with strong experience and a high approval rating to ensure the quality of the dataset.  We did not select the workers based on their qualification for HappyDB as our task is cognitively easier than SQuAD's and we want to avoid bias in our corpus.  HappyDB is similar to SQuAD in terms of the scale of the crowdsourced dataset.  However, unlike SQuAD, which was designed specifically for studying the question answering problem, the problems that HappyDB can be used for are more open-ended. "
    }
  ],
  [
    "A Dataset of German Legal Documents for Named Entity Recognition",
    {
      "Sentences": "Until now, NER has not received a lot of attention in the legal domain, developed approaches are fragmented and inconsistent with regard to their respective methods, datasets and typologies used.  Among the related work, there is no agreement regarding the selection of relevant semantic categories from the legal domain.  In addition, corpora or datasets of legal documents with annotated named entities do not appear to exist, which is, obviously, a stumbling block for the development of data-driven NER classifiers.  Dozier et al. (2010) describe five classes for which taggers are developed based on dictionary lookup, patternbased rules, and statistical models.  These are jurisdiction (a geographic area with legal authority), court, title (of a document), doctype (category of a document), and judge.  The taggers were tested with documents such as US case law, depositions, pleadings etc.  Cardellino et al. (2017) develop an ontology of legal concepts, making use of NERC (6 classes), LKIF (69 classes) and YAGO (358 classes).  On the NERC level, entities were divided in abstraction, act, document, organization, person, and non-entity.  With regard to LKIF, company, corporation, contract, statute etc.  are used.  Unfortunately, the authors do not provide any details regarding the questions how the entities were categorised or if there is any correlations between the different levels.  They work with Wikipedia articles and decisions of the European Court of Human Rights.  Glaser et al. (2018) use GermaNER (Benikova et al., 2015) and DBpedia Spotlight (Mendes et al., 2011;Daiber et al., 2013) for the recognition of person, location and organization entities.  References are identified based on the rules described by Landthaler et al. (2016).  The authors created an evaluation dataset of 20 court decisions. "
    }
  ],
  [
    "MUCS@LT-EDI-EACL2021:CoHope-Hope Speech Detection for Equality, Diversity, and Inclusion in Code-Mixed Texts",
    {
      "Sentences": "Researchers have developed a vast range of datasets, tools and models for Text Classification (TC).  However, comparatively very less work has been done on the classification of code-mixed texts and the available literature focus on SA and OLI tasks for several languages pairs.  Hope Speech detection is anew challenge that has been explored rarely.  Some of recent studies on TC tasks for code-mixed texts are given below:  (Chakravarthi et al., 2020b) presents an overview of OLI shared task on code-mixed texts in Dravidian languages consisting of two subtasks A and B to classify a given text into \"offensive\" or \"not-offensive\" categories.  While Subtask A is to  classify code-mixed Ma-En YouTube comments, SubTask B is to classify Romanized Malayalam and Romanized Tamil texts from YouTube or Twitter comments.  Datasets used in this shared tasks are described in (Chakravarthi et al., 2020c) and (Chakravarthi et al., 2020a).  Two models based on different configurations of LSTM proposed by (Renjit and Idicula, 2020) for the OLI shared task obtained a weighted F1-score of 0.53 for Romanized Malayalam text in Subtask B.  A Universal LM has been trained for Ma-En code-mixed texts from Wikipedia articles in native script combined with  translated and transliterated versions by (Arora, 2020).  The authors transferred the obtained LM to TC model from fastai library to classify code-mixed texts in Ma-En and obtained 0.91, 0.74 weighted F1-score for Subtask A and Romanized Malayalam text of Subtask B respectively.   \"Sentiment Analysis of Dravidian Languages in Code-Mixed Text\" which focuses on SA of code-mixed texts in Ta-En and Ma-En language pairs (Chakravarthi et al., 2020d) is another shared task on Dravidian languages.  Datasets described in (Chakravarthi et al., 2020c) and (Chakravarthi et al., 2020a) are used in this shared task and they include five categories, namely, \"Positive\", \"Negative\", \"Unknown state\", \"Mixed-Feelings\", and \"Other languages\" for each language pairs.  The  overall results of this shared task reported in leaderboard illustrates that XLM-Roberta model proposed by (Sun and Zhou, 2020) with a weighted F1-score of 0.65 and 0.74 for Ta-En and Ma-En language pairs respectively obtained first rank for both subtasks.  The proposed XLM-Roberta model uses extracted output of Convolution Neural Networks (CNN) which enables it to utilize the semantic information from texts.  Another XLM-Roberta model proposed by (Ou and Li, 2020) ensembles pre-trained multi-language models and K-folding method to classify code-mixed texts.  The proposed model with 0.63 and 0.74 weighted F1-score obtained third and first ranks on Ta-En and Ma-En language pairs respectively. "
    }
  ],
  [
    "Disfluency Detection using Auto-Correlational Neural Networks",
    {
      "Sentences": "Approaches to disfluency detection task fall into three main categories: noisy channel models, parsing-based approaches and sequence tagging approaches.  Noisy channel models (NCMs) (Johnson and Charniak, 2004;Johnson et al., 2004) use complex tree adjoining grammar (TAG) (Shieber and Schabes, 1990) based channel models to find the \"rough copy\" dependencies between words.  The channel model uses the similarity between the reparandum and the repair to allocate higher probabilities to exact copy reparandum words.  Using the probabilities of TAG channel model and a bigram language model (LM) derived from training data, the NCM generates n-best disfluency analyses for each sentence at test time.  The analyses are then reranked using a language model which is sensitive to the global properties of the sentence, such as a syntactic parser based LM (Johnson and Charniak, 2004;Johnson et al., 2004).  Some works have shown that rescoring the n-best analyses with external n-gram (Zwarts and Johnson, 2011) and deep learning LMs (Jamshid Lou and Johnson, 2017) trained on large speech and non-speech corpora, and using the LM scores along with other features (i.e.  pattern match and NCM ones) into a MaxEnt reranker (Johnson et al., 2004) improves the performance of the baseline NCM, although this creates complex runtime dependencies.   Parsing-based approaches detect disfluencies while simultaneously identifying the syntactic structure of the sentence.  Typically, this is achieved by augmenting a transition-based dependency parser with anew action to detect and remove the disfluent parts of the sentence and their dependencies from the stack (Rasooli and Tetreault, 2013;Honnibal and Johnson, 2014;Yoshikawa et al., 2016).  Joint parsing and disfluency detection can compare favorably to pipelined approaches, but requires large annotated treebanks containing both disfluent and syntatic structures for training.   Our proposed approach, based on an autocorrelational neural network (ACNN), belongs to the class of sequence tagging approaches.  These approaches use classification techniques such as conditional random fields (Liu et al., 2006;Ostendorf and Hahn, 2013;Zayats et al., 2014;Ferguson et al., 2015), hidden Markov models (Liu et al., 2006;Schuler et al., 2010) and deep learning based models (Hough and Schlangen, 2015;Zayats et al., 2016) to label individual words as fluent or disfluent.  In much of the previous work on sequence tagging approaches, improved performance has been gained by proposing increasingly complicated labeling schemes.  In this case, a model with begin-inside-outside (BIO) style states which labels words as being inside or outside of edit region is usually used as the baseline sequence tagging model.  Then in order to come up with different pattern matching lexical cues for repetition and correction disfluencies, they extend the baseline state space with new explicit repair states to consider the words at repair region, in addition to edit region (Ostendorf and Hahn, 2013;Zayats et al., 2014, 2016).  A model which uses such labeling scheme may generate illegal label sequences at test time.  As a solution, integer linear programming (ILP) constraints are applied to the output of classifier to avoid inconsistencies between neighboring labels (Georgila, 2009;Georgila et al., 2010;Zayats et al., 2016).  This contrasts with our more straightforward approach, which directly labels words as being fluent or disfluent, and does not require any post-processing or annotation modifications.   The most similar work to ours is recent work by Zayats et al. (2016) that investigated the performance of a bidirectional long-short term memory network (BLSTM) for disfluency detection.  Zayats et al. (2016) reported that a BLSTM operating only on words underperformed the same model augmented with hand-crafted pattern match features and POS tags by 7% in terms of f-score.  In addition to lexically grounded features, some works incorporate prosodic information extracted from speech (Kahn et al., 2005;Ferguson et al., 2015;Tran et al., 2018).  In this work, our primary motivation is to rectify the architectural limitations that prevent deep neural networks from automatically learning appropriate features from words alone.  Therefore, our proposed model eschews manually engineered features and other representations derived from dependency parsers, language models or tree adjoining grammar transducers that are used to find \"rough copy\" dependencies.  Instead, we aim to capture these kinds of dependencies automatically. "
    }
  ],
  [
    "Sentence Embedding Alignment for Lifelong Relation Extraction",
    {
      "Sentences": "Lifelong Learning without Catastrophic Forgetting Recent lifelong learning research mainly focuses on overcoming the catastrophic forgetting phenomenon (French, 1999;McCloskey and Cohen, 1989;McClell and et al., 1995;Ratcliff, 1990), i.e., knowledge of previous tasks is abruptly forgotten when learning on anew task.   Existing research mainly follow two directions: the first one is memory-based approach (LopezPaz and Ranzato, 2017;Anonymous, 2019), which saves some previous samples and optimizes anew task with a forgetting cost defined on the saved samples.  These methods have shown strength in alleviating catastrophic forgetting, but the computational cost grows rapidly with the number of previous tasks.  The second direction is to consolidate parameters that are important to previous tasks (Kirkpatrick et al., 2016;Liu et al., 2018;Ritter et al., 2018;Zenke et al., 2017).  For example, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2016) slows down learning on weights that are important to previous tasks.  These methods usually do not need to save any previous data and only train on each task once.  But their abilities to overcome catastrophic forgetting are limited.   Lifelong Learning with Dynamic Model Architecture There is another related direction on dynamically changing the model structure (i.e., adding new modules) in order to learn the new task without interfering learned knowledge for previous tasks, such as (Xiao et al., 2014;Rusu et al., 2016;Fernando et al., 2017).  These approaches could successfully prevent forgetting.  However, they do not suit many lifelong settings in NLP.  First, it cannot benefit from the positive transfer between tasks.  Second, the size of the model grows dramatically with the number of observed tasks, which makes it infeasible for real-world problems where there area lot of tasks.  Remark It is worth noting that the term lifelong learning is also widely used in (Chen et al., 2015;Chen, 2015;Shu et al., 2016, 2017), which mainly focus on how to represent, reserve and extract knowledge of previous tasks.  These works belong to a research direction different from lifelong learning without catastrophic forgetting. "
    }
  ],
  [
    "#TeamINF at SemEval-2018 Task 2: Emoji Prediction in Tweets",
    {
      "Sentences": "Emojis can express diverse types of contents in a visual way, adapting to the informal style of communication in social networks.  The meaning expressed by emoticons has been explored to allow or improve various tasks related to the sentiment analysis, as in (Hogenboom et al., 2013, 2015).   Emojis can also be used to label excerpts of texts where they occur, thus making it possible to construct sentiment lexical.  In this context, in (Go et al., 2009) and (Castellucci et al., 2015) use a distant supervision over the emotionally marked textual contents to form a sentiment classifier and construct a lexicon of polarity.  While Novak et al. 2015 constructed lexicons and drew a map of sentiments of the 751 most used emoji.   In the work of Barbieri et al. 2017, the authors investigated the relationship between words and emojis, studying the new task of predicting which emoji are evoked by text-based tweet messages.  The authors trained several models based on Long Memory Short-Term networks (LSTMs).   In (Barbieri et al., 2016) the authors explore the meaning and use of emojis in four languages: American English, British English, Peninsular Spanish and Italian.  By performing several experiments the researchers were able to compare  how the semantics of emoji vary according to the languages.  In a first experiment, they investigated whether the meaning of a single emoji is preserved in all variations of language.  In the second experiment, they compared the general semantic models of the 150 most frequent emoji in all languages.  In this study it was possible to find out that the general semantics of the most frequent emoji is similiar.   Finally, given the context of the challenge of Semeval 2018 (task 2, subtask 1), we propose a model capable of predicting emoji corresponding to the tweets. "
    }
  ],
  [
    "CogAlign: Learning to Align Textual Neural Representations to Cognitive Language Processing Signals",
    {
      "Sentences": "Eye-tracking for NLP.  Eye-tracking data have proved to be associated with language comprehension activity in human brains by numerous research in neuroscience (Rayner, 1998;Henderson and Ferreira, 1993).  In cognitively motivated NLP, several studies have investigated the impact of eye-tracking data on NLP tasks.  In early works, these signals have been used in machine learning approaches to NLP tasks, such as part-of-speech tagging (Barrett et al., 2016), multiword expression extraction (Rohanian et al., 2017), syntactic category prediction (Barrett and Sgaard, 2015).  In neural models, eyetracking data are combined with word embeddings to improve various NLP tasks, such as sentiment analysis (Mishra et al., 2017) and NER (Hollenstein and Zhang, 2019).  Eye-tracking data have also been used to enhance or constrain neural attention in (Barrett et al., 2018;Sood et al., 2020b,a; Takmaz et al., 2020).   EEG for NLP.  Electroencephalography (EEG) measures potentials fluctuations caused by the activity of neurons in cerebral cortex.  The exploration of EEG data in NLP tasks is relatively limited.  Chen et al. (2012) improve the performance of automatic speech recognition (ASR) by using EEG signals to classify the speaker's mental state.  Hollenstein et al. (2019a) incorporate EEG signals into NLP tasks, including NER, relation extraction and sentiment analysis.  Additionally, Muttenthaler et al. (2020) leverage EEG features to regularize attention on relation extraction.   Adversarial Learning.  The concept of adversarial training originates from the Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) in computer vision.  Since then, it has been also applied in NLP (Denton et al., 2015;Ganin et al., 2016).  Recently, a great variety of studies attempt to introduce adversarial training into multi-task learning in NLP tasks, such as Chinese NER (Cao et al., 2018), crowdsourcing learning (Yang et al., 2018), cross-lingual transfer learning (Chen et al., 2018;Kim et al., 2017), just name a few.  Different from these studies, we use adversarial learning to deeply align cognitive modality to textual modality at the sentence level. "
    }
  ],
  [
    "Keep the Primary, Rewrite the Secondary: A Two-Stage Approach for Paraphrase Generation",
    {
      "Sentences": "The automatic generation of paraphrases is important for many downstream NLP applications and it has attracted a number of different approaches.  Early researches included rule-based approaches (McKeown, 1979;Meteer and Shaked, 1988) and data-driven methods (Madnani and Dorr, 2010).  With the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task.  Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality.  Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever.  Other works proposed to adopt techniques like reinforcement learning (Li et al., 2018) and unsupervised learning (Roy and Grangier, 2019) for this task.   While achieving satisfactory results, these above methods do not offer users the way to control the generation process in a fine-grained way.  To incorporate controllability into the generation model, different approaches have been proposed.  Iyyer et al. (2018) trained the model to produce the paraphrased sentence with a given syntax.  Li et al. (2019) proposed to adopt an external word aligner to train the model to generate paraphrases from different levels.  In Fu et al. (2019)'s work, the model generates paraphrases by planning the neighbour of words and realizing the complete sentence. "
    }
  ],
  [
    "Empowering Language Understanding with Counterfactual Reasoning",
    {
      "Sentences": "Counterfactual sample.  Constructing counterfactual samples has become an emergent data augmentation technique in natural language processing, which has been used in a wide spectral of language understanding tasks, including SA (Kaushik et al., 2019;Yang et al., 2020), NLI (Kaushik et al., 2019), named entity recognition (Zeng et al., 2020) question answering (Chen et al., 2020), dialogue system (Zhu et al., 2020), vision-language navigation (Fu et al., 2020).  Beyond data augmentation under the standard supervised learning paradigm, aline of research explores to incorporate counterfactual samples into other learning paradigms such as adversarial training (Zhu et al., 2020;Fu et al., 2020;Teney et al., 2020) and contrastive learning (Liang et al., 2020).  This work lies in an orthogonal direction that incorporates counterfactual samples into the decision making procedure of model inference.   Counterfactual inference.  A line of research attempts to enable deep neural networks with counterfactual thinking by incorporating counterfactual inference (Yue et al., 2021;Wang et al., 2021;Niu et al., 2021;Tang et al., 2020;Feng et al., 2021).  These methods perform counterfactual inference over the model predictions according to a pre-defined causal graph.  Due to the requirement of causal graph, such methods are hard to be generalized to different tasks.  Our method does not suffer from such limitation since working on the counterfactual samples which can be generated without a comprehensive causal graph.   Hard sample.  A wide spectral of machine learning techniques are related to dealing with the hard samples in language understanding.  For instance,   adversarial training (Khashabi et al., 2020) enhances the model robustness against perturbations and attacks, which are hard samples for normally trained models.  Debiased training (Tu et al., 2020;Utama et al., 2020) eliminates the spurious correlation or bias in training data to enhance the generalization ability and deal with out-of-distribution samples.  In addition to the training phase, a few inference techniques might improve the model performance on hard samples, including posterior regularization (Srivastava et al., 2018) and causal inference (Yu et al., 2020;Niu et al., 2021).  However, both techniques require domain knowledge such as prior or causal graph tailored for specific applications.  On the contrary, this work provides a general paradigm that can be used for most language understanding tasks. "
    }
  ],
  [
    "Improving Lexical Choice in Neural Machine Translation",
    {
      "Sentences": "The closest work to our lex model is that of Arthur et al. (2016), which we have discussed already in Section 4.  Recent work by Liu et al. (2016) has very similar motivation to that of our fixnorm model.  They reformulate the output layer in terms of directions and magnitudes, as we do here.  Whereas we have focused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes' directions with something like a margin.  Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification.   Handling rare words is an important problem for NMT that has been approached in various ways.  Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015;Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016;Gu et al., 2016;Luong et al., 2015b).  However, these methods only help with unknown words, not rare words.  An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016;Chung et al., 2016;Luong and Manning, 2016).  Our approach is different in that we try to identify and address the root of the rare word problem.  We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well.   Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation.  However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model.  It would be interesting to see if our methods benefit the Trans   former network and other models as well. "
    }
  ],
  [
    "TOI-CNN: a Solution of Information Extraction on Chinese Insurance Policy",
    {
      "Sentences": "The work of contract analysis is typically divided into two categories, segmentation and information extraction (IE).  Segmentation (Hasan et al., 2008;Loza Mencia, 2009) aims to outline the structure of a conventional text format by annotating title, section, subsection, and so on.  Information extraction (Cohen and McCallum, 2004;Piskorski and Yangarber, 2013) focuses on the classification of words, phrases or sentences.  Recent works of contract information extraction have addressed recognition of some essential elements in legal documents (Curtotti and Mccreath, 2010;Indukuri and Krishna, 2010).  Chalkidis et al. (2017) extracted the contract element, types of which are contract title, contracting parties, date, contract period, legislation refs and so on.  The extraction method was based on Logistic Regression, SVM (Chalkidis et al., 2017) and BILSTM (Chalkidis and Androutsopoulos, 2017) with POS tag embeddings and hand-crafted features.  Garcia-Constantino et al. (2017) presented the system called CLIEL for extracting information from commercial law documents.  CLIEL identified five element categories similar to the literature mentioned in (Chalkidis et al., 2017) by rule-based layout detection.  Azzopardi et al. (2016) developed a mixture extraction method of regular expressions and named entity to extract information from contract clauses, and provided an intelligent contract editing tool to lawyers.  Previous works of contract information extraction always focused on title, date, layout, contracting party, etc.  They are not directly related to the semantics of contracts, and could not provide deep insight into contract understanding.  The insurance policies are formal legal documents and usually have general elemental compositions, e.g., coverage, payment, and period.  In this paper, we investigate how to interpret insurance clauses.  Some examples of ETIP are shown in Sec.  3.   The tasks of information extraction could be Named Entity Recognition (NER) (Nadeau and Sekine, 2007;Ritter et al., 2011), Information Extraction by Text Segmentation (IETS) (Cortez and Da Silva, 2013;Hu et al., 2017), etc.  NER typically recognizes persons, organizations, locations, dates, amounts, etc.  IETS identifies attributes from semi-structured records in the form of continuous text, e.g., product description and ads.  The previous IE works on contracts (Azzopardi et al., 2016;Chalkidis et al., 2017) are similar to NER.   Recently researchers pushed the field of NER towards nested representations of named entities.  Muis and Lu (2017) incorporated mention separators to capture how mentions overlap with one another.  Both of two works relied on hand-crafted features.  Ju et al. (2018) designed a sequential stack of flat NER layers that detects nested entities.  One bidirectional LSTM layer represented word sequences and CRF layer on top of the LSTM layer decoded label sequences globally.  Katiyar and Cardie (2018) presented a standard LSTM-based sequence labeling model to learn the nested entity hypergraph structure for an input sentence.  Our ETIP problem is a variant of nested NER, called lengthy nested NER.  The type of nested entities varies from phrase to clause.  However, in the previous nested NER datesets (Kim et al., 2003;Doddington et al., 2004), the type of nested entities only contains short phrase and the average length is approximately three words. "
    }
  ],
  [
    "ArgueBERT: How To Improve BERT Embeddings for Measuring the Similarity of Arguments",
    {
      "Sentences": "Alternative Pre-Training Objectives The original BERT model uses two different pre-training objectives to train text embeddings that can be used for different NLP tasks.  Firstly masked language modeling (MLM) and secondly next sentence prediction.  However, Liu et al. (2019) have shown that BERT's next sentence prediction is not as effective as expected and that solely training on the MLM task can slightly improve the results on downstream tasks.  Since then there have been attempts to improve the pre-training of BERT by replacing the training objectives.   Lewis et al. (2020) propose, inter alia, token deletion, text infilling and sentence permutation as alternative pre-training tasks.  Their experiments show that the performance of the different pre-training objectives highly depends on the NLP task it is applied to.  Inspired by this we want to explore tasks that perform well on measuring the semantic similarity of arguments.   Lan et al. (2020) propose a sentence ordering task instead of the next sentence prediction, which is similar to our argument order prediction.  They find that sentence ordering is a more challenging task than predicting if a sentence follows another sentence.  Instead of continuous text, we use dialog data from argumentation datasets, as we hope to encode structural features of arguments into our pre-trained embeddings.   Clark et al. (2020) use replaced token detection instead of MLM, where they do not mask tokens within the sentence, but replace some with alternative tokens that also fit into the sentence.  In this way they implement a contrastive learning approach into BERT's pre-training, by training the model to differentiate between real sentences and negative samples.  Their approach outperforms a model pre-trained on MLM on all tasks.   Argument Embeddings Embeddings of textual input that encode semantic and syntactical features are crucial for NLP tasks.  Some research has already been conducted using the BERT model or its embeddings to measure the similarity of arguments.  These are described briefly in the following.   Reimers et al. (2019a) use, inter alia, BERT for argument classification and clustering as part of an open-domain argument search.  This task involves firstly classification of arguments concerning their topic, and afterwards clustering the arguments in terms of their similarity.  They achieve the best results with a fine-tuned BERT model, when incorporating topic knowledge into the network.   In a proximate work Reimers and Gurevych (2019b) introduce SBERT which serves abase for our work.  They train a BERT model in a siamese architecture to produce embeddings of textual input for tasks like semantic similarity prediction.  The model is described in detail in Section 3.1.   Dumani et al. (2020) build upon the work of Reimers et al. (2019a) and propose a framework for the retrieval and ranking of arguments, which are both sub-tasks of an argument search engine.   Thakur et al. (2020) present an optimized version of SBERT and publish anew argument similarity corpus, which we also use for evaluation in this work.  They expand the training data for the SBERT model through data augmentation, using the original BERT model for labeling sentence pairs.   To the best of our knowledge there are currently no contextualized embeddings developed especially for the task of measuring the similarity of arguments. "
    }
  ],
  [
    "Can Taxonomy Help? Improving Semantic Question Matching using Question Taxonomy",
    {
      "Sentences": "Rapid growth of community question and answer (cQA) forums have intensified the necessity for semantic question matching in QA setup.  Answer retrieval of semantically similar questions has drawn the attention of researchers in very recent times (M ` arquez et al., 2015;Nakov et al., 2016).  It solves the problem of question starvation in cQA forums by providing a semantically similar question which has already been answered.  In literature, there have been attempts to address the problem of finding the most similar match to a given question, for e.g. Burke et al. (1997) and Mlynarczyk and Lytinen (2005).  Wang et al. (2009) have presented syntactic tree based matching for finding semantically similar questions.  \u2018Similar question retrieval' has been modeled using various techniques such as topic modeling (Li and Manandhar, 2011), knowledge graph representation (Zhou et al., 2013) and machine translation (Jeon et al., 2005).  Semantic kernel based similarity methods for QA have also been proposed in (Filice et al., 2016;Croce et al., 2017;Croce et al., 2011).   Answer selection in QA forums is similar to the question similarity task.  In recent times, researchers have been investigating DL-based models for answer selection (Wang and Nyberg, 2015;Severyn and Moschitti, 2015;Feng et al., 2015).  Most of the existing works either focus on better representations for questions or linguistic information associated with the questions.  On the other hand, the model proposed in this paper is a hybrid model.  We also present a thorough empirical study of how sophisticated DL models can be used along with a question taxonomy concepts for semantic question matching. "
    }
  ]
]