<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper by following the guidelines below:

   1. Divide the summary into subsections. Use either subsections present in the text, or create subsections based on the key topics discussed in the text. Subsections must be placed in the chronological order.
   2. Include the information (e.g., definition of a concept) that helps to understand the topic of subsection if it is provided in the text. Otherwise, ignore this step.
   3. Per subsection, list the cited authors that contributed to the topic of the subsection in chronological order. The list must be in the bullet point form. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)".
   4. For each author's entry, include the contribution they made, according to the text, on the topic of the subsection. These form the authors - contribution pair. There are two ways to spot a contribution:
      - When "Author et al. (2019)" notation is met in the text, the contribution follows next. It usually ends within the same sentence if not grammatically stated otherwise.
      - When the authors’ notation within the brackets is met (e.g. "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)"), the contribution can be found before the brackets. Do not include information after the brackets in the contribution.
   5. Some contributions are proposed by the authors of the target text. These contributions can be recognized by the use of the forms of the word “we” in the text (e.g., “our approach”, ”our motivation”, “we aim”).
   6. Use one bullet point to represent the authors - contribution pair.
   7. The author - contribution notation must follow one of these structures: 
      - If one author:
         - Author et al., 2020: contribution
      - If multiple authors: 
         - Author et al., 2020, Author et al., 2023, Author et al., 2008: contribution
      - If authors of the paper:
         - Authors of the paper: contribution
   8. Be precise in the body of contribution:
      - Do not use information not directly supported by the text
      - Do not leave out information necessary to understand the summary without reading the text (e.g., abbreviation definitions)
      - Be brief, include only the essential information

Ensure that your answer is accurate, brief and doesn't contain any information not directly supported by the text. Ensure that the summary includes all authors mentioned in the text and their contributions.

Text: "Domain adaptation strategies can be separated into four categories: data selection, data generation, instance weighting and model interpolation Chu and Wang (2018).  We focus our discussion on data selection and instance weighting, as these are closely related to our approach.   Data-centric methods.  Models are trained using in-domain and out-of-domain data to evaluate out-of-domain data and compute a similarity score.  Using a cut-off threshold on these scores the training data can be selected.  Language Models Moore and Lewis (2010); Axelrod et al. (2011); Duh et al. (2013) or joint models Cuong and Sima'an (2014); Durrani et al. (2015) can traditionally be applied to score corpora.  Recently convolutional neural networks (CNN) Chen et al. (2016) were used.  Our work has similarities to this work but uses instance weighting rather than data selection.   In settings where the amount of parallel training corpora is not sufficient, generating pseudo-parallel sentences by information retrieval Utiyama and Isahara (2003), self-enhancing Lambert et al. (2011) or parallel word embeddings Marie and Fujita (2017).  Aside from generating sentences, other approaches generate monolingual n-grams Wang et al. (2014) or parallel phrase pairs Chu (2015).   In general, data-centric methods (data selection and data generation) are not SMT specific and can be directly applied to NMT.  However, because these methods are not directly related to NMT's training criterion, they only lead to minor improvements Wang et al. (2017a).   Model-centric methods.  Instance Weighting is a technique from SMT and was introduced to NMT as well Wang et al. (2017b).  An in-domain language model was trained to measure the similarity between sentences and the in-domain data via cross-entropy.  The weights are then integrated into the training objective.  We improve on their work by using state-of-the-art neural classifiers and showing that they are more effective than cross-entropy.   Two works that are closer to our work are Wang et al. (2018) and Chen et al. (2017).  In Wang et al. (2018) they generate sentence embeddings for all in-domain sentences and then measure the distance between every sentence and the in-domain core.  The underlying assumption is that the core of all in-domain sentence embeddings is atypical representative and proximity in their sentence embeddings indicates being part of the same domain.  This approach is appropriate when we have in-domain parallel text, but we study a different scenario, with no access to in-domain parallel text, which means the encoder has no access to in-domain training examples.  In Chen et al. (2017) a domain classifier is incorporated into the NMT system, using features from the encoder to distinguish between in-domain and out-of-domain data.  The classifier probabilities are used to weight sentences with regard to their similarity to in-domain data, when training the neural network.  Scaling the loss function is similar to multiplying the learning rate with the instance weight.  The classifier and NMT are trained at the same time, whereas we chose an approach with pretrained neural classifiers which are trained on a small amount of monolingual data (the scenario we study) with no access to parallel in-domain data.   Finally, while some previous work we have mentioned did look at various ways to use domain classification, such previous work has not focused on how to weight the classifier probabilities for effective use in NMT, which we showed is important for obtaining translation quality improvements, particularly when using neural classifiers which can be overconfident. "<|end|>
<|assistant|>