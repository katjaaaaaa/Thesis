<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper by following the guidelines below:

   1. Divide the summary into subsections. Use either subsections present in the text, or create subsections based on the key topics discussed in the text. Subsections must be placed in the chronological order.
   2. Include the information (e.g., definition of a concept) that helps to understand the topic of subsection if it is provided in the text. Otherwise, ignore this step.
   3. Per subsection, list the cited authors that contributed to the topic of the subsection in chronological order. The list must be in the bullet point form. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)".
   4. For each author's entry, include the contribution they made, according to the text, on the topic of the subsection. These form the authors - contribution pair. There are two ways to spot a contribution:
      - When "Author et al. (2019)" notation is met in the text, the contribution follows next. It usually ends within the same sentence if not grammatically stated otherwise.
      - When the authors’ notation within the brackets is met (e.g. "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)"), the contribution can be found before the brackets. Do not include information after the brackets in the contribution.
   5. Some contributions are proposed by the authors of the target text. These contributions can be recognized by the use of the forms of the word “we” in the text (e.g., “our approach”, ”our motivation”, “we aim”).
   6. Use one bullet point to represent the authors - contribution pair.
   7. The author - contribution notation must follow one of these structures: 
      - If one author:
         - Author et al., 2020: contribution
      - If multiple authors: 
         - Author et al., 2020, Author et al., 2023, Author et al., 2008: contribution
      - If authors of the paper:
         - Authors of the paper: contribution
   8. Be precise in the body of contribution:
      - Do not use information not directly supported by the text
      - Do not leave out information necessary to understand the summary without reading the text (e.g., abbreviation definitions)
      - Be brief, include only the essential information

Ensure that your answer is accurate, brief and doesn't contain any information not directly supported by the text. Ensure that the summary includes all authors mentioned in the text and their contributions.

Text: "NER has drawn the attention of NLP researchers because several downstream tasks such as entity linking (Gupta et al., 2017), relation extraction (Mintz et al., 2009;Liu et al., 2017), co-reference resolution (Chang et al., 2013) and conversation system (Ren et al., 2019) rely on it.  Several methods have been proposed on flat named entity recognition (Lample et al., 2016;Ma and Hovy, 2016;Strubell et al., 2017) while few of them address nested entities.  Early work on nested entities rely on hand-craft features or rule-based postprocessing (Zhang et al., 2004;Zhou et al., 2004;Zhou, 2006).  They detect the innermost flat entities with a Hidden Markov Model and then use rule-based post-processing to extract the outer entities.   While most work concerns about named entities, Lu and Roth (2015) present a novel hypergraph-based method to tackle the problem of entity mention detection.  One issue of their method is the spurious structure of hyper-graphs.  Muis and Lu (2017) improve the method of Lu and Roth (2015) by incorporating mention separators along with features.   Recent studies reveal that stacking sequence model like conditional random filed(CRF) layer can extract entities from inner to outer.  Alex et al. (2007) propose several CRF-based methods for the GENIA dataset.  However, their approach can not recognize nested entities of the same type.  Finkel and Manning (2009) present a chart-based parsing method where each named entity is a constituent in the parsing tree.  However, their method is not scalable to larger corpus with a cubic time complexity.  Ju et al. (2018) dynamically stack flat NER layers to extract nested entities, each flat layer is based on a Bi-LSTM layer and then a cascaded CRF layer.  Their model suffers error propagation from layer to layer, an inner entity can not be detected when a outer entity is identified first.   It is difficult for sequence model, like CRF, to extract nested entities where a token can be included in several entities.  Wang et al. (2018) present a transition-based model for nested mention detection using a forest representation.  One drawback of their model is the greedy training and decoding.  Sohrab and Miwa (2018) consider all possible regions in a sentence and classify them into their entity type or non-entity.  However, their exhaustive method considers too many irrelevant regions(non-entity regions) into detecting entity types and the regions are classified individually, without considering the contextual information.  Our model focuses on the boundary-relevant regions which is much fewer and the explicit leveraging of boundary information helps to locate entities more precisely."<|end|>
<|assistant|>