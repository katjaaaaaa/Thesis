<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper by following the guidelines below:

   1. Divide the summary into subsections. Use either subsections present in the text, or create subsections based on the key topics discussed in the text. Subsections must be placed in the chronological order.
   2. Include the information (e.g., definition of a concept) that helps to understand the topic of subsection if it is provided in the text. Otherwise, ignore this step.
   3. Per subsection, list the cited authors that contributed to the topic of the subsection in chronological order. The list must be in the bullet point form. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)".
   4. For each author's entry, include the contribution they made, according to the text, on the topic of the subsection. These form the authors - contribution pair. There are two ways to spot a contribution:
      - When "Author et al. (2019)" notation is met in the text, the contribution follows next. It usually ends within the same sentence if not grammatically stated otherwise.
      - When the authors’ notation within the brackets is met (e.g. "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)"), the contribution can be found before the brackets. Do not include information after the brackets in the contribution.
   5. Some contributions are proposed by the authors of the target text. These contributions can be recognized by the use of the forms of the word “we” in the text (e.g., “our approach”, ”our motivation”, “we aim”).
   6. Use one bullet point to represent the authors - contribution pair.
   7. The author - contribution notation must follow one of these structures: 
      - If one author:
         - Author et al., 2020: contribution
      - If multiple authors: 
         - Author et al., 2020, Author et al., 2023, Author et al., 2008: contribution
      - If authors of the paper:
         - Authors of the paper: contribution
   8. Be precise in the body of contribution:
      - Do not use information not directly supported by the text
      - Do not leave out information necessary to understand the summary without reading the text (e.g., abbreviation definitions)
      - Be brief, include only the essential information

Ensure that your answer is accurate, brief and doesn't contain any information not directly supported by the text. Ensure that the summary includes all authors mentioned in the text and their contributions.

Text: "Until now, NER has not received a lot of attention in the legal domain, developed approaches are fragmented and inconsistent with regard to their respective methods, datasets and typologies used.  Among the related work, there is no agreement regarding the selection of relevant semantic categories from the legal domain.  In addition, corpora or datasets of legal documents with annotated named entities do not appear to exist, which is, obviously, a stumbling block for the development of data-driven NER classifiers.  Dozier et al. (2010) describe five classes for which taggers are developed based on dictionary lookup, patternbased rules, and statistical models.  These are jurisdiction (a geographic area with legal authority), court, title (of a document), doctype (category of a document), and judge.  The taggers were tested with documents such as US case law, depositions, pleadings etc.  Cardellino et al. (2017) develop an ontology of legal concepts, making use of NERC (6 classes), LKIF (69 classes) and YAGO (358 classes).  On the NERC level, entities were divided in abstraction, act, document, organization, person, and non-entity.  With regard to LKIF, company, corporation, contract, statute etc.  are used.  Unfortunately, the authors do not provide any details regarding the questions how the entities were categorised or if there is any correlations between the different levels.  They work with Wikipedia articles and decisions of the European Court of Human Rights.  Glaser et al. (2018) use GermaNER (Benikova et al., 2015) and DBpedia Spotlight (Mendes et al., 2011;Daiber et al., 2013) for the recognition of person, location and organization entities.  References are identified based on the rules described by Landthaler et al. (2016).  The authors created an evaluation dataset of 20 court decisions. "<|end|>
<|assistant|>