<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Divide the summary into subsections. Use either subsections present in the text, or create subsections based on the key topics discussed in the text. Subsections must be placed in the chronological order. Per subsection, list the cited authors that contributed to the topic of the subsection in chronological order. The list must be in the bullet point form. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: "The automatic generation of paraphrases is important for many downstream NLP applications and it has attracted a number of different approaches.  Early researches included rule-based approaches (McKeown, 1979;Meteer and Shaked, 1988) and data-driven methods (Madnani and Dorr, 2010).  With the advances of neural networks, recent approaches tackle this problem by treating it as a sequence-to-sequence language generation task.  Prakash et al. (2016) proposed to modify the networks structure to improve the generation quality.  Cao et al. (2017), Wang et al. (2019), and Kazemnejad et al. (2020) proposed to improve the model performance by leveraging external resources, including phrase dictionary, semantic annotations, and an off-the-shelf pre-trained neural retriever.  Other works proposed to adopt techniques like reinforcement learning (Li et al., 2018) and unsupervised learning (Roy and Grangier, 2019) for this task.   While achieving satisfactory results, these above methods do not offer users the way to control the generation process in a fine-grained way.  To incorporate controllability into the generation model, different approaches have been proposed.  Iyyer et al. (2018) trained the model to produce the paraphrased sentence with a given syntax.  Li et al. (2019) proposed to adopt an external word aligner to train the model to generate paraphrases from different levels.  In Fu et al. (2019)'s work, the model generates paraphrases by planning the neighbour of words and realizing the complete sentence. "<|end|>
<|assistant|>