<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Divide the summary into subsections. Use either subsections present in the text, or create subsections based on the key topics discussed in the text. Subsections must be placed in the chronological order. Per subsection, list the cited authors that contributed to the topic of the subsection in chronological order. The list must be in the bullet point form. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: "Eye-tracking for NLP.  Eye-tracking data have proved to be associated with language comprehension activity in human brains by numerous research in neuroscience (Rayner, 1998;Henderson and Ferreira, 1993).  In cognitively motivated NLP, several studies have investigated the impact of eye-tracking data on NLP tasks.  In early works, these signals have been used in machine learning approaches to NLP tasks, such as part-of-speech tagging (Barrett et al., 2016), multiword expression extraction (Rohanian et al., 2017), syntactic category prediction (Barrett and Sgaard, 2015).  In neural models, eyetracking data are combined with word embeddings to improve various NLP tasks, such as sentiment analysis (Mishra et al., 2017) and NER (Hollenstein and Zhang, 2019).  Eye-tracking data have also been used to enhance or constrain neural attention in (Barrett et al., 2018;Sood et al., 2020b,a; Takmaz et al., 2020).   EEG for NLP.  Electroencephalography (EEG) measures potentials fluctuations caused by the activity of neurons in cerebral cortex.  The exploration of EEG data in NLP tasks is relatively limited.  Chen et al. (2012) improve the performance of automatic speech recognition (ASR) by using EEG signals to classify the speaker's mental state.  Hollenstein et al. (2019a) incorporate EEG signals into NLP tasks, including NER, relation extraction and sentiment analysis.  Additionally, Muttenthaler et al. (2020) leverage EEG features to regularize attention on relation extraction.   Adversarial Learning.  The concept of adversarial training originates from the Generative Adversarial Nets (GAN) (Goodfellow et al., 2014) in computer vision.  Since then, it has been also applied in NLP (Denton et al., 2015;Ganin et al., 2016).  Recently, a great variety of studies attempt to introduce adversarial training into multi-task learning in NLP tasks, such as Chinese NER (Cao et al., 2018), crowdsourcing learning (Yang et al., 2018), cross-lingual transfer learning (Chen et al., 2018;Kim et al., 2017), just name a few.  Different from these studies, we use adversarial learning to deeply align cognitive modality to textual modality at the sentence level. "<|end|>
<|assistant|>
1. **Eye-Tracking for NLP**:
   - **Rayner (1998)** and **Henderson and Ferreira (1993)** associated eye-tracking data with language comprehension activity in human brains.
   - In early works, eye-tracking signals were used in machine learning approaches for NLP tasks:
 	    - **Barrett et al. (2016)** applied eye-tracking data to part-of-speech tagging.
 	    - **Rohanian et al. (2017)** used eye-tracking data for multiword expression extraction.
 	    - **Barrett and Sgaard (2015)** explored eye-tracking data in syntactic category prediction.
   - Neural models combined eye-tracking data with word embeddings to improve NLP tasks:
 	    - **Mishra et al. (2017)** used eye-tracking data for sentiment analysis.
 	    - **Hollenstein and Zhang (2019)** incorporated eye-tracking data into named entity recognition (NER).
   - Eye-tracking data also influenced neural attention:
 	    - **Barrett et al. (2018)** and **Sood et al. (2020b,a)** explored enhancing or constraining neural attention using eye-tracking data.
 	    - **Takmaz et al. (2020)** investigated similar approaches.

2. **EEG for NLP**:
   - Electroencephalography (EEG) measures neuronal activity in the cerebral cortex.
   - Limited exploration of EEG data in NLP tasks:
 	    - **Chen et al. (2012)** improved automatic speech recognition (ASR) by classifying speaker mental states using EEG signals.
 	    - **Hollenstein et al. (2019a)** incorporated EEG signals into NLP tasks, including NER, relation extraction, and sentiment analysis.
 	    - **Muttenthaler et al. (2020)** used EEG features to regularize attention in relation extraction.

3. **Adversarial Learning in NLP**:
   - Adversarial training, originating from Generative Adversarial Nets (GANs) in computer vision, has been applied in NLP:
        - **Denton et al. (2015)** and **Ganin et al. (2016)** explored adversarial training.
    - Recent studies introduced adversarial training into multi-task NLP learning:
        - **Cao et al. (2018)** applied adversarial training to Chinese named entity recognition (NER).
        - **Yang et al. (2018)** used adversarial training for crowdsourcing learning.
        - **Chen et al. (2018)** and **Kim et al. (2017)** explored cross-lingual transfer learning using adversarial techniques.
        - The current study focuses on using adversarial learning to align cognitive modality with textual modality at the sentence level.<|end|>
<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Divide the summary into subsections. Use either subsections present in the text, or create subsections based on the key topics discussed in the text. Subsections must be placed in the chronological order. Per subsection, list the cited authors that contributed to the topic of the subsection in chronological order. The list must be in the bullet point form. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: "NER has drawn the attention of NLP researchers because several downstream tasks such as entity linking (Gupta et al., 2017), relation extraction (Mintz et al., 2009;Liu et al., 2017), co-reference resolution (Chang et al., 2013) and conversation system (Ren et al., 2019) rely on it.  Several methods have been proposed on flat named entity recognition (Lample et al., 2016;Ma and Hovy, 2016;Strubell et al., 2017) while few of them address nested entities.  Early work on nested entities rely on hand-craft features or rule-based postprocessing (Zhang et al., 2004;Zhou et al., 2004;Zhou, 2006).  They detect the innermost flat entities with a Hidden Markov Model and then use rule-based post-processing to extract the outer entities.   While most work concerns about named entities, Lu and Roth (2015) present a novel hypergraph-based method to tackle the problem of entity mention detection.  One issue of their method is the spurious structure of hyper-graphs.  Muis and Lu (2017) improve the method of Lu and Roth (2015) by incorporating mention separators along with features.   Recent studies reveal that stacking sequence model like conditional random filed(CRF) layer can extract entities from inner to outer.  Alex et al. (2007) propose several CRF-based methods for the GENIA dataset.  However, their approach can not recognize nested entities of the same type.  Finkel and Manning (2009) present a chart-based parsing method where each named entity is a constituent in the parsing tree.  However, their method is not scalable to larger corpus with a cubic time complexity.  Ju et al. (2018) dynamically stack flat NER layers to extract nested entities, each flat layer is based on a Bi-LSTM layer and then a cascaded CRF layer.  Their model suffers error propagation from layer to layer, an inner entity can not be detected when a outer entity is identified first.   It is difficult for sequence model, like CRF, to extract nested entities where a token can be included in several entities.  Wang et al. (2018) present a transition-based model for nested mention detection using a forest representation.  One drawback of their model is the greedy training and decoding.  Sohrab and Miwa (2018) consider all possible regions in a sentence and classify them into their entity type or non-entity.  However, their exhaustive method considers too many irrelevant regions(non-entity regions) into detecting entity types and the regions are classified individually, without considering the contextual information.  Our model focuses on the boundary-relevant regions which is much fewer and the explicit leveraging of boundary information helps to locate entities more precisely."<|end|>
<|assistant|>