<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Per concept list each cited paper's authors and contributions they've made. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: â€œTo the best of our knowledge, HappyDB is the first crowdsourced corpus of happy moments which can be used for understanding the language people use to describe happy events.  There has been recent interest in creating datasets in the area of mental health.  Althoff et al. (Althoff et al., 2016) conducted a large scale analysis on counseling conversational logs collected from short message services (SMS) for mental illness study.  They studied how various linguistic aspects of conversations are correlated with conversation outcomes.  Mihalcea et al. (Mihalcea and Liu, 2006) performed text analysis on blog posts from LiveJournal (where posts can be assigned happy/sad tags by their authors).  Lin et al. (Lin et al., 2016) measure stress from short texts by identifying the stressors and the stress levels.  They classified tweets into 12 stress categories defined by the stress scale in (Holmes and Rahe, 1967).  Last year alone, there were multiple research efforts that obtained datasets via crowdsourcing and applied natural language techniques to understand different corpora.  For example, SQuAD (Rajpurkar et al., 2016) created a largescale dataset for question-answering.  The crowdsourced workers were asked to create questions based on a paragraph obtained from Wikipedia.  They employed MTurk workers with strong experience and a high approval rating to ensure the quality of the dataset.  We did not select the workers based on their qualification for HappyDB as our task is cognitively easier than SQuAD's and we want to avoid bias in our corpus.  HappyDB is similar to SQuAD in terms of the scale of the crowdsourced dataset.  However, unlike SQuAD, which was designed specifically for studying the question answering problem, the problems that HappyDB can be used for are more open-ended. " <|end|>
    <|assistant|>
**Key Concepts:**

1. HappyDB: A crowdsourced corpus of happy moments used for analyzing language describing happiness.
2. Mental Health Datasets: Recent interest in creating datasets for mental health studies.
3. Counseling Conversational Logs: Analyzed by Althoff et al. (2016) to understand linguistic aspects correlated with conversation outcomes.
4. Blog Post Analysis: Conducted by Mihalcea and Liu (2006) on LiveJournal posts, categorizing them into happy/sad tags.
5. Stress Measurement from Short Texts: Lin et al. (2016) identified stressors and stress levels in tweets, classifying them into stress categories.
6. Crowdsourced Datasets: Various research efforts obtained datasets via crowdsourcing and applied natural language techniques.
7. SQuAD: Rajpurkar et al. (2016) created a large-scale question-answering dataset based on Wikipedia paragraphs, utilizing experienced MTurk workers.
8. Worker Selection for HappyDB: Workers were not selected based on qualification to avoid bias, unlike SQuAD's approach.
9. HappyDB vs. SQuAD: HappyDB is similar in scale but differs in its open-ended nature compared to SQuAD's specific question-answering focus.

**Contributions:**

1. Althoff et al. (2016): Analyzed counseling conversational logs to understand linguistic aspects correlated with conversation outcomes.
2. Mihalcea and Liu (2006): Conducted text analysis on LiveJournal posts, categorizing them into happy/sad tags.
3. Lin et al. (2016): Identified stressors and stress levels in tweets, classifying them into stress categories.
4. Rajpurkar et al. (2016): Created a large-scale question-answering dataset (SQuAD) from Wikipedia paragraphs using experienced MTurk workers.
5. Authors of the paper: Workers were not selected based on qualification to avoid bias in HappyDB's creation.
6. Authors of the paper: HappyDB, similar in scale to SQuAD, offers more open-ended applications compared to SQuAD's specific question-answering purpose.<|end|>
    <|user|>
    I rated this summary based on the following criteria:
    1. Quality of the summary's structure: The summary follows the paper's structure and contains necessary subsections if they were introduced in the paper. In other terms, if the text adds a concept and then provides the cited papers related to the concept, the model must structure the summary to highlight the relation between them.
    2. Quality of the summarization: The summary has no hallucinations, and all the information can be found in the source text.
    3. Quality of the sources' citations: Each cited paper mentioned in the target paper is introduced in the summary and contains a brief but clear description of what this source's contribution is.
    Based on these criteria I ranked the summary as follows:
Quality structure: 2/5 (Reasoning: big chunk structure hallucination. Case in the text: the whole "key concepts" section is unnecessary and must be removed.)
Quality summarization: 3/5 (Reasoning: one hallucination. Case in the text: "... LiveJournal posts, categorizing them into happy/sad tags.")
Quality sources citation: 4/5 (Reasoning: one paper is not mentioned)

You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Per concept list each cited paper's authors and contributions they've made. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: "To the best of our knowledge, HappyDB is the first crowdsourced corpus of happy moments which can be used for understanding the language people use to describe happy events.  There has been recent interest in creating datasets in the area of mental health.  Althoff et al. (Althoff et al., 2016) conducted a large scale analysis on counseling conversational logs collected from short message services (SMS) for mental illness study.  They studied how various linguistic aspects of conversations are correlated with conversation outcomes.  Mihalcea et al. (Mihalcea and Liu, 2006) performed text analysis on blog posts from LiveJournal (where posts can be assigned happy/sad tags by their authors).  Lin et al. (Lin et al., 2016) measure stress from short texts by identifying the stressors and the stress levels.  They classified tweets into 12 stress categories defined by the stress scale in (Holmes and Rahe, 1967).  Last year alone, there were multiple research efforts that obtained datasets via crowdsourcing and applied natural language techniques to understand different corpora.  For example, SQuAD (Rajpurkar et al., 2016) created a largescale dataset for question-answering.  The crowdsourced workers were asked to create questions based on a paragraph obtained from Wikipedia.  They employed MTurk workers with strong experience and a high approval rating to ensure the quality of the dataset.  We did not select the workers based on their qualification for HappyDB as our task is cognitively easier than SQuAD's and we want to avoid bias in our corpus.  HappyDB is similar to SQuAD in terms of the scale of the crowdsourced dataset.  However, unlike SQuAD, which was designed specifically for studying the question answering problem, the problems that HappyDB can be used for are more open-ended. " <|end|>
<|assistant|>
1. **HappyDB: Crowdsourced Corpus of Happy Moments**
	- **Authors**: The authors of the paper the text was taken from.
	- **Contribution**: Creating the first crowdsourced corpus of happy moments, which can be used to understand the language people use to describe happy events.

2. **Interest in Mental Health Datasets**
	- **Authors**: Not explicitly mentioned.
	- **Contribution**: Highlighting recent interest in creating datasets related to mental health.

3. **Althoff et al. (2016)**
	- **Authors**: Althoff et al.
	- **Contribution**: Conducting a large-scale analysis on counseling conversational logs collected from SMS for mental illness study. Studying how linguistic aspects of conversations correlate with conversation outcomes.

4. **Mihalcea et al. (2006)**
	- **Authors**: Mihalcea and Liu
	- **Contribution**: Performing text analysis on blog posts from LiveJournal, where posts can be assigned happy/sad tags by their authors.

5. **Lin et al. (2016)**
	- **Authors**: Lin et al.
	- **Contribution**: Measuring stress from short texts by identifying stressors and stress levels. Classifying tweets into 12 stress categories based on the stress scale from (Holmes and Rahe, 1967).

6. **SQuAD (Rajpurkar et al., 2016)**
	- **Authors**: Rajpurkar et al.
	- **Contribution**: Creating a large-scale dataset for question-answering. Crowdsourced workers were asked to create questions based on paragraphs from Wikipedia. Ensuring dataset quality by employing experienced MTurk workers.

7. **HappyDB vs. SQuAD**
	- **Authors**: The authors of the paper the text was taken from.
	- **Contribution**: Comparing HappyDB to SQuAD. While both have large-scale crowdsourced datasets, HappyDB is more open-ended and not specifically designed for question answering.<|end|>
<|user|>
Based on these criteria I ranked the summary as follows:
Quality structure: 1/5 (Reasoning: summary does not follow the structure of contribution in the prompt. Case in the text: the placement of **Authors** and **Contribution** on different bullet points)
Quality summarization: 5/5
Quality sources: 5/5

You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Per concept list each cited paper's authors and contributions they've made. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: "To the best of our knowledge, HappyDB is the first crowdsourced corpus of happy moments which can be used for understanding the language people use to describe happy events.  There has been recent interest in creating datasets in the area of mental health.  Althoff et al. (Althoff et al., 2016) conducted a large scale analysis on counseling conversational logs collected from short message services (SMS) for mental illness study.  They studied how various linguistic aspects of conversations are correlated with conversation outcomes.  Mihalcea et al. (Mihalcea and Liu, 2006) performed text analysis on blog posts from LiveJournal (where posts can be assigned happy/sad tags by their authors).  Lin et al. (Lin et al., 2016) measure stress from short texts by identifying the stressors and the stress levels.  They classified tweets into 12 stress categories defined by the stress scale in (Holmes and Rahe, 1967).  Last year alone, there were multiple research efforts that obtained datasets via crowdsourcing and applied natural language techniques to understand different corpora.  For example, SQuAD (Rajpurkar et al., 2016) created a largescale dataset for question-answering.  The crowdsourced workers were asked to create questions based on a paragraph obtained from Wikipedia.  They employed MTurk workers with strong experience and a high approval rating to ensure the quality of the dataset.  We did not select the workers based on their qualification for HappyDB as our task is cognitively easier than SQuAD's and we want to avoid bias in our corpus.  HappyDB is similar to SQuAD in terms of the scale of the crowdsourced dataset.  However, unlike SQuAD, which was designed specifically for studying the question answering problem, the problems that HappyDB can be used for are more open-ended. "<|end|>
<|assistant|>
## Key Concepts in HappyDB: A Crowdsourced Corpus of Happy Moments

**1. Motivation**

* Authors: Not mentioned in the excerpt
* Contribution: The paper describes the creation of HappyDB, a novel crowdsourced corpus of happy moments. This corpus aims to improve understanding of the language people use to describe positive experiences.

**2. Related Work**

* Authors: Althoff et al. (2016)
* Contribution: Analyzed counseling conversation logs to study how linguistic aspects of conversations correlate with mental health outcomes.

* Authors: Mihalcea and Liu (2006)
* Contribution: Performed text analysis on blog posts with assigned happy/sad tags to understand language use related to emotions.

* Authors: Lin et al. (2016)
* Contribution: Measured stress from short texts by identifying stressors and stress levels in tweets.

**3. Comparison with SQuAD Dataset**

* Authors: Rajpurkar et al. (2016)
* Contribution: Created a large-scale crowdsourced dataset (SQuAD) for question answering, where workers create questions based on Wikipedia passages.

* Authors: Not mentioned in the excerpt
* Contribution: HappyDB is similar to SQuAD in size but focuses on open ended exploration of happy moments through language, rather than a specific task like question answering.
<|end|>
<|user|>
I rated this summary based on the following criteria:
    1. Quality of the summary's structure: The summary follows the paper's structure and contains necessary subsections if they were introduced in the paper. In other terms, if the text adds a concept and then provides the cited papers related to the concept, the model must structure the summary to highlight the relation between them.
    2. Quality of the summarization: The summary has no hallucinations, and all the information can be found in the source text.
    3. Quality of the sources' citations: Each cited paper mentioned in the target paper is introduced in the summary and contains a brief but clear description of what this source's contribution is.
Based on these criteria I ranked the summary as follows:
Quality structure: 1/5 (Reasoning: summary does not follow the structure of contribution in the prompt. Case in the text: the placement of "*Authors:" and "*Contribution:" on different bullet points)
Quality translation: 5/5
Quality citations: 4/5 (Reasoning: one paper not mentioned)


You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts and/or subsections. Per concept list each cited paper's authors and contributions they've made. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from. The structure of each contribution must be as following:
- Author at al. (2020): contribution
To notate the authors of the paper the text was taken from, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.
For each example article summary a raking is provided. The criteria of the ratings are provided above.
Base the generated summary on the summaries above that received high ratings, and avoid the practices used in the summaries with low ratings which are explained in the "Reasoning" section next to the ratings.

Text:'There are two lines of works conducted on multimodal sentiment analysis.   One is focusing on utterance-level multimodal feature fusion.  These methods use the features of the overall utterance.  For example, they first extract the frame-level visual or acoustic features and then average them to obtain the final features, which are called utterance-level features.  The utterance-level textual features can be obtained by applying RNNs for words.  The obtained utterance-level features are fed into the fusion model to get the multimodal representation.  Some models have been proposed for effective multimodal feature fusion.  Zadeh et al. (2017) proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions.  But this method uses the three-fold Cartesian product to fuse the multimodal features, which makes the time cost very high.  To address it, Liu et al. (2018) presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.  Mai et al. (2020) proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.   The utterance-level features mainly contain global information, which may fail to capture local information.  Therefore, recent works are mostly focusing on word-level multimodal feature fusion.  And our work in this paper is also based on wordlevel features.  To extract word-level features, the first step is applying force alignment to obtain the timestamps of each word including the start time and end time.  And then following the timestamps, the utterance is split into some video clips.  Finally, word-level visual or acoustic features are obtained by averaging the frame-level features of the video clips.  Based on word-level features, lots of methods are proposed for performing word-level multimodal feature fusion.  Zadeh et al. (2018a) proposed the Memory Fusion Network(MFN) to capture the interactions across both different modalities and timesteps.  Inspired by the observation that the meaning of words often varies dynamically in different non-verbal contexts, Wang et al. (2019) proposed the Recurrent Attended Variation Embedding Network (RAVEN).  This model applies the Attention Gating module to fuse the word-level features, which can dynamically use the non-verbal features to shift the word embeddings.  Tsai et al. (2019) presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions, motivated by the great success of transformer in NLP(Vaswani et al., 2017).   Besides, there is a related work (Pham et al., 2019) need to be noticed, which proposed that translation from a source to a target modality provides away to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations.  Comparing to this work, our model has a significant difference.  That is we use the crossmodal prediction task to distinguish the shared and private non-textual features instead of training the model as an auxiliary task.  In this way, we can obtain more useful information by deeply probing the cross-modal prediction model. '<|end|>
    <|assistant|>