<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts or subsections if explicitly stated in the text. Per concept/subsection list all cited authors in order they are presented in the text. Per author notate their contribution to the topic. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from.
The contribution notation must follow this structure:
- Author at al. (2020): contribution
If multiple authors contribute to one point, use the following structure:
- Author et al. (2020), Author et al. (2023), Author et al. (2008), Author et al. (2033): contribution
To notate the authors of the text, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: There are two lines of works conducted on multimodal sentiment analysis.   One is focusing on utterance-level multimodal feature fusion.  These methods use the features of the overall utterance.  For example, they first extract the frame-level visual or acoustic features and then average them to obtain the final features, which are called utterance-level features.  The utterance-level textual features can be obtained by applying RNNs for words.  The obtained utterance-level features are fed into the fusion model to get the multimodal representation.  Some models have been proposed for effective multimodal feature fusion.  Zadeh et al. (2017) proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions.  But this method uses the three-fold Cartesian product to fuse the multimodal features, which makes the time cost very high.  To address it, Liu et al. (2018) presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.  Mai et al. (2020) proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.   The utterance-level features mainly contain global information, which may fail to capture local information.  Therefore, recent works are mostly focusing on word-level multimodal feature fusion.  And our work in this paper is also based on wordlevel features.  To extract word-level features, the first step is applying force alignment to obtain the timestamps of each word including the start time and end time.  And then following the timestamps, the utterance is split into some video clips.  Finally, word-level visual or acoustic features are obtained by averaging the frame-level features of the video clips.  Based on word-level features, lots of methods are proposed for performing word-level multimodal feature fusion.  Zadeh et al. (2018a) proposed the Memory Fusion Network(MFN) to capture the interactions across both different modalities and timesteps.  Inspired by the observation that the meaning of words often varies dynamically in different non-verbal contexts, Wang et al. (2019) proposed the Recurrent Attended Variation Embedding Network (RAVEN).  This model applies the Attention Gating module to fuse the word-level features, which can dynamically use the non-verbal features to shift the word embeddings.  Tsai et al. (2019) presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions, motivated by the great success of transformer in NLP(Vaswani et al., 2017).   Besides, there is a related work (Pham et al., 2019) need to be noticed, which proposed that translation from a source to a target modality provides away to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations.  Comparing to this work, our model has a significant difference.  That is we use the crossmodal prediction task to distinguish the shared and private non-textual features instead of training the model as an auxiliary task.  In this way, we can obtain more useful information by deeply probing the cross-modal prediction model.<|end|>
<|assistant|>
**Utterance-Level Multimodal Feature Fusion**

- **Zadeh et al. (2017)**: Proposed Tensor Fusion to capture unimodal, bimodal, and trimodal interactions. However, this method uses the three-fold Cartesian product to fuse the multimodal features, which results in high time cost.
- **Liu et al. (2018)**: Presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process.
- **Mai et al. (2020)**: Proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively.

**Word-Level Multimodal Feature Fusion**

- **Zadeh et al. (2018a)**: Proposed the Memory Fusion Network (MFN) to capture the interactions across both different modalities and timesteps.
- **Wang et al. (2019)**: Proposed the Recurrent Attended Variation Embedding Network (RAVEN). This model applies the Attention Gating module to fuse the word-level features, which can dynamically use the non-verbal features to shift the word embeddings.
- **Tsai et al. (2019)**: Presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions, motivated by the great success of transformer in NLP (Vaswani et al., 2017).
- **Pham et al. (2019)**: Proposed that translation from a source to a target modality provides a way to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations.

**Authors of the Paper**

- **Authors of the Paper**: Their work is based on word-level features. They use the crossmodal prediction task to distinguish the shared and private non-textual features instead of training the model as an auxiliary task. This approach allows them to obtain more useful information by deeply probing the cross-modal prediction model.<|end|>
<|user|>
You are a summarization model. Generate a summary of a text taken from a scientific paper. Divide the summary into key concepts or subsections if explicitly stated in the text. Per concept/subsection list all cited authors in order they are presented in the text. Per author notate their contribution to the topic. The examples of how authors can be notated in the text are "Author et al. (2019)" or "(Author et al., 2017)" or "(Author et al., 2017; Author et al., 1998; Author et al., 2022)". If no authors are introduced, assign the contribution to the authors of the paper the text was taken from.
The contribution notation must follow this structure:
- Author at al. (2020): contribution
If multiple authors contribute to one point, use the following structure:
- Author et al. (2020), Author et al. (2023), Author et al. (2008), Author et al. (2033): contribution
To notate the authors of the text, use the following structure:
- Authors of the paper: contribution
Ensure that your answer is accurate and doesn't contain any information not directly supported by the text.

Text: 'subsection(2.1 Style transfer: not too far from paraphrasing)  Similarly to Xu et al. (2012) we see language ST as a task composed of two linked subtasks: paraphrasing and style adjustment.  In our paper we claim that the ST task consists mostly in good paraphrasing and much less in adding the target style.  Following this idea, we focus on background methods for the first task paraphrasing, and then for style transfer.   Traditionally, the paraphrasing task involved rule and dictionary-based approaches (McKeown, 1983;Bolshakov and Gelbukh, 2004;Kauchak and Barzilay, 2006).  Another popular method was the statistical paraphrase generation that recombined words probabilistically in order to create new sentences (Quirk et al., 2004;Wan et al., 2005;Zhao et al., 2009).  Currently, DNNs are used for automatic paraphrasing, e.g. by sequence-to-sequence (seq2seq) models (Sutskever et al., 2014).   Presumably, the first paper presenting a deep learning model for the paraphrase task is the one by Prakash et al. (2016).  The authors successfully compared their residual LSTM to previous LSTM-derived seq2seq models (Hochreiter and Schmidhuber, 1997;Schuster and Paliwal, 1997;Bahdanau et al., 2014;Vaswani et al., 2017).  In parallel, upon research on variational autoencoders (VAEs), e.g. Chung et al. (2015), other approaches to paraphrasing emerged (Bowman et al., 2016;Gupta et al., 2018).   More recently Li et al. (2018b) implemented paraphrase generation with deep reinforcement learning that proved better performance than the previous seq2seq results on Twitter (Lan et al., 2017) and Quora (Quora, 2017) datasets.  Insufficient corpora for paraphrase generation gave motivation to new practical studies on unsupervised approach (Artetxe et al., 2018;Conneau and Lample, 2019).  Recently, Roy and Grangier (2019) proposed a monolingual system for paraphrasing (without translation) and compared it to the unsupervised and translation methods, presenting various linguistic characteristics for each of them. subsection(2.2 Style transfer methods) We name here only some existing solutions.  Xu et al. (2012) used a phrase-based MT method on Shakespeare M2A corpus.  This research was followed by Jhamtani et al. (2017), who improved the results with the DNN seq2seq approach using a copy mechanism.   Rao and Tetreault (2018) adapted phrased-based and neural MT models for formality transfer using GYAFC corpus.  Later Niu et al. (2018) improved results on GYAFC by creating a multi-task system for both formality transfer and English-French MT.   It is worth mentioning Dryjanski et al. (2018) who used DNNs both elements: generation of ST phrases and their positions related to the input sentence.   Another distinctive study was conducted for text simplification (TS) task with the matched WikipediaSimple Wikipedia parallel data, e.g. Wubben et al. (2012;Wang et al. (2016).  What is significant for TS are the results for automatic measures in Xu et al. (2016) followed by Alva-Manchego et al. (2019).  We draw our inspiration from the authors, along with the results of Xu et al. (2012) in our measure propositions.   Our solution has common features with Wieting and Gimpel (2018).  The authors demonstrated that using pretrained embeddings from a large parallel paraphrase corpus (∼50 millions) and out-of-the-box models, it was possible to reach state-of-the-art results on several SemEval semantic textual similarity competitions.  In our work, instead of using pretrained embeddings we follow Johnson et al. (2017) and train the multi-task model on a single language, but with paraphrases and a small neutral-to-style dataset.  Compared to the previous solutions, our approach can be seen as a universal method to tackle text ST (e.g.  formality transfer, simplification and more).'<|end|>
<|assistant|>