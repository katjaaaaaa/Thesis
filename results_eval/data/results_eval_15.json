{
 "results": " 1. **Research on Text Classification (TC)**:\n   - Researchers have developed datasets, tools, and models for TC.\n   - Contributions:\n     - **Chakravarthi et al. (2020b)**: Presented an overview of the OLI shared task on code-mixed texts in Dravidian languages, consisting of two subtasks A and B.\n     - **Chakravarthi et al. (2020c)**: Described the datasets used in the OLI shared tasks.\n     - **Chakravarthi et al. (2020a)**: Described the datasets used in the TC tasks for code-mixed texts.\n\n2. **Code-Mixed Text Classification**:\n   - Comparatively less work has been done on the classification of code-mixed texts.\n   - Contributions:\n     - **Renjit and Idicula (2020)**: Proposed two models based on different configurations of LSTM for the OLI shared task, achieving a weighted F1-score of 0.53 for Romanized Malayalam text in Subtask B.\n     - **Arora (2020)**: Trained a Universal Language Model (LM) for Ma-En code-mixed texts and transferred it to a TC model from the fastai library, obtaining 0.91 and 0.74 weighted F1-scores for Subtask A and Romanized Malayalam text of Subtask B respectively.\n\n3. **Sentiment Analysis of Code-Mixed Texts**:\n   - Focus on sentiment analysis of code-mixed texts in Ta-En and Ma-En language pairs.\n   - Contributions:\n     - **Sun and Zhou (2020)**: Proposed the XLM-Roberta model for sentiment analysis, achieving a weighted F1-score of 0.65 and 0.74 for Ta-En and Ma-En language pairs respectively.\n     - **Ou and Li (2020)**: Proposed another XLM-Roberta model that ensembles pre-trained multi-language models and K-folding method, obtaining 0.63 and 0.74 weighted F1-scores for Ta-En and Ma-En language pairs respectively."
}