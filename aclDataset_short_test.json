[
  [
    "Lexicon Guided Attentive Neural Network Model for Argument Mining",
    {
      "Sentences": "Neural networks have been used in varieties of AM tasks.  To improve the vanilla LSTM model, Stab et al. (2018a) use attention mechanism to fuse topic and sentence information together.  In the work of Laha and Raykar (2016), they present several bi-sequence classification models on different datasets.  However, rather than using some sophisticated architecture such as attention, it considers only different concatenation or condition method on the output of LSTM.  Eger et al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components.   Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018).  In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models.  Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores.  Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words.  Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task.   However, in AM, seldom works directly combine lexicon with models.  By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision.  Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM.  Al-Khatib et al. (2016) consider the distant supervision method.  Most of these works use the end-to-end training paradigm with the outside resources only for generating the weak label, which may not fully leverage the information of the lexicons. "
    }
  ],
  [
    "FontLex: A Typographical Lexicon based on Affective Associations",
    {
      "Sentences": "We begin with a review of previous studies and tools that have approached the topic of semantic attributes of fonts or the goal of recommending fonts.   subsection(2.1.  Semantic Attributes of Fonts)  Through a crowdsourced study, O'Donovan et al. (2014) associate 200 fonts with 37 semantic attributes (e.g., happy).  They ask users to pick one of two presented fonts for a given attribute, and then based on these selections assign scores between 0 and 100 for each font-attribute combination.  The resulting dataset is publicly available and will be discussed further in Section 3.  Kulahcioglu and de Melo (2018) extend the above crowdsourced dataset using deep Convolutional Neural Network (CNN) embeddings as a means of obtaining a similarity measure between fonts.  To predict semantic attribute scores for a font outside the dataset, the authors take weighted averages of the nearest four font scores, as determined by the embeddings.  Based on leave-one-out cross validation test results, the method is able to predict scores with around 9% mean absolute error.  In an online survey conducted by Shaikh et al. (2006), the characteristics of 20 fonts are assessed with respect to 15 adjective pairs (e.g., stable unstable).  The fonts are presented using alphabetic, numeral, and common symbols.  Further studies (Velasco et al., 2014;Velasco et al., 2015) analyze the relationship between visual font characteristics and taste attributes (sweet, sour, etc.) through user studies.  They conclude that round fonts exhibit an association with sweet taste.  Finally, many font-focused websites (Sam Berlow and Sherman, 2017;dafont.com, 2017;Bloch, 2017) allow contributors to tag fonts with attributes, some of which are more semantic than visual.   subsection(2.2.  Font Recommendation)  O'Donovan et al. (2014) present a method of proposing fonts that are similar to a given font that is currently being used.  In their experiments, they find that semantic attributes are more conducive to predicting the similarity of fonts than geometrical features.  Thus, making use of a set of semantic attributes, they learn a font similarity metric based on crowdsourced comparisons, in which users need to assess which of two presented fonts is more similar to a provided reference font.  Wang et al. (2015) rely on a deep learning approach to find similar fonts.  It is claimed that a qualitative comparison of both methods reveals this approach as producing better results than the former one by O'Donovan et al. (2014).  Using vector representations, Qiao (2017) aims to identify fonts that are both contrasting and complementary.  The system can either propose a novel pair of fonts, or suggest a second font for an already specified one.  The vector representations are provided online.  The force-directed graph visualization developed by Data Scope Analytics (2017) displays 458 fonts and 1,807 cousages gathered by Sam Berlow and Sherman (2017).  The visualization in Ho (2017) displays around 800 font embeddings mapped into a 2D space.  Several websites, including those of Sam Berlow and Sherman (2017), Canva.com (2017) and Mills (2017), provide font pair suggestions gathered from users or from other web sources.   subsection(2.3.  Impact of Font Choices)  A number of Stroop-style studies have been conducted to investigate the effect of font characteristics on perception.  Hazlett et al. (2013) asked users to judge whether a displayed word is positive or negative, comparing 5 fonts and 25 words that are all strongly associated with positive or negative emotion.  The results indicate that congruent typefaces yield faster responses.  Lewis and Walker (1989) ask users to press a left hand key if the words slow or heavy appear, versus aright hand one if fast or light appears.  In a second experiment, they display related words (e.g., fox) instead of the original words (e.g., fast) to ensure that the user needs to grasp the meaning of the displayed word.  In both experiments, they repeat the tasks with congruent and incongruent fonts, finding that the former significantly reduce the response time.  In terms of survey-style studies, Juni and Gross (2008) present newspaper articles using two different fonts.  Their survey reveals that the same text is perceived as more humorous or angry when read in a certain font compared to another.  Shaikh (2007b) presents documents to participants using congruent, incongruent, and neutral fonts, while soliciting ratings to assess the perception of the document (e.g., as exciting) as well as the perceived personality of the author (e.g., in terms of trustworthiness).  The findings show strong effects across the assessed font types with respect to the perception of documents, whereas congruent and neutral fonts appear to evoke similar perceptions of an author's personality.  Shaikh et al. (2007) study the effect of the choice of font on email perception.  Their results suggest that fonts with low congruency may result in different perceptions of an email than fonts with medium to high congruency.  A similar study on the perception of a company website (Shaikh, 2007a) demonstrates that neutral and low congruency fonts can negatively affect a company's perception in terms of professionalism, believability, trust, and intent to act on the site.  Many studies in marketing analyze font effects, especially in packaging design.  For instance, Fligner (2013) shows that fonts associated with the attribute natural increase the perceived healthfulness of products when used in their packaging, particularly if the products' intrinsic cues (e.g., being fat-free) and extrinsic ones (e.g., being sold at Whole     Foods Market) also concur.  Childers and Jass (2002) establish that the semantic attributes of a font bear an impact on user perception for both high and low engagement levels.  Through experiments using bottled water of a fictional brand, Van Rompay and Pruyn (2011) finds additional evidence that the congruence between fonts and other design elements influence the perception of brand credibility, aesthetics, and value. "
    }
  ],
  [
    "Probabilistic Verb Selection for Data-to-Text Generation",
    {
      "Sentences": "The most successful NLG applications, from the commercial perspective, have been data-to-text NLG systems which generate textual descriptions of databases or datasets (Reiter, 2007;Belz and Kow, 2009).  A typical example is the automatic generation of textual weather forecasts from weather data that has been used by Environment Canada and UK Met Office (Goldberg et al., 1994;Belz, 2008;Sripada et al., 2014).  The TREND system (Boyd, 1998) focuses on generating descriptions of historical weather patterns.  Their method concentrates primarily on the detection of upward and downward trends in the weather data, and uses a limited set of verbs to describe different types of movements.  Ramos-Soto et al. (2013) also address the surface realization of weather trend data by creating an \"intermediate language\" for temperature, wind etc.  and then using four different ways to verbalize temperatures based on the minimum, maximum and trend in the time frame considered.  An empirical corpus-based study of human-written weather forecasts has been conducted in SUMTIMEMOUSAM (Reiter et al., 2005), and one aspect of their research focused on verb selection in weather forecasts.  They built a classifier to predict the choice of verb based on type (speed vs.  direction), information content (change or transition from one wind state to another) and near-synonym choice.  There is more and more interest in using NLG to enhance accessibility, for example by describing data in the form of graphs etc.  to visually impaired people.  In such NLG systems, there has also been exploration into the generation of text for trend data which should be automatically adapted to users' reading levels (Moraes et al., 2014).  There exists wide-spread usage of NLG systems on the financial and business data.  For example, the SPOTLIGHT system developed at A.C.  Nielsen automatically generated readable English text based on the analysis of large amounts of retail sales data.  For another example, in 2016 Forbes reported that FactSet used NLG to automatically write hundreds of thousands of company descriptions a day.  It is not difficult to imagine that different kinds of such data-to-text NLG systems can be utilized by a modern chatbot like Amazon Echo or Microsoft XiaoIce (Shum et al., 2018) to enable users access a variety of online data resources via natural language conversation.   Typically, a complete data-to-text NLG system implements a pipeline which involves both content selection (\"what to say\") and surface realization (\"how to say\").  In recent years, researchers have made much progress in the end-to-end joint optimization of those two aspects: Angeli et al. (2010) treat the generation process as a sequence of local decisions represented by log-linear models; Konstas and Lapata (2013) employ a probabilistic context-free grammar (PCFG) specifying the structure of the event records and complement it with an n -gram language model as well as a dependency model; the most advanced method to date is the LSTM recurrent neural network (RNN) based encoder-aligner-decoder model proposed by Mei et al. (2016) which is able to learn content selection and surface realization together directly from database-text pairs.  The verb selection problem that we focus on in this paper belongs to the lexicalization step of content selection, more specifically, sentence planning.  Similar to the above mentioned joint optimization methods, our approach to verb selection is also automatic, unsupervised, and domain-independent.  It would be straightforward to generalize our proposed model to select other types of words (like adjectives and adverbs), or even textual templates as used by Angeli et al. (2010), to describe numerical data.  Due to its probabilistic nature, our proposed model could be plugged into, or interpolated with, a bigger end-to-end probabilistic model (Konstas and Lapata, 2013) relatively easily, but it is not obvious how this model could fit into a neural architecture (Mei et al., 2016).   The existing work on lexicalization that is most similar to ours is a corpus based method for verb selection developed by Smiley et al. (2016) at Thomson Reuters.  They analyze the usage patterns of verbs expressing percentage changes in a very large corpus, the Reuters News Archive.  For each verb, they calculate the interquartile range (IQR) of its associated percentage changes in the corpus.  Given anew percentage change, their method randomly selects a verb from those verbs whose IQRs cover the percentage in question, with equal probabilities.  A crowdsourcing based evaluation has demonstrated the superiority of their verb selection method to the random baseline that just chooses verbs completely randomly.  It is notable that their method has been incorporated into Thomson Reuters Eikon, their commercial datato-text NLG software product for macro-economic indicators and mergers-and-acquisitions deals (Plachouras et al., 2016).  We will make experimental comparisons between our proposed approach and theirs in Section 5. "
    }
  ],
  [
    "Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations",
    {
      "Sentences": "subsection(2.1 Natural Question Generation)  Previous research has directly approached the task of automatically generating questions for many useful applications such as augmenting data for the QA tasks (Li et al., 2018;Sun et al., 2018;Zhang and Bansal, 2019), helping semantic parsing (Guo et al., 2018) and machine reading comprehension (Yu et al., 2020;Yuan et al., 2017), improving conversation quality (Mostafazadeh et al., 2016;Dong et al., 2019), and providing student exercises for education purposes (Koncel-Kedziorski et al., 2016a).   Various NQG methods are developed which can be divided into two categories: heuristic based approaches and neural network based approaches (Pan et al., 2019;Kurdi et al., 2020).  The former generates questions in two stages: it first obtains intermediate symbolic representations and then constructs the natural language questions by either rearranging the surface form of the input sentence or generating with pre-defined question templates.  The latter neural approaches view the NQG task as a sequence-to-sequence (seq2seq) learning problem and jointly learn generation process in an endto-end manner (Yao et al., 2018;Zhou et al., 2018).   subsection(2.2 Math Word Problem Generation)  Different from standard NQG tasks, generating MWPs not only needs the syntax, semantics and coherence of the output narratives, but requires understandings of the underlying symbolic representations and the arithmetic relationship between quantities.  In general, MWP generation approaches can be divided into three categories: (1) template based approaches; (2) rewriting based approaches; and (3) neural network based approaches.   Template based approaches usually fall into a similar two-stage process: they first generalize an existing problem into a template or a skeleton, and then generate the MWP sentences from the templates (Williams, 2011;Polozov et al., 2015;Bekele, 2020).  Deane and Sheehan (2003) used semantic frames to capture both scene stereotypical expectations and semantic relationships among words and utilized a variant of second-order predicate logic to generate MWPs.  Wang and Su (2016) leveraged the binary expression tree to represent the story of the MWP narrative and composed the natural language story recursively via a bottom-up tree traversal.  Template based approaches heavily rely on the tedious and limited hand-crafted templates, leading to very similar generated results.  This cannot meet the demand of a large number of high-quality and diverse MWPs.   Rewriting based approaches target the MWP generation problem by editing existing human-written MWP sentences to change their theme without changing the underlying story (Koncel-Kedziorski et al., 2016a; Moon-Rembert and Gilbert, 2019).  For example, Koncel-Kedziorski et al. (2016a) proposed a rewriting algorithm to construct new texts by substituting thematically appropriate words and phrases.  Rewriting based approaches are more flexible compared with templates based approaches.  However, there are several drawbacks that prevent them from providing the large number of MWPs.  First, the generation process is based on existing MWPs, which significantly limits the generation ability.  Second, students easily fall into rote memorization since it is too trivial to notice that the underlying mathematical equations are still unchanged.   Recent attempts have been focused on exploiting neural network based approaches that generating MWPs from equations and topics in an end-toend manner (Zhou and Huang, 2019;Liyanage and Ranathunga, 2020).  Zhou and Huang (2019) designed a neural network with two encoders to fuse information of both equations and topics and dualattention mechanism to generate relevant MWPs.  Liyanage and Ranathunga (2020) tackled the generation problem by using the long short term memory network with enhanced input features, such as character embeddings, word embeddings and part-of-speech tag embeddings.   The closest work to our approach is Zhou and Huang (2019) and the main differences are as follows: (1) Zhou and Huang (2019) directly encode the equation by a single-layer bidirectional gated recurrent unit (GRU), while we first convert equations into Levi graph and conduct the encoding by the GGNN model; (2) instead of directly using the pre-trained embeddings of similar words given the topic, we choose to learn the topic relevant representations from an external CSKG; and (3) we choose to use the VAE framework to promoting more diverse results. "
    }
  ],
  [
    "Evaluating the Representational Hub of Language and Vision Models",
    {
      "Sentences": "Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017;Linzen et al., 2016;Alishahi et al., 2017;Zhang and Bowman, 2018;Conneau et al., 2018).  Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017;Shekhar et al., 2017;Suhr et al., 2017).   Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn anew task better, faster, or with less data.  Transfer learning has proved successful in computer vision (e.g.  Razavian et al. (2014)) as well as in computational linguistics (e.g., Conneau et al. (2017)).  However, little has been done in this respect for visually grounded natural language processing models.   In this work, we combine these different research lines and explore transfer learning techniques in the domain of language and vision tasks.  In particular, we use the FOIL diagnostic dataset (Shekhar et al., 2017) and investigate to what extent skills learned through different multimodal tasks transfer.   While transfering the knowledge learned by a pre-trained model can be useful in principle, Conneau et al. (2018) found that randomly initialized models provide strong baselines that can even outperfom pre-trained classifiers (see also Wieting and Kiela (2019)).  However, it has also been shown that these untrained, randomly initialized models can be more sensitive to the size of the training set than pretrained models are (Zhang and Bowman, 2018).  We will investigate these issues in our experiments. "
    }
  ],
  [
    "Learning a Policy for Opportunistic Active Learning",
    {
      "Sentences": "Active learning methods aim to identify examples that are likely to be the most useful in improving a supervised model.  A number of metrics have been proposed to evaluate examples, including uncertainty sampling (Lewis and Gale, 1994), densityweighted methods (Settles and Craven, 2008), expected error reduction (Roy and McCallum, 2001), query by committee (Seung et al., 1992), and the presence of conflicting evidence (Sharma and Bilgic, 2016); as surveyed by Settles (2010).  Some of these metrics can be extended to the multilabel setting, by assuming that one-vs-all classifiers are learned for each label, and that all the learned classifiers are comparable (Brinker, 2006;Singh et al., 2009;Li et al.).  Label statistics have also been incorporated into heuristics for selecting instances to be queried (Yang et al., 2009;Li and Guo, 2013).  There have also been Bayesian approaches that select both an instance and label to be queried (Qi et al., 2009;Vasisht et al., 2014).  Our work aims to learn a policy for choosing between queries that can use information from many such indicators, but learns to combine them appropriately for a given task.   Thomason et al. (2017) define the setting of opportunistic active learning, and apply it to an interactive task of grounding natural language descriptions of objects.  They compare two static policies to demonstrate that using opportunistic queries improves task performance.  We try to learn the optimal policy for this task using reinforcement learning, and compare to a policy similar to theirs.   Recently, there has been interest in using reinforcement learning to learn a policy for active learning.  Fang et al. (2017) use deep Qlearning to acquire a policy that sequentially examines unlabeled examples and decides whether or not to query for their labels; using it to improve named entity recognition in low resource languages.  Also, Bachman et al. (2017) use metalearning to jointly learn a data selection heuristic, data representation and prediction function for a distribution of related tasks.  They apply this to one shot recognition of characters from different languages, and in recommender systems.  In contrast to these works, we learn a policy for a task that contains both possible actions that are active learning queries, and actions that complete the current task, thus resulting in a greater explorationexploitation trade-off.   More similar to our setup is that of Woodward and Finn (2017) which uses reinforcement learning with a recurrent-neural-network-based Qfunction in a sequential one-shot learning task to decide between predicting a label and acquiring the true label at a cost.  This setup also has a higher cost than standard active learning where the test set is separated out.  This is a continuous task without clearly separated interactions or episodes.  In our setting, each episode or interaction allows for querying and requires completion of an interaction, which further increases the trade-off between model improvement and exploitation.  Further, we consider a multilabel setting, which increases the number of actions at each decision step.   There are other works that employ various types of turn-taking interaction to learn models for language grounding.  Some of these use a restricted vocabulary (Cakmak et al., 2010;Kulick et al., 2013), or additional knowledge of predicates (for example that \"red\" is a color) (Mohan et al., 2012).  Others do not use active learning (Kollar et al., 2013;Parde et al., 2015;De Vries et al., 2017;Yu et al., 2017), or do not learn a policy that guides the interaction (Vogel et al., 2010;Thomason et al., 2016, 2017).   Also related to our work is the use of reinforcement learning in dialog tasks, such as slotfilling and recommendation (Wen et al., 2015;Pietquin et al., 2011), understanding natural language instructions or commands (Padmakumar et al., 2017;Misra et al., 2017), and open domain conversation (Serban et al., 2016;Das et al., 2017).  These typically do not use active learning.  In our task, the policy needs to trade-off model improvement against task completion. "
    }
  ],
  [
    "Assessing SRL Frameworks with Automatic Training Data Expansion",
    {
      "Sentences": "Overview on SRL frameworks FrameNet defines frame-specific roles that are shared among predicates evoking the same frame.  Thus, generalization across predicates is possible for predicates that belong to the same existing frame, but labeling predicates for unseen frames is not possible.  Given the high number of frames FrameNet covers about 1,200 frames and 10K role labels large training datasets are required for system training.   PropBank offers a small role inventory of five core roles (A0 to A4) for obligatory arguments and around 18 roles for optional ones.  The core roles closely follow syntactic structures and receive a predicate-specific interpretation, except for the Agent-like A0 and Patient-like A1 that implement Dowty's proto-roles theory (Dowty, 1991).   VerbNet defines about 35 semantically defined thematic roles that are not specific to predicates or predicate senses.  Predicates are labeled with Levintype semantic classes (Levin, 1993).  VerbNet is typically assumed to range between FrameNet with respect to its rich semantic representation and PropBank with its small, coarse-grained role inventory.   Comparison of SRL frameworks Previous experimental work compares VerbNet and PropBank: Zapirain et al. (2008) find that PropBank SRL is more robust than VerbNet SRL, generalizing better to unseen or rare predicates, and relying less on predicate sense.  Still, they aspire to use more meaningful VerbNet roles in NLP tasks and thus propose using automatic PropBank SRL for core role identification and then converting the PropBank roles into VerbNet roles heuristically to VerbNet, which appears more robust in cross-domain experiments compared to training on VerbNet data.  Merlo and van der Plas (2009) also confirm that PropBank roles are easier to assign than VerbNet roles, while the latter provide better semantic generalization.  To our knowledge, there is no experimental work that compares all three major SRL frameworks.   German SRL frameworks and data sets The SALSA project (Burchardt et al., 2006;Rehbein et al., 2012) created a corpus annotated with over 24,200 predicate argument structures, using English FrameNet frames as a basis, but creating new frames for German predicates where required.   About 18,500 of the manual SALSA annotations were converted semi-automatically to PropBank-style annotations for the CoNLL 2009 About 18,500 of the manual SALSA annotations were converted semi-automatically to PropBank-style annotations for the CoNLL 2009 shared task on syntactic and semantic dependency labeling (Hajic et al., 2009).  Thus, the CoNLL dataset shares a subset of the SALSA annotations.  To create PropBank-style annotations, the predicate senses were numbered such that different frame annotations for a predicate lemma indicate different senses.  The SALSA role labels were converted to PropBank-style roles using labels A0 and A1 for Agentand Patient-like roles, and continuing up to A9 for other arguments.  Instead of spans, arguments were defined by their dependency heads for CoNLL.  The resulting dataset was used as a benchmark dataset in the CoNLL 2009The resulting dataset was used as a benchmark dataset in the CoNLL 2009 shared task.   For VerbNet, Mujdricza-Maydt et al. (2016) recently published a small subset of the CoNLL shared task corpus with VerbNet-style roles.  It contains 3,500 predicate instances for 275 predicate lemma types.  Since there is no taxonomy of verb classes for German corresponding to original VerbNet classes, they used GermaNet (Hamp and Feldweg, 1997) to label predicate senses.  GermaNet provides a fine-grained sense inventory similar to the English WordNet (Fellbaum, 1998).   Automatic SRL systems for German State-ofthe-art SRL systems for German are only available for PropBank labels: Bjorkelund et al. (2009) developed mate-tools; Roth and Woodsend (2014) and Roth and Lapata (2015) improved on matetools SRL with their mateplus system.  We base our experiments on the mateplus system.    Training data generation In this work, we use a corpus-based, monolingual approach to training data expansion.  Furstenau and Lapata (2012) propose monolingual annotation projection for lowerresourced languages: they create data labeled with FrameNet frames and roles based on a small set of labeled seed sentences in the target language.  We apply their approach to the different SRL frameworks, and for the first time to VerbNet-style labels.   Other approaches apply cross-lingual projection (Akbik and Li, 2016) or paraphrasing, replacing FrameNet predicates (Pavlick et al., 2015) or PropBank arguments (Woodsend and Lapata, 2014) in labeled texts.  We do not employ these approaches, because they assume large role-labeled corpora. "
    }
  ],
  [
    "Using mention accessibility to improve coreference resolution",
    {
      "Sentences": "A particularly successful way to leverage mention classification has been to specialise modelling by mention type.  Denis and Baldridge (2008) learn five different models, one each for proper name, definite nominal, indefinite nominal, third person pronoun, and non-third person pronoun.  Bengtson and Roth (2008) and Durrett and Klein (2013) implement specialisation at the level of features within a model, rather than explicitly learning separate models.  Bengtson and Roth (2008) prefix each base feature generated with the type of the current mention, one of proper name, nominal, or pronoun, for instance nominal-head match:true.  Durrett and Klein (2013) extend from this by learning a model over three versions of each base feature: unprefixed, conjoined with the type of the current mention, and conjoined with concatenation of the types of the current mention and candidate antecedent mention: nominal+nominal-head match=true.   The success of Durrett and Klein is possible due to the large training dataset provided by OntoNotes (Pradhan et al., 2007).  In this work, we successfully extend data-driven specialisation still further: Section 4 shows how we can discover fine-grained patterns in reference expression usage, and Section 5 how these patterns can be used to significantly improve the performance of a strong coreference system. "
    }
  ],
  [
    "Building a Sentiment Corpus of Tweets in Brazilian Portuguese",
    {
      "Sentences": "Several works present new methods and approaches for tasks such as polarity classification (Turney, 2002;Pang and Lee, 2005), detection of irony (Carvalho et al., 2009;Reyes et al., 2012) and aspect extraction in text (Hu and Liu, 2004).  One of the major issues of this area is the building of datasets for evaluating methods and for training machine learning models.  Turney (2002), one of the first works on polarity classification, used product reviews labeled as \"recommended\" and \"not recommended\".  The source of the data was a website called Epinions, where users could evaluate products and leave a five star score for each review.  The authors considered any review with less than 3 stars as \"not recommended\".  Pang et al. (2002) uses a similar score (star rating) in order to compile a corpus of movie reviews on three classes (positive, negative and neutral).  The automatic approach worked very well for building large datasets, but the method limited research on domains where users input an objective score.  Despite of the challenges of the manual annotation, researches began building new datasets by training annotators to label the data.  Socher et al. (2013) introduces Stanford Sentiment Treebank, a relabeling of the previous IMDB corpus presented in (Pang and Lee, 2005).  SemEval, an important semantic evaluation event, also produces several datasets for English designed for SA tasks (Nakov et al., 2016).  Some authors even used distant supervision techniques for automatic labeling large datasets quickly using features such as emoticons (Go et al., 2009).  In Brazilian Portuguese, several works presented corpora for SA.  Freitas et al. (2012) introduce ReLi, a sentiment corpus of book reviews manually annotated in three classes (positive, neutral and negative).  The authors have chosen books from different genres in order to vary the linguistic phenomena in the corpus (from teenage books to literature classics).  ReLi contains annotation of semantic ori entation, part-of-speech tagging and aspect of opinion, and it was later used as resource for researches in SA (Balage et al., 2013;Brum et al., 2016).  One of the issues on this corpus observed on the literature is the unbalanced classes the majority of sentences is neutral (72%), while the negative class represents only 4% of the data.  On the product review domain, Hartmann et al. (2014) presented Buscape corpus, a large corpora in Brazilian Portuguese.  The corpus contains 13.685 reviews labeled as positive and negative, using scores given by users on Buscape, a popular e-commerce website.  A similar dataset is Mercado Livre corpus, introduced in Avanco (2015), containing 43.818 product reviews also labeled automatically and balanced between the two classes.  Silva et al. (2011) collected a corpus from Twitter in Portuguese.  The dataset was collected by searching two entities in the social network (Dilma and Serra, two running candidates at the time) and manually annotated as positive or negative.  The corpus contains 76.358 documents balanced between positive and negative.  The corpus was originally constructed for sentiment stream analysis meaning it contains several retweets and links, phenomena that may interfere on sentiment classification but is vital to maintain the stream for the former task.  Also on binary polarity classification, Moraes et al. (2015) introduce the Corpus 7x1, a brazilian portuguese corpus on Twitter comments during the 2014 World Cup semi-finals.  The corpus presents some interesting user behavior such as irony, sarcasm, cheering and angry due to the final match score.  Corpus 7x1 contains 2.728 tweets labeled manually in three classes the neutral class represents tweets that do not align with either positive or negative sentiments.  Moraes et al. (2016) also uses Twitter as the source of data, but compile a corpus of computer products containing subsection(2.317 tweets.  The data is manually labeled in three classes) and the authors also performed experiments on SA using lexical-based classifiers and SVM.  A large Twitter corpus was compiled by (Correa Junior et al., 2017) using distant supervision.  The authors labeled tweets in Brazilian Portuguese using emojis representing positive and negative sentiments following the work of Go et al. (2009) in English.  The corpus contains 554.623 positive tweets and 425.444 negative.  The approach is a fast way to label data, but the method can not guarantee the absence of noise data such as irony, sarcasm or incorrect labels. "
    }
  ],
  [
    "Supervised Clustering of Questions into Intents for Dialog System Applications",
    {
      "Sentences": "Intent is a key concept for building dialog systems and is therefore a central research topic in the area.  In particular, recent general-purpose dialog systems have to rely on extensive intent modeling to be able to correctly analyze a wide variety of user queries.  This has led to a considerable amount of research on data-driven intent modeling.   In particular, Xu et al. (2013) represent query's intent as trees and employ a procedure for mapping an NL query into a tree-structured intent.  The problem of this approach is that anew set of intent trees is required for new domains.  Kim et al. (2016); Celikyilmaz et al. (2011) use semisupervised approaches with large amounts of unlabeled data to improve the accuracy in mapping user queries into intents.  However, they still require a small amount of labeled data in order to learn a given intent.  Chen et al. (2016a) train a Convolutional Deep Neural Network to jointly learn the representations of human intents and associated utterances.  Chen et al. (2016b) propose feature-enriched matrix factorization to model open domain intents.  This leverages knowledge from Wikipedia and Freebase to acquire information from unexplored domains according to new users' requests.  Unfortunately, it also requires external knowledge bases to induce concepts appearing within the intents.   Approaching the same problem from the opposite direction, several studies investigate algorithms for automatic question clustering.  Wen et al. (2001) propose to cluster together queries that lead to the same group of web pages that are frequently selected by users.  Jeon et al. (2005) use machine translation to estimate word translation probabilities and retrieve similar questions from question archives.  Li et al. (2008) try to infer the intent of unlabeled queries according to the proximity with respect to the labeled queries in a click graph.  Beitzel et al. (2007) propose to automatically classify web queries from logs into a set of topics by using a combination of different techniques, either supervised or unsupervised.  The extracted topics are further used for efficient web search.  Deepak (2016) presents MiXKmeans, a variation of k-means algorithm, suited for clustering threads present on forums and Community Question Answering websites.  However, most techniques use unsupervised clustering to group similar questions/queries, without modeling intents.  In contrast, our study relies on supervised clustering to learn intent-based similarity.   Finally, our work is related to a large body of research on dialog acts (Stolcke et al., 2000;Kim et al., 2010;Chen et al., 2018): our low-level intent labels (Table 1) can be seen as very finegrained dialog acts (Core and Allen, 1997;Bunt et al., 2010;Oraby et al., 2017).   However, our paper's objective is different as our goal is not to rigidly define intents and then exploit them to derive a semantic interpretation.  We focus on two contributions: first, we aim at providing a tool to help implementing dialog managers such that the designer can more easily create categories from precomputed clusters.  Note that having in mind hundreds of questions to create intent category from scratch is clearly an exigent task.   Second, our approach can dynamically cluster questions with the same semantics, without any concept annotation.  Indeed, the important concepts will be learned from the training data, which constitutes a much simpler annotation task than the creation of adhoc dialogue acts.  In particular, this can help with domain-specific intents: the domain-level semantics will be learned from data with no need for advanced manual engineering. "
    }
  ],
  [
    "Improving Hypernymy Extraction with Distributional Semantic Classes",
    {
      "Sentences": "subsection(2.1.  Extraction of Hypernyms)  In her pioneering work, Hearst (1992) proposed to extract hypernyms based on lexical-syntactic patterns from text.  Snow et al. (2004) learned such patterns automatically based on a set of hyponym-hypernym pairs.  Pantel and Pennacchiotti (2006) presented another approach for weakly supervised extraction of similar extraction patterns.  These approaches use some training pairs of hypernyms to bootstrap the pattern discovery process.  For instance, Tjong Kim Sang (2007) used web snippets as a corpus for extraction of hypernyms.  More recent approaches exploring the use of distributional word representations for extraction of   hypernyms and co-hyponyms include (Roller et al., 2014;Weeds et al., 2014;Necsulescu et al., 2015;Vylomova et al., 2016).  They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation.  Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains.  Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym.  Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms.  Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016).  Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure.  Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl text corpus to ensure high lexical coverage.  In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date.   subsection(2.2.  Taxonomy and Ontology Learning)  Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999;Biemann, 2005;Cimiano, 2006;Bordea et al., 2015;Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007;Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013;Braslavski et al., 2016).  Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011;Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies.  Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns.  Graph algorithms are used to induce a proper tree from the binary relations harvested from text.   subsection(2.3.  Induction of Semantic Classes)  This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts.  While this approach performs a hard clustering and does not label clusters, these drawbacks are addressed in (Pantel and Lin, 2002), where words can belong to several clusters, thus representing senses, and in (Pantel and Ravichandran, 2004), where authors aggregate hypernyms per cluster, which come from Hearst patterns.  The main difference to our approach is that we explicitly represent senses both in clusters and in their hypernym labels, which enables us to connect our sense clusters into a global taxonomic structure.  Consequently, we are the first to use semantic classes to improve hypernymy extraction.  Ustalov et al. (2017b) proposed a synset induction approach based on global clustering of word senses.  The authors used the graph constructed of dictionary synonyms, while we use distributionally-induced graphs of senses. "
    }
  ],
  [
    "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
    {
      "Sentences": "Transfer learning for domain adaptation Prior work has shown the benefit of continued pretraining in domain (Alsentzer et al., 2019;Chakrabarty et al., 2019;Lee et al., 2019).  We have contributed further investigation of the effects of a shift between a large, diverse pretraining corpus and target domain on task performance.  Other studies (e.g., Huang et al., 2019) have trained language models (LMs) in their domain of interest, from scratch.  In contrast, our work explores multiple domains, and is arguably more cost effective, since we continue pretraining an already powerful LM.   Task-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g.  in Howard and Ruder, 2018;Phang et al., 2018;Sun et al., 2019).  In the presence of domain shift between train and test data distributions of the same task, domain-adaptive pretraining (DAPT) is sometimes used to describe what we term TAPT (Logeswaran et al., 2019;Han and Eisenstein, 2019).  Related approaches include language modeling as an auxiliary objective to task classifier fine-tuning (Chronopoulou et al., 2019;Radford et al., 2018) or consider simple syntactic structure of the input while adapting to task-specific data (Swayamdipta et al., 2019).  We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets.  See Table 11 in Appendix A for a summary of multi-phase pretraining strategies from related work.   Data selection for transfer learning Selecting data for transfer learning has been explored in NLP (Moore and Lewis, 2010;Ruder and Plank, 2017;Zhang et al., 2019, among others).  Dai et al. (2019) focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in 5.2.  Concurrent to our work, Aharoni and Goldberg (2020) propose data selection methods for NMT based on cosine similarity in embedding space, using DISTILBERT (Sanh et al., 2019) for efficiency.  In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks.  Khandelwal et al. (2020) introduced k NN-LMs that allows easy domain adaptation of pretrained LMs by simply adding a datastore per domain and no further training; an alternative to integrate domain information in an LM.  Our study of human-curated data 5.1 is related to focused crawling (Chakrabarti et al., 1999) for collection of suitable data, especially with LM reliance (Remus and Biemann, 2016).   What is a domain? Despite the popularity of domain adaptation techniques, most research and practice seems to use an intuitive understanding of domains.  A small body of work has attempted to address this question (Lee, 2001;Eisenstein et al., 2014;van der Wees et al., 2015;Plank, 2016;Ruder et al., 2016, among others).  For instance, Aharoni and Goldberg (2020) define domains by implicit clusters of sentence representations in pretrained LMs.  Our results show that DAPT and TAPT complement each other, which suggests a spectra of domains defined around tasks at various levels of granularity (e.g., Amazon reviews for a specific product, all Amazon reviews, all reviews on the web, the web). "
    }
  ],
  [
    "Overview on NLP Techniques for Content-based Recommender Systems for Books",
    {
      "Sentences": "In their paper "A survey of book recommender systems", Alharthi et al. (2017) present a detailed survey on different approaches to book recommendation, compiled from over 30 papers up to 2017.  These publications report results from CBF, CF and other methods obtained on the BookCrossing, LitRec, LibraryThing, INEX, and Amazon reviews datasets.  Only LitRec (Vaz et al.) dataset uses data from GoodReads.   In the current study, we will focus on models for GoodReads built on the goodbooks-10k dataset.  Recent publications written on this dataset mostly deal with collaborative filtering.  Out of 11 unique papers in English on recommender systems retrieved by Google Scholar when search is performed for goodbooks-10k (Le, 2019;Kula, 2017;Recommendation; Greenquist et al., 2019;Zhang et al., 2019, 2018;Paudel et al., 2018;Khanom et al., 2019;Kouris et al., 2018;Yang et al., 2018;Hiranandani et al., 2019), 10 examine algorithms for Collaborative filtering, two (Le, 2019;Greenquist et al., 2019) implement hybrid systems, and only one (Le, 2019) implement a simple content-based recommender.  We will examine the content-based systems or components of hybrid systems, developed on goodbooks-10k, and will compare them to systems using another dataset for GoodReads LitRec.   subsection(3.1 Overview)  An overview of published content-based approaches for GoodReads is shown in Table 1.   In their bachelor thesis, Le (2019) implement simple collaborative, content-based and hybrid systems for book recommendations.  Their content-based recommender uses only ratings data and leaves aside books metadata.  They achieve a best score of 0.842 of root-mean-square error (RMSE) for FunkSVD algorithm.     Greenquist et al. (2019) implement a CBF/CF hybrid system.  To gather more information, they merge goodbooks-10k data with Amazon reviews data.  For books representations, they use tfidf vectors of the books descriptions, tags, and shelves.  Authors report using book descriptions in their content-based approach, but it is unclear how they obtained the descriptions, as the latter are not present in the goodbooks-10k dataset.   In addition to published ones, there are many other approaches to the goodbook-10k dataset, implemented and shared by community members on platforms such as Kaggle.com.  On Kaggles goodbook-10k dataset page, there are 31 shared kernels.  Some of them contain demonstrations on the development of content-based systems, including ones that use tags information.  It can be seen that, as mentioned in (Greenquist et al., 2019), tags are turned into tf-idf vectors, and cosine similarity is used for determining the books that are the most similar to a given one.   Alharthi and Inkpen (2019) use the Litrec dataset to develop a book recommendation system based on the linguistic features of the books.  Litrec dataset has ratings of 1,927 users of 3,710 literary books and contains the complete text of books tagged with part-of-speech labels.   The content-based systems that Alharthi and Inkpen develop are based on the analysis of lexical, character-based, syntactic, characterization and style features of the books texts.  Feature sets are learned from book texts converted into a numerical value using one-hot encoding.   For linguistics analysis, the authors use Linguistic Inquiry and Word Count (LIWC) (Tausczik and Pennebaker, 2010), which is a popular resource that focuses on grammatical, content and psychological word categories.  Using LIWC 2015Using LIWC 2015 dictionary, Alharthi and Inkpen compute 94 categories, such as: percent of latinate words, function words, affect words, social words, perpetual processes etc.   Other text measurements are computed by using GutenTags built-in tagger (Brooke et al., 2015), which uses a stylistic lexicon to calculate stylistic aspects usually considered when analyzing English literature.  The six styles are colloquial (informal) vs.  literary, concrete vs.  abstract and subjective vs.  objective.  In addition, they use the fiction-aware named entity recognizer LitNER to identify number of characters and number of locations mentioned in a book.  Finally, a text readability measurement is introduced, by calculating the Flesch reading ease score.  All 120 features are used for finding most similar books using knearest neighbours (kNN) and Extreme Trees (ET) algorithms, and are tested against CBF baselines: LDA, LSI, VSM and Doc2vec.  Both kNN and ET achieved higher scores than the baselines in both precision@10 (0.36 and 0.37, respectively) and recall@10 (0.17 for both).   Unfortunately, in goodbooks-10k, the book content is not available, and it would be extremely hard to gather and process this data.  Therefore introducing stylometry and content features, as the ones mentioned above, would be impossible.  The only suitable way of incorporating linguistic features would be by analyzing tags or by scraping books descriptions and reviews available at GoodReads.  subsection(3.2 Critiques on Content-Based)  Recommenders  The main critique with regards to the systems developed on goodreads-10k is the lack of usage of textual data, such as tags available.  Even in the cases where tags are used, no attention has been paid to the fact that most of the tags follow a hierarchical structure (eg, \"biographical\", \"biographical-fiction\", \"biographical-memoir\") and some have similar or equal meaning (e.g.  \"ya-dystopian\", \"youngadult-dystopian\", \"teen-dystopian\", \"dystopian\", \"antiutopian\", \"utopia-dystopia\").   In order to deal with hierarchy, ambiguity and synonyms, some data normalization and natural language processing techniques could be adopted.  Tf-idf vectorization is a common technique in text processing; however, it is arguable whether it is the most suitable way for vectorizing tags information, as their distribution does not necessarily follow the one of words in natural text.   Another observation is that the systems developed do not take advantage of the recent advances in machine learning algorithms, especially deep learning, despite the large volume of the data available.   subsection(3.3 General Critiques)  One general critique on the observed publications is the lack of standardization in dataset preparation and of evaluation metrics usage.  Firstly, some redefine a \"like\" as having a rating of at least 3 stars, while others dont provide a clear definition of a like.  Secondly, some drop users having less than 5 ratings, others less than 10, and yet others seem not to take out any users.  And lastly, since the initial dataset does not come with established training and test sets, the way different researchers have performed the train/test split seem to diverge.  All these three factors result in different datasets used by the researchers, and therefore, lead to noncomparable results.   Another factor that makes the results noncomparable is the difference in evaluation metrics used.  As it can be seen in Table 1, there is a huge variety of metrics, such as those that measure rating predictions (root mean squared error (RMSE)), ranking metrics (precision@k, recall@K, mean average precision (MAP)), metrics for coverage (catalog coverage (CC)), metrics for personalization, diversity and novelty.  Without suitable, unified metrics, it would be impossible to credibly prove that the use of NLP techniques will significantly improve RS performance on the current task.   Since general dataset preparation and the choice of the evaluation metrics is a considerable topic, not central for this research proposal, we would like to leave it for future research. "
    }
  ],
  [
    "Filtering Noisy Parallel Corpus using Transformers with Proxy Task Learning",
    {
      "Sentences": "In the WMT18 shared task, participants mostly used similar techniques in components as prefiltering, scoring the sentence pairs, and using a classifier for feature functions.  Teams applied prefiltering rules to eliminate noisy data, including:  \u2022 short or lengthy sentences;  \u2022 sentence pairs with few words and unbalanced token lengths; \u2022 sentence pairs with unmatched names, numbers, web addresses, etc.;  \u2022 sentences where a language identifier fails to identify a source or target language type.   Scoring functions were mostly used to correlate qualified texts.  Participants also used sentence embeddings (Bouamor and Sajjad, 2018;Axelrod et al., 2011;Artetxe and Schwenk, 2019) altogether with a similarity function to detect the similarity of pairs.  The WMT19 shared task focused on lowresource languages, namely Nepali-English and Sinhala-English.  Participants mostly applied basic filtering techniques similar to those used in 2018.  Chaudhary et al. (2019) used sentence embeddings that were trained on parallel sentence pairs.  Another approach was to train a machine translation system on the clean data and then used it to translate the non-English side to make a comparison.  Several metrics were used to match sentence pairs such as METEOR, Levenshtein distance, and BLEU.   We found that our work relates to the submission from Bernier-Colborne and Lo (2019).  However, their submission was unable to show the effectiveness of the proposed method due to potential issues in the pretraining process.  Besides the parallel corpus filtering task, we come across several works utilizing a similar approach.  In Yang et al. (2019), BERT rescoring method is more effective at bitext mining than heuristic scoring methods, i.e., marginal cosine distance.  In Gregoire and Langlais (2018), a similar negative random sampling technique has been used for generating synthetic bad pairs.  Also, attempts to create harder negative pairs were proven effective in bitext mining (Guo et al., 2018). "
    }
  ],
  [
    "Leveraging Visual Question Answering to Improve Text-to-Image Synthesis",
    {
      "Sentences": "Initial T2I approaches (Reed et al., 2016;Dash et al., 2017) adopted the conditional GAN (cGAN) (Mirza and Osindero, 2014) and AC-GAN (Odena et al., 2016) ideas to replace the conditioning variable by a text embedding which allows to condition the generator on a textual description.  Analog to many current approaches, our approach is also based on AttnGAN (Xu et al., 2017), which incorporates an attention mechanism on word features to allow the network to synthesize fine-grained details.   In terms of using VQA for T2I, to our knowledge the only other approach is VQA-GAN (Niu et al., 2020).  However, in contrast to their approach, our architecture is simpler, we do not use layout information, work on individual QA pairs, and produce higher resolution images (256x256 vs.  128x128). "
    }
  ],
  [
    "Deep Neural Networks for Syntactic Parsing of Morphologically Rich Languages",
    {
      "Sentences": "Both the baseline (Berkeley parser) and the current state-of-the-art model on the SPMRL Shared Task 2014 (Bjorkelund et al., 2014) rely on probabilistic context free grammar (PCFG)-based features.  The latter uses a product of PCFG with latent annotation based models (Petrov, 2010), with a coarse-to     fine decoding strategy.  The output is then discriminatively reranked (Charniak and Johnson, 2005) to select the best analysis.  In contrast, the parser used in this paper constructs the parse tree in a greedy manner and relies only on word, POS tags and morphological embeddings.   Several other papers have reported results for the SPMRL Shared Task 2014.  (Hall et al., 2014) introduced an approach where, instead of propagating contextual information from the leaves of the tree to internal nodes in order to refine the grammar, the structural complexity of the grammar is minimized.  This is done by moving as much context as possible onto local surface features.  This work was refined in (Durrett and Klein, 2015), taking advantage of continuous word representations.  The system used in this paper also leverages words embeddings but has two major differences.  First, it proceeds step-by-step in a greedy manner (Durrett and Klein, 2015) by using structured inference (CKY).  Second, it leverages a compositional node feature which propagates information from the leaves to internal nodes, which is exactly what is claimed not to be done.   (Fernandez-Gonzalez and Martins, 2015) proposed a procedure to turn a dependency tree into a constituency tree.  They showed that encoding order information in the dependency tree make it isomorphic to the constituent tree, allowing any dependency parser to produce constituents.  Like the parser we used, their parser do not need to binarize the treebank as most of the others constituency parsers.  Unlike this system, we do not use the dependency structure as an intermediate representation and directly perform constituency parsing over raw words. "
    }
  ],
  [
    "Subtopic-driven Multi-Document Summarization",
    {
      "Sentences": "subsection(2.1 Extractive Methods)  Extractive methods select sentences from documents to form a summary.  A typical framework is based on a graph, where sentences are vertices and similarities between sentences are edge weights, e.g., TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004).  They apply a random walk to explore the relationships among sentences and then produce a sentence ranking.  MEAD (Radev et al., 2000) is a centroid-based method, which ranks sentences based on a set of features, including centroid value, positional value, first sentence overlap and redundancy.   However, the graph-based models do not perform well when the sentence graph is not well connected.  Wan and Yang (2008) claim that the document set is usually composed of a few themes, which are represented by sets of sentences.  They apply conditional Markov Random Walk and HITS on clusters, separately.  Alternatively, Haghighi and Vanderwende (2009) adopt a hierarchical Latent Dirichlet Allocation based model to discover the multiple themes within a document set.  Similarly, Gong et al. (2010) use the theme structure to define the representation for each sentence.  However, their solutions only consider statistical knowledge.  Semantic and contextual information among sentences are neglected.   Following the idea of themes, Banerjee et al. (2015) suggest that the sentences in the most important document of the set are relevant to the sentences in the other documents.  Hence, they cluster sentences based on those in the most important document.  However, there may not always exist a most important document in the set.  Further, the relationships among documents are not necessarily to be conclusive.  In fact, there are many kinds of relationships among documents (e.g., similar, complementary, or evolutionary).   Liu et al. (2015) adopt the idea of reconstruction.  They apply a two-level sparse representation model and reconstruct the document by extracted sentences with constraints.  Similarly, Ma et al. (2016) try to minimize the reconstruction error between selected sentences and the document set with a neural model.  Cao et al. (2017) make use of multi-task learning by incorporating classification task with summarization to train better sentence representations for reconstruction.   subsection(2.2 Abstractive Methods)  Summarizing multiple documents in an abstractive way is even harder.  The generation should take many inputs into account.  Some abstractive methods on single document summarization have been proposed recently (See et al., 2017;Zhou et al., 2018), and they benefit from an attentive sequence-to-sequence model (Sutskever et al., 2014).  In fact, abstractive approaches for MDS are more like words or phrases recombination.   Bing et al. (2015) propose an abstractive MDS solution.  They extract salient noun and verb phrases from a constituency tree, then produce sentences with representative phrases via integer linear programming.  Later, Li et al. (2017b 2017c) adopt a similar two-stage model, but they first estimate sentence and phrase salience via an auto-encoder framework.   Recently, some studies have turned to readers' comments to help identify crucial points to include in a summary.  Li et al. (2015) propose a sparse coding method to generate summaries that not only cover key content in news but also focuses highlighted by readers' comments.  However, they do not consider semantic information.  Later, they propose a deep learning method (Li et al., 2017a) that jointly models the focuses of news set and readers' comments.  But they do not tackle sequential context information among sentences and treat them as separate instances.  In contrast, we deal with sequential context information within each document and the relationships among documents. "
    }
  ],
  [
    "NoiseQA: Challenge Set Evaluation for User-Centric Question Answering",
    {
      "Sentences": "Question Answering QA systems have a rich history in NLP, with early successes in domainspecific applications (Green et al., 1961;Woods, 1977;Wilensky et al., 1988;Hirschman and Gaizauskas, 2001).  Considerable research effort has been devoted to collecting datasets to support a wider variety of applications (Quaresma and Pimenta Rodrigues, 2005;Monroy et al., 2009;Feng et al., 2015;Liu et al., 2015;Nguyen, 2019;Jin et al., 2019) and improving model performance on them (Lally et al., 2017;Wang et al., 2018;Yu et al., 2018;Yang et al., 2019).  We too focus on QA systems but center the utility to users rather than new applications or techniques.   There has also been interest in studying the interaction between speech and QA systems.  Lee et al. (2018a) examine transcription errors for Chinese QA, and Lee et al. (2018b) propose Spoken SQuAD, with spoken contexts and text-based questions, but they address a fundamentally different use case of searching through speech.  Closest to our work is that of Peskov et al. (2019), which studies mitigating ASR errors in QA, assuming whitebox access to the ASR systems.  Most such work automatically generates and transcribes speech using TTS-ASR pipelines, similar to how our synthetic set is constructed.  However, our results show that TTS does not realistically replicate human voice variation.  Besides, stakeholders relying on commercial transcription services will not have whitebox access to ASR; our post-hoc mitigation strategies would be better suited for such cases.  Challenge sets Model robustness evaluation with adversarial schemes is common in NLP tasks (Smith, 2012), including dependency parsing (Rimell et al., 2009), information extraction (Schneider et al., 2017), natural language inference (Marelli et al., 2014;Naik et al., 2018;Glockner et al., 2018), machine translation (Isabelle et al., 2017;Belinkov and Bisk, 2018;Bawden et al., 2018;Burlot and Yvon, 2017) and QA (Jia and Liang, 2017;Aspillaga et al., 2020).  Unlike most prior work, we do not create our challenge sets to break QA systems, but rather for a more realistic evaluation of the systems' real-world utility. "
    }
  ],
  [
    "Combining CNNs and Pattern Matching for Question Interpretation in a Virtual Patient Dialogue System",
    {
      "Sentences": "Question identification has been formulated as at least two distinct tasks.  Multi-class logistic regression is a standard approach that can take advantage of class-specific features but requires a good amount of training data for each class.  A pairwise setup involves a more general binary classification decision which is then made for each label, choosing the highest confidence match.   Early work (Ravichandran et al., 2003) found that treating a question answering task as a maximum entropy re-ranking problem outperformed using the same system as a classifier.  DeVault et al. (2011) observed maximum entropy systems performed well with simple n-gram features.  Jaffe et al. (2015) explored a log-linear pairwise ranking model for question identification and found it to outperform a multiclass baseline along the lines of DeVault et al. However, Jaffe et al. (2015) used a much smaller dataset with only about 915 user turns, less than one-fourth as many as in the current dataset.  For this larger dataset, multiclass logistic regression outperforms a pairwise ranking model.  With no pairwise comparisons, a multiclass classifier is also much faster, lending itself to real-time use.   It is probable that multiclass vs.  pairwise approaches' overall effectiveness depends on the amount of training data; pairwise ranking methods have potential advantages for cross-domain and one-shot learning tasks (Vinyals et al., 2016) where data is sparse or non-existent.  In the closely related task of short-answer scoring, Sakaguchi et al. (2015a) found that pairwise methods could be effectively combined with regression-based approaches to improve performance in sparse-data cases.   Other work involving dialogue utterance classification has traditionally required a large amount of data.  For example, Suendermann et al. (2009) acquired 500,000 dialogues with over 2 million utterances, observin that statistical systems outperform rule-based ones as the amount of data increases.  Crowdsourcing for collecting additional dialogues (Ramanarayanan et al., 2017) could alleviate data sparsity problems for rare categories by providing additional training examples, but this technique is limited to more general domains that do not require special training/skills.  In the current medical domain, workers on common crowdsourcing platforms are unlikely to have the expertise required to take a patient's medical history in a natural way, so any data collected with this method would likely suffer quality issues and fail to generalize to real medical student dialogues.  Rossen and Lok (2012) have developed an approach for collecting dialogue data for virtual patient systems, but their approach does not directly address the issue that even as the number of dialogues collected increases, there can remain along tail of relevant but infrequently asked questions.   CNNs have been used to great effect for image identification (Krizhevsky et al., 2012) and are becoming common for natural language processing.  In general, CNNs are used for convolution over input language sequences, where the input is often a matrix representing a sequence word embeddings (Kim, 2014).  Intuitively, word embedding kernels are convolving n-grams, ultimately generating features that represent n-grams over word vectors of length equal to the kernel width.  CNNs are very popular in systems for tasks like paraphrase detection (Yin and Schutze, 2015;Yin et al., 2016;He et al., 2015), community question answering (Das et al., 2016;Barbosa et al., 2016) and even machine translation (Gehring et al., 2017).  Characterbased models that embed individual characters as input units are also possible, and have been used for language modeling (Kim et al., 2016) to good effect.  It is worth noting that character sequences are more robust to spelling errors and potentially have the same expressive capability as word sequences given long enough character sequences. "
    }
  ],
  [
    "Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations",
    {
      "Sentences": "Emotion Recognition in Conversation Several studies have tackled the ERC task.  Hazarika et al. (2018a,b) used memory networks for recognizing humans emotion in conversation, where two distinct memory networks consider the interspeaker interaction.  DialogueRNN (Majumder et al., 2019) employs an attention mechanism for grasping the relevant utterance from the entire conversation.  More related to our method is the DialogueGCN model proposed by Ghosal et al. (2019), in which RGAT is used for modeling both self-dependency and inter-speaker dependency.  This model has achieved state-of-the-art performance on several conversational datasets.  On the other hand, as away of considering contextual information, Luo and Wang (2019) proposed to propagate each of the utterances into an embedded vector.  Likewise, a pre-trained BERT model (Devlin et al., 2018) has been used for generating dialogue features to combine several utterances by inserting separate tokens (Yang et al., 2019).   Graph Neural Network Graph-based neural networks are used in various tasks.  The fundamental model is the graph convolutional network (GCN) (Kipf and Welling, 2016), which uses a fixed adjacency matrix as the edge weight.  Our method is based on RGCN (Schlichtkrull et al., 2018) and GAT (Velickovic et al., 2017).  The RGCN model prepares a different structure for each relation type and hence considers selfdependency and inter-speaker dependency separately.  The GAT model uses an attention mechanism to attend to the neighborhood's representations of the utterances.   Position Encodings In our work, positional information is added to the graphical structure.  Several studies add position encodings to several structures, such as self-attention networks (SANs) and GCN.  SANs (Vaswani et al., 2017) perform the attention operation under the position-unaware assumption, in which the positions of the input are ignored.  In response to this issue, the absolute position (Vaswani et al., 2017) or relative position (Shaw et al., 2018), or structure position (Wang et al., 2019) are used to capture the sequential order of the input.  Similarly, graph-based neural networks do not take sequential information.  In the design of proteins, the relative spatial structure between proteins is modeled in order to account for the complex dependencies in the protein sequence and is applied to the edges of the graph representations (Ingraham et al., 2019). "
    }
  ],
  [
    "From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains",
    {
      "Sentences": "In the following, we give abroad overview of existing EL approaches, annotation support and HumanIn-The-Loop annotation.   Entity Linking describes the task of disambiguating mentions in a text against a knowledge base.  It is typically approached in three steps: 1) mention detection, 2) candidate generation and 3) candidate ranking (Shen et al., 2015) (Fig.  2).  Mention detection most often relies either on gazetteers or pretrained named entity recognizers.  Candidate generation either uses precompiled candidate lists derived from labeled data or uses full-text search.  Candidate ranking assigns each candidate a score, then the candidate with the highest score is returned as the final prediction.  Existing systems rely on the availability of certain resources like a large Wikipedia as well as software tools and often are restricted in the knowledge base they can link to.  Off-the-shelf systems like Dexter (Ceccarelli et al., 2013), DBPedia Spotlight (Daiber et al., 2013) and TagMe (Ferragina and Scaiella, 2010) most often can only link against Wikipedia or a related knowledge base like Wikidata or DBPedia.  They require good Wikipedia coverage for computing frequency statistics like popularity, view count or PageRank (Guo et al., 2013).  These features work very well for standard datasets due to their Zipfian distribution of entities, leading to high reported scores on stateof-the art datasets (Ilievski et al., 2018;Milne and Witten, 2008).  However, these systems are rarely applied out-of-domain such as in digital humanities or classical studies.  Compared to state-of-the-art approaches, only a limited amount of research has been performed on entity linking against domainspecific knowledge bases.  AGDISTIS (Usbeck et al., 2014) developed a knowledge-base-agnostic approach based on the HITS algorithm.  The mention detection relies on gazetteers compiled from resources like Wikipedia and thereby performs string matching.  Brando et al. (2016) propose REDEN, an approach based on graph centrality to link French authors to literary criticism texts.  It requires additional linked data that is aligned with the custom knowledge base-they use DBPedia.  As we work in a domain-specific low resource setting, access to large corpora which can be used to compute popularity priors is limited.  We do not have suitable named entity linking tools, gazetteers or a sufficient amount of labeled training data.  Therefore, it is challenging to use state of the art systems.   Human-in-the-loop annotation HITL machine learning describes an interactive scenario where a machine learning (ML) system and a human work together to improve their performance.  The ML system gives predictions, and the human corrects if they are wrong and helps to spot things that have been overlooked by the machine.  The system uses this feedback to improve, leading to better predictions and thereby reducing the effort of the human.  In natural language processing, it has been applied in scenarios like interactive text summarization (Gao et al., 2018), parsing (He et al., 2016) or data generation (Wallace et al., 2019).  Regarding machine-learning assisted annotation, Yimam et al. (2014) propose an annotation editor that during annotation, interactively trains a model using annotations made by the user.  They use string matching and MIRA (Crammer and Singer, 2003) as recommenders, evaluate on POS and NER annotation and show improvement in annotation speed.  TASTY (Arnold et al., 2016) is a system that is able to perform EL against Wikipedia on the fly while typing a document.  A pretrained neural sequence tagger is being used that performs mention detection.  Candidates are precomputed and the candidate is chosen that has the highest text similarity.  The system updates its suggestions after interactions such as writing, rephrasing, removing or correcting suggested entity links.  Corrections are used as training data for the neural model.  However, due to the following reasons, it is not yet suitable for our scenario.  In order to overcome the cold start problem, it needs annotated training data in addition to a precomputed index for candidate generation.  It also only links against Wikipedia. "
    }
  ],
  [
    "Automatically Identifying Gender Issues in Machine Translation using Perturbations",
    {
      "Sentences": "Our study builds on the literature around gender bias in machine translation.  Cho et al. (2019) use sentence templates to probe for differences in Korean pronouns.  Prates et al. (2019) and Stanovsky et al. (2019) also use sentence templates, but filled with word lists, of professions and adjectives in the former, and professions in the latter.  A separate but related line of work focuses on generating correct inflections when translating to gender-marking languages (Vanmassenhove et al., 2018;Moryossef et al., 2019). "
    }
  ],
  [
    "Knowledge-Guided Paraphrase Identification",
    {
      "Sentences": "subsection(5.1 Paraphrase Identification)  Traditional methods for paraphrase identification (PI) are based on word or string similarity measurements.  VBS (Mihalcea et al., 2006) applies cosine similarity with tf-idf weighting.  STS (Islam and Inkpen, 2009) and KM (Kozareva and Montoyo, 2006) measure the similarity based on both semantic and string similarity.  MCS (Mihalcea et al., 2006) obtains the similarity scores based on multiple word similarity computation methods.   Recently, deep learning methods advance the performance for PI.  REL-TK (Filice et al., 2015), L.D.C Model (Wang et al., 2016) and MultiPerspective CNN (He et al., 2015) employ convolutional neural network (CNN) to extract features for similarity measurement.  SAMS-RecNN (Cheng and Kartsaklis, 2015) and SHPNM (Socher et al., 2011) model sentence representations via recursive neural networks.  ESIM (Chen et al., 2017), PWIM (He and Lin, 2016) and SSE (Nie and Bansal, 2017) apply LSTM to learn sentence representations for predictions.  Both DecAtt (Parikh et al., 2016) and ESIM (Chen et al., 2017) employ attention to learn the interactions between two sentences.   BERT (Devlin et al., 2019) and other pre-trained language models (Liu et al., 2019) achieve stateof-the-art performance on PI.  However, existing works do not incorporate domain knowledge for  PI, and hence, cannot achieve satisfactory performance on domain specific PI task.  Different from existing works, the proposed method exploits the unstructured knowledge and applies a novel gating mechanism to automatically aggregate the lexical and syntactic information for predictions.   subsection(5.2 Knowledge-enhanced Language model)  Incorporating external knowledge into language model is effective for downstream tasks and recently attracts lots of attentions.  Recent works (Zhang et al., 2019;Liu et al., 2020;Xiong  et al., 2019;Peters et al., 2019;Cui et al., 2020;Song et al., 2021;Hu et al., 2019;Ye et al., 2019) explore how to introduce knowledge graphs to enhance language models for downstream tasks.  However, knowledge graph may be not available for each domain since its construction needs lots of human efforts.  Moreover, a structured knowledge graph contains entities and relations, but the knowledge associated with each entity may be incomplete, which may be difficult to provide enough help for paraphrase identification.   To take advantage of unstructured knowledge, a lot of works (Chakraborty et al., 2020;He et al., 2020b; Beltagy et al., 2019;Peng et al., 2019;Huang et al., 2019;Lee et al., 2020) propose to pre-train language models on domain specific text.  However, the pre-training objective function is usually not designed to capture knowledge concepts and their explanations, and only leads to limited improvement with intensive computation costs.  Compared with the existing works, our proposed method can leverage external knowledge effectively to achieve significant improvements without a computationally expensive pre-training stage. "
    }
  ],
  [
    "Do Neural Dialog Systems Use the Conversation History Effectively? An Empirical Study",
    {
      "Sentences": "Since this work aims at investigating and gaining an understanding of the kinds of information a generative neural response model learns to use, the most relevant pieces of work are where similar analyses have been carried out to understand the behavior of neural models in other settings.  An investigation into how LSTM based unconditional language models use available context was carried out by Khandelwal et al. (2018).  They empirically demonstrate that models are sensitive to perturbations only in the nearby context and typically use only about 150 words of context.  On the other hand, in conditional language modeling tasks like machine translation, models are adversely affected by both synthetic and natural noise introduced anywhere in the input (Belinkov and Bisk, 2017).  Understanding what information is learned or contained in the representations of neural networks has also been studied by \"probing\" them with linear or deep models (Adi et al., 2016;Subramanian et al., 2018;Conneau et al., 2018).   Several works have recently pointed out the presence of annotation artifacts in common text and multi-modal benchmarks.  For example, Gururangan et al. (2018) demonstrate that hypothesisonly baselines for natural language inference obtain results significantly better than random guessing.  Kaushik and Lipton (2018) report that reading comprehension systems can often ignore the entire question or use only the last sentence of a document to answer questions.  An and et al. (2018) show that an agent that does not navigate or even see the world around it can answer questions about it as well as one that does.  These pieces of work suggest that while neural methods have the potential to learn the task specified, its design could lead them to do so in a manner that doesn't use all of the available information within the task.   Recent work has also investigated the inductive biases that different sequence models learn.  For example, Tran et al. (2018) find that recurrent models are better at modeling hierarchical structure while Tang et al. (2018) find that feedforward architectures like the transformer and convolutional models are not better than RNNs at modeling long-distance agreement.  Transformers however excel at word-sense disambiguation.  We analyze whether the choice of architecture and the use of an attention mechanism affect the way in which dialog systems use information available to them. "
    }
  ],
  [
    "End-to-End Simultaneous Translation System for IWSLT2020 Using Modality Agnostic Meta-Learning",
    {
      "Sentences": "Simultaneous Translation: The earlier works in simultaneous translation such as (Cho and Esipova, 2016;Gu et al., 2016;Press and Smith, 2018;Dalvi et al., 2018) lack the ability to anticipate the words with missing source context.  The waitk model introduced by (Ma et al., 2019a) brought in many improvements by introducing a simultaneous translation module which can be easily integrated into most of the sequence to sequence models.  Arivazhagan et al. (2019) introduced MILk which is capable of learning an adaptive schedule by using hierarchical attention; hence it performs better on the latency quality trade-off.  Waitk and MILk are both capable of anticipating words and achieving specified latency requirements by varying the hyper-parameters.   Speech to Text Translation: Most of the existing systems tackle the problem of simultaneous speech to text translation using a cascaded pipeline of online ASR and MT systems.   Previous works such as (Niehues et al., 2016);(Ari et al., 2020) propose re-translation strategies for simultaneous speech translation, but their use case is limited to settings where output revision is allowed.   Although there has been some work towards making an end-to-end offline speech translation modules, the paucity of training datasets remains a bottleneck.  The work done by (Ma et al., 2019a) cannot simply be extended to the domain of simultaneous speech translation as we discussed earlier.  Similar to our model (Gu et al., 2016) also uses a pre-trained model for the simultaneous translation task.  However, they use a full-sentence model during pre-training, unlike ours.  Our proposed model alleviates these issues, both our pre-training and fine-tuning training phases are done in an online fashion, hence avoiding any train-inference discrepancies.  Our model has a controllable latency which can be specified by k.   Meta Learning: Meta-Learning approaches have been particularly useful with low resource problems since they inherently learn to adapt to anew problem with less training examples.  Andrychowicz et al. (2016); Ha et al. (2016) focuses more on the meta policy while MAML system proposed by (Finn et al., 2017) focuses more on finding a good initialization point for the target tasks.  The work done by (Indurthi et al., 2020) and (Gu et al., 2018b) employ the MAML algorithm for low resource settings in offline speech-to-text and text-to-text translation task.  In this work, we adopt these strategies to the online translation tasks. "
    }
  ]
]